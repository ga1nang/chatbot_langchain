{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Hi There\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    token=os.getenv('HF_TOKEN')\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning (ML) is a sub-field of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide.\\n\\nThe primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly. Machine learning is a key component of the broader category of AI, and it’s the AI technology that’s behind services like recommendation systems, anti-spam filters, and voice-enabled personal assistants like Siri and Alexa.\\n\\nMachine learning is a part of the artificial intelligence (AI) domain, and it can be categorized into the following types:\\n\\n1. Supervised Learning\\n2. Unsupervised Learning\\n3. Semi-supervised Learning\\n4. Reinforcement Learning\\n\\nTypes of Machine Learning:\\n\\n1. Supervised Learning: Supervised learning is a type of machine learning algorithm that aims to predict an output variable based on the known input and output pairs. In supervised learning, the model is trained on labeled data, which means that the data is already tagged with the correct output. The goal is to learn a mapping function from input to output, such that given new input data, the model can predict the correct output. Examples of supervised learning include classification, regression, and support vector machines.\\n\\n2. Unsupervised Learning: Unsupervised learning is a type of machine learning algorithm that does not require labeled data. Instead, the model is trained on unlabeled data and the goal is to find hidden patterns or structure in the data. Unsupervised learning algorithms include clustering, dimensionality reduction, and anomaly detection. The goal of unsupervised learning is to learn the underlying distribution of the data and use this information to make predictions or generate insights.\\n\\n3. Semi-supervised Learning: Semi-supervised learning is a type of machine learning algorithm that combines the benefits of supervised and unsupervised learning. Semi-supervised learning algorithms are trained on a small amount of labeled data and a large amount of unlabeled data. The goal is to use the labeled data to'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('What is machine learning?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and how can it help businesses? Generative AI refers to a type of artificial intelligence (AI) that can create new content, such as text, images, or music, based on the data it has been trained on. This is different from traditional AI, which is typically used for tasks such as data analysis or decision making.\\n\\nOne of the key benefits of generative AI for businesses is its ability to automate content creation tasks. For example, a company could use generative AI to write blog posts, product descriptions, or social media posts. This can help businesses save time and resources, while also ensuring that the content is consistent and of high quality.\\n\\nGenerative AI can also be used to create new and innovative products. For example, a fashion company could use generative AI to design new clothing styles, or a music company could use it to create new songs. This can help businesses stay ahead of the competition and offer unique and exciting products to their customers.\\n\\nAnother benefit of generative AI is its ability to learn and improve over time. As the AI is exposed to more data, it can learn new patterns and improve its ability to create content that is relevant and engaging to its audience. This can help businesses stay up-to-date with the latest trends and adapt to changing market conditions.\\n\\nOverall, generative AI has the potential to revolutionize the way businesses create and deliver content, and can help them stay competitive in an increasingly crowded and fast-paced market. However, it is important for businesses to carefully consider the ethical implications of using AI to create content, and to ensure that they are using it in a way that is transparent and respectful of their audience.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('What is generative AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer: Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Question:{question}\n",
    "Answer: Lets think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_50100\\1648248190.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who won the cricket World up 2011',\n",
       " 'text': 'First, we know that the cricket World Cup is a major international tournament.\\nSecond, it is held every four years.\\nThird, the World Cup that took place in 2011 was the 10th edition of the tournament.\\nNow, to find out who won the 10th edition, we can search for the results of the 2011 Cricket World Cup.\\nAccording to the records, the 2011 Cricket World Cup was won by the Indian national cricket team. They defeated Sri Lanka in the final match, which was played on April 2, 2011, at the Wankhede Stadium in Mumbai, India.\\nTherefore, the answer to the question is India.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chain.invoke('Who won the cricket World up 2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
