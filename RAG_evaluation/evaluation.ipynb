{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "#define dataset: these are your test cases\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"What is LangChain?\"},\n",
    "        {\"question\": \"What is LangSmith?\"},\n",
    "        {\"question\": \"What is OpenAI?\"},\n",
    "        {\"question\": \"What is Google?\"},\n",
    "        {\"question\": \"What is Mistral?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"A framework for building LLM applications\"},\n",
    "        {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        {\"answer\": \"A technology company known for search\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "eval_llm = ChatOpenAI(model='gpt-4o', temperature=0.0)\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def evaluate_length(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"ouput\") or \"\"\n",
    "    required = example.outputs.get(\"answer\") or \"\"\n",
    "    score = int(len(prediction) < 2 * len(required))\n",
    "    return {\"key\":\"length\", \"score\":score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple wrapper that maps the input keys from our dataset to the function we want to call, \n",
    "#and then also maps the output of the function to the output key we expect.\n",
    "\n",
    "def langsmith_app(inputs):\n",
    "    output = my_app(inputs['question'])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\data_scien\\course\\Complete_genai_langchain_huggingface\\learn\\chatbot_langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chatbot-752ea048' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/d6be29c2-eb80-4935-9351-404b63c7d9be/compare?selectedSessions=05dfb4ae-152d-44fa-b340-dd4a696dfc9c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:08,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_length, qa_evaluator],\n",
    "    experiment_prefix='chatbot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-4-639fe38d' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/d6be29c2-eb80-4935-9351-404b63c7d9be/compare?selectedSessions=c847dd9b-2c97-4b5c-a93d-218854ce2c7a\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app_1(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "def langsmith_app_1(inputs):\n",
    "    output = my_app_1(inputs[\"question\"])\n",
    "    return {\"output\": output}\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app_1, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"openai-4\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'strict-openai-4-eca8b534' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/d6be29c2-eb80-4935-9351-404b63c7d9be/compare?selectedSessions=fbb5c914-b4fc-4bf2-81c5-8c1436b1a4ef\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "def my_app_2(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "def langsmith_app_2(inputs):\n",
    "    output = my_app_2(inputs[\"question\"])\n",
    "    return {\"output\": output}\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app_2, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"strict-openai-4\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
