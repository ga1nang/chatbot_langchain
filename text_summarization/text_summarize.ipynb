{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir('e:/data_scien/course/Complete_genai_langchain_huggingface/learn/chatbot_langchain/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000267F4E637A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000267F4E62720>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "llm = ChatGroq(api_key=api_key, model='Gemma2-9b-It')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"\n",
    "Artificial intelligence (AI) engineers are responsible for developing, programming and training the complex networks of algorithms that make up AI so that they can function like a human brain. \n",
    "This role requires combined expertise in software development, programming, data science and data engineering. \n",
    "Though this career is related to data engineering, AI engineers are rarely required to write the code that develops scalable data sharing. Instead, artificial intelligence developers locate and pull data from a variety of sources, create, develop and test machine learning models and then utilize application program interface (API) calls or embedded code to build and implement AI applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArtificial intelligence (AI) engineers are responsible for developing, programming and training the complex networks of algorithms that make up AI so that they can function like a human brain. \\nThis role requires combined expertise in software development, programming, data science and data engineering. \\nThough this career is related to data engineering, AI engineers are rarely required to write the code that develops scalable data sharing. Instead, artificial intelligence developers locate and pull data from a variety of sources, create, develop and test machine learning models and then utilize application program interface (API) calls or embedded code to build and implement AI applications.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message = [\n",
    "    SystemMessage(content='You are expert with experise in summarizing speeched.'),\n",
    "    HumanMessage(content=f\"Please provide a short and concise summary of the following speech:/n Text:{speech}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\data_scien\\course\\Complete_genai_langchain_huggingface\\learn\\chatbot_langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_45964\\2694584740.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(chat_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Summary:**\\n\\nAI engineers design, program, and train AI systems. They leverage expertise in software development, programming, data science, and data engineering to locate, process, and analyze data, create and test machine learning models, and implement AI applications using APIs or embedded code.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 159, 'total_tokens': 216, 'completion_time': 0.067340415, 'prompt_time': 0.096081127, 'queue_time': 0.0016080829999999963, 'total_time': 0.163421542}, 'model_name': 'Gemma-7b-It', 'system_fingerprint': 'fp_7d8efeb0b1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4c29b006-0f25-4339-bf67-3e3cb74759f0-0', usage_metadata={'input_tokens': 159, 'output_tokens': 57, 'total_tokens': 216})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'speech'], input_types={}, partial_variables={}, template='\\nWrite a summary of the following speech:\\nSpeech:{speech}\\nTranslate the precise summary to {language}\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "generic_template = \"\"\"\n",
    "Write a summary of the following speech:\n",
    "Speech:{speech}\n",
    "Translate the precise summary to {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['speech', 'language'],\n",
    "    template=generic_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite a summary of the following speech:\\nSpeech:\\nArtificial intelligence (AI) engineers are responsible for developing, programming and training the complex networks of algorithms that make up AI so that they can function like a human brain. \\nThis role requires combined expertise in software development, programming, data science and data engineering. \\nThough this career is related to data engineering, AI engineers are rarely required to write the code that develops scalable data sharing. Instead, artificial intelligence developers locate and pull data from a variety of sources, create, develop and test machine learning models and then utilize application program interface (API) calls or embedded code to build and implement AI applications.\\n\\nTranslate the precise summary to German\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_prompt = prompt.format(speech=speech, language='German')\n",
    "complete_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(complete_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Summary:**\\n\\nDie künstliche Intelligenz (AI) -Ingenieure sind für die Entwicklung, Programmierung und Training der komplexen Algorithmen verantwortlich, die das Verhalten eines menschlichen Gehirns imitieren. Diese Rolle erfordert eine Kombination von Kenntnissen in Softwareentwicklung, Programmierung, Datenwissenschaft und Datenengineering. Obwohl diese Karriere mit dem Daten Engineering zusammenhängt, werden AI-Ingenieure selten aufgerufen, Code zur Entwicklung einer skalierbaren Datenfreigabe zu schreiben. Stattdessen suchen und holen AI-Entwickler Daten von verschiedenen Quellen, erstellen, entwickeln und testen Machine-Learning-Modelle und verwenden API-Anrufe oder eingebetteten Code, um AI-Anwendungen zu erstellen und zu implementieren.\\n\\n**German Translation:**\\n\\nDie künstliche Intelligenz (AI) -Ingenieure sind für die Entwicklung, Programmierung und Training der komplexen Algorithmen verantwortlich, die das Verhalten eines menschlichen Gehirns imitieren. Diese Rolle erfordert eine Kombination von Kenntnissen in Softwareentwicklung, Programmierung, Datenwissenschaft und Datenengineering.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 220, 'prompt_tokens': 150, 'total_tokens': 370, 'completion_time': 0.262624851, 'prompt_time': 0.103645057, 'queue_time': 0.0008165920000000049, 'total_time': 0.366269908}, 'model_name': 'Gemma-7b-It', 'system_fingerprint': 'fp_7d8efeb0b1', 'finish_reason': 'stop', 'logprobs': None}, id='run-a77bcae9-8ed2-47cd-afdb-441f74e41d4d-0', usage_metadata={'input_tokens': 150, 'output_tokens': 220, 'total_tokens': 370})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chain = prompt|llm\n",
    "summary = llm_chain.invoke({'language':'German', 'speech': speech})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StuffDocumentChain Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 0}, page_content='10.1. AVERAGING/BAGGING 285\\nized situation, we have mindependent datasets of size n, composed of independent and\\nidentically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\\nWe obtain for each of them an estimator ˆf(j)\\nλ, wherej∈{1,...,m}andλis an associ-\\nated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\\nregression, the new predictor, ˆfbag\\nλ, is simply the average of all ˆf(j)\\nλ,j= 1,...,m .\\nIf we denote bias(j)(x) =E[ˆf(j)\\nλ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\\nλ(x)]\\n(assuming\\nthatxis ﬁxed and only taking expectations with respect to the data), the n they are the\\nsame for all j∈{1,...,m}and the bias of ˆfbag\\nλis the same as the base bias for a single\\ndataset (and thus so is the squared bias). At the same time, the va riance is divided by m\\nbecause the datasets are assumed to be independent.\\nThus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\\na higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\\nindependent datasets, it is relevant to concatenate all mdatasets into one large dataset\\nwithN=nmobservations and learn a single predictor with these: the generaliza tion\\nperformance of the average of mpredictors will often be the same as the one of the single\\npredictor on the large dataset, but with potential computational beneﬁts. We now give a\\nfew examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\\nThek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\\nprediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\\nbias was upper-bounded by 8 B2diam(X)2(2k\\nn)2/d(ford⩾2). At the same time, the\\nvariance was bounded byσ2\\nk, whereσ2is a bound on the noise variance on top of the\\ntarget function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\\nreplications, we get an excess risk upper-bounded by\\nσ2\\nkm+ 8B2diam(X)2(2k\\nn)2/d\\n.\\nWhen optimizing this bound with respect to k, we get that k1+2/d∝n2/d\\nm, leading to\\nk∝1\\nmd/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\\n(which is consistent with favoring higher variance estimators). The overall excess risk ends\\nup being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\\nobservations.\\nThus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\\nestimating independently, and combining linearly does not lead to an ov erall improved\\nstatistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\\nputational advantages when the mestimators can be computed in parallel (and totally\\nindependently). We thus obtain a distributed algorithm with the same worst-case pre-\\ndictive performance as for a single machine.\\nNote here that there is an upper bound on the number of replication s (and thus the\\nability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\\nand thus,mcannot grow larger than n2/d.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 1}, page_content='286 CHAPTER 10. ENSEMBLE LEARNING\\nExercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\\njority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\\nsection4.1.4to derive an upper bound on the expected risk, what is the corr esponding\\noptimal choice of mwhen using independent datasets?\\nRidge regression. Following the analysis from section 7.6.6, the variance of the ridge\\nregression estimator was proportional toσ2\\nnλ−1/αand the bias proportional to λt/s(see\\nprecise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\\nproportional toσ2\\nnmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\\nobservations (and the same regularization parameter). Again, wit h the proper choice\\nof regularization parameter (lower λthan for the full dataset), there is no statistical\\nadvantage. Still, there may be a computational one, not only for pa rallel processing but\\nalso with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\\ntime for ridge regression is superlinear in the number of observation s with running-time\\ncomplexities between O(n2) andO(n3) if no low-rank approximations are used.\\nExercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\\ntime complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\\nsplit of the data into mchunks? What is the optimal value of m?\\nExercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\\nan estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\\nproblem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\\nj=1ˆf(j)(x))\\nthe majority vote clas-\\nsiﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\\ntion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\\nE[1ˆfbag(x)̸=f∗(x)]⩽exp(\\n−m\\n2(ε(x))2\\n+)\\n.If∀x∈X,ε(x)⩾η >0, show that the expected\\nexcess risk is less than exp(\\n−m\\n2η2)\\n.\\nBeyond independent datasets. Having independent datasets may not be possible,\\nand one typically needs to artiﬁcially create such replicated dataset s from a single one,\\nwhich is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\\nstill, but this time with a potentially higher bias.\\n10.1.2 Bagging\\nWe consider datasets D(b), obtained with random weights v(b)\\ni∈R+,i= 1,...,n . For\\nthe bootstrap,1we consider nsamples from the original ndata points with replacement,\\nwhich correspond to integer weights v(b)\\ni∈N,i= 1,...,n , that sum to n. Such sets of\\nweights are sampled independently mtimes. We study m=∞for simplicity; that is,\\ninﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\\nmoderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\\n1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\\n(1994) for an introduction to bootstrapping methods in statistic s.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 2}, page_content='10.1. AVERAGING/BAGGING 287\\nwhich is important for highly variable predictors (which usually imply a lar ge estimation\\nvariance).\\nFor linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\\nsquare loss, such as kernel ridge regression or local averaging, t his leads to another linear\\nestimator. Therefore, this provides alternative ways of regulariz ing, which typically may\\nnot provide a strong statistical gain over existing methods but pro vide a computational\\ngain, in particular when each estimator is very eﬃcient to compute (s ee related examples\\nin section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\\nvariance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\\nlarizing methods. See also the end of section 10.2 for a short description of “random\\nforests,” which is also partially based on bagging.\\nFor simplicity, we will consider averaging estimators obtained by rand omly selecting s\\nobservations from the navailable ones, doing this many times (inﬁnitely many for the\\nanalysis), and averaging the predictions.\\nExercise 10.4 Show that when sampling nelements with replacement from nitems, the\\nexpected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\\nto inﬁnity.\\nOne-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\\nwhere the strong eﬀect of bagging is striking. The analysis in this sub section follows\\nfrom Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\\nthe pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\\n(ignoring ties), then we can write the bagged estimate as\\nˆf(x) =n∑\\ni=1Viy(i)(x),\\nwhere the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\\nprobability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\\nsubsample of size s. We consider sampling without replacement and leave sampling with\\nreplacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\\nTo select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\\nthat theith-nearest neighbor is selected but none of the closer neighbors, which leaves\\ns−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\\nVi= 0, while otherwise Vi=(n\\ns)−1(n−i\\ns−1)\\n, as the total number of subsets of size sis(n\\ns)\\n,\\nand there are(n−i\\ns−1)\\nrelevant ones.\\nWe can now use the reasoning from section 6.3.2. Since for any x, the weights given\\nto each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\\nvariance term is equal to∑n\\ni=1V2\\ni. To obtain a bound, we note that for i⩽n−s+ 1,\\nVi=s\\nn−s+ 1∏s−2\\nj=0(n−i−j)\\n∏s−2\\nj=0(n−j)=s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn−j)\\n⩽s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn)\\n,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 3}, page_content='288 CHAPTER 10. ENSEMBLE LEARNING\\nleading to, upper-bounding the sum by an integral,\\nn∑\\ni=1V2\\ni⩽s2\\n(n−s+ 1)2n∑\\ni=1(\\n1−i\\nn)2(s−1)⩽ns2\\n(n−s+ 1)2∫1\\n0(1−t)2(s−1)dt\\n⩽ns2\\n(n−s+ 1)21\\n2s−1⩽ns\\n(n−s+ 1)2=s\\nn1\\n(1 + 1/n−s/n)2.\\nFor the bias term, we need to bound∑n\\ni=1Vi·E[\\n∥x−x(i)(x)∥2]\\n, where the expectation\\nis with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\\nconditioning on the data and x, this isB2multiplied by the expectation of the distance\\nto the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\\non a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\\ns2/difd⩾2 (which we now\\nassume).\\nThus, the overall excess risk is less than\\n4B2diam(X)21\\ns2/d+s\\nn1\\n(1 + 1/n−s/n)2,\\nwhich we can balance by choosing s1+2/d∝n, leading to the same performance as the\\nk-nearest neighbor for a well-chosen k, but now with a bagged esimate.\\nIn ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\\neﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\\nneighbor estimate, and when sdecreases, the variance indeed decreases while the bias\\nincreases.\\n10.2 Random Projections and Averaging\\nIn section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\\ncan also be done through random projections of all observations. Such random projections\\ncan be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\\n(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\\nand (3) for kernel methods, using random features such as pres ented in section 7.4. Such\\nrandom projections can also reduce the number of samples while kee ping the dimension\\nﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\\nreduced size).\\nIn this section, we consider random projections for ordinary least -squares (OLS), with\\nthe same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\\ndesign matrix, in two settings:\\n•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minθ∈Rd∥Sy−SΦθ∥2\\n2, whereS∈Rs×n\\nis an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\\nments). This is an idealization of subsampling done in the previous sect ion. Here,\\nwe typically have n > s > d (more observations than the feature dimension), and'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 4}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 289\\nFigure 10.1. Subsampling estimates with m= 20 subsampled datasets, for varying sub-\\nsampling ratios n/s, with an estimation of the testing error. When n/s= 1, we recover\\nthe 1-nearest neighbor classiﬁer (which overﬁts), and when n/sgrows, we get better ﬁts\\nuntil underﬁtting kicks in. Optimal testing error is obtained for n/s= 8.\\none of the beneﬁts of sketching is to be able to store a reduced rep resentation of\\nthe data ( Rs×dinstead of Rn×d).\\n•Random projection : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minη∈Rs∥y−ΦSη∥2\\n2, where\\nS∈Rd×sis a more general sketching matrix. Here, we typically have d > n >\\ns(high-dimensional situation). The beneﬁts of random projection a re twofold:\\nreduction in computation time and regularization. This corresponds to replacing\\nthe corresponding feature vectors ϕ(x)∈RdbyS⊤ϕ(x)∈Rs. We will consider\\nGaussian matrices, but also subsampling matrices, and draw connec tions with kernel\\nmethods.\\nIn the following sections, we study these precisely for the OLS fram ework (it could\\nalso be done for ridge regression). We ﬁrst brieﬂy mention a common ly used and related\\napproach.\\nRandom forests. A popular algorithm called “random forests” ( Breiman ,2001) mixes\\nboth dimension reduction by projection and bagging: decision trees are learned on a\\nbootstrapped sample of the data, while selecting a random subset o f features at every\\nsplitting decision. This algorithm has nice properties (invariance to re scaling of the\\nvariables and robustness in high dimension due to the random featur e selection) and can\\nbe extended in many ways. See Biau and Scornet (2016) for details.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 5}, page_content='290 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.1 Gaussian Sketching\\nFollowing section 3.3on OLS, we consider a design matrix Φ ∈Rn×dwith rankd(i.e.,\\nΦ⊤Φ∈Rd×dinvertible), which implies n⩾d. We consider s > d Gaussian random\\nprojections, with typically s⩽n, but this is not necessary in the analysis that follows.\\nThe estimator ˆθ(j)is obtained by using S(j)∈Rs×n, withj= 1,...,m , wherem\\ndenotes the number of replications. We then consider ˆθ=1\\nm∑m\\nj=1ˆθ(j). Whenm= 1,\\nthis is a single sketch.\\nWe will consider the same ﬁxed design assumptions as in section 3.5; that is,y=\\nΦθ∗+ε, whereε∈Rnhas independent zero-mean components with variance σ2, and\\nθ∗∈Rd. Our goal is to compute the ﬁxed design error1\\nnEε,S∥Φˆθ−Φθ∗∥2\\n2, where we take\\nexpectations with respect to both the learning problem (in the ﬁxed design setting, the\\nnoise vector ε) and the added randomization (the sketching matrices S(j),j= 1,...,m ).\\nTo compute this error, we ﬁrst need to compute expectations and variances with\\nrespect to the random projections, assuming that εis ﬁxed.\\nWe ﬁrst introduce a representation tool that will allow simple expres sions of all predic-\\ntion vectors S(j)Φ. Since the Gaussian matrices S(j)are invariant by left and right multi-\\nplication by an orthogonal matrix, we can assume that the singular v alue decomposition\\n(SVD) of Φ = UDV⊤, whereV∈Rd×dis orthogonal (i.e., V⊤V=VV⊤=I),D∈Rd×d\\nis an invertible diagonal matrix, and U∈Rn×dhas orthonormal columns (i.e., U⊤U=I;\\nremember than n⩾d), is such that U=(I\\n0)\\n, and we can write S(j)=(\\nS(j)\\n1S(j)\\n2)\\n, with\\nS(j)\\n1∈Rs×dandS(j)\\n2∈Rs×(n−d). We can also split yasy=(y1\\ny2)\\nfory1∈Rdand\\ny2∈Rn−d.\\nWe can write the normal equation that deﬁnes ˆθ(j)∈Rd, for eachj∈{1,...,m}(i.e.,\\n(Φ⊤(S(j))⊤S(j)Φ)ˆθ(j)= Φ⊤(S(j))⊤S(j)y), leading to the following closed-form estimators\\nˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\\nregarding the SVD of Φ, we have S(j)Φ =S(j)\\n1DV⊤. We can then expand the prediction\\nvector in Rnas\\nΦˆθ(j)= Φ(Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\\n=(I\\n0)\\nDV⊤(VD(S(j)\\n1)⊤S(j)\\n1DV⊤)−1VD(S(j)\\n1)⊤S(j)y\\n=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)y=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤(S(j)\\n1y1+S(j)\\n2y2)\\n=(y1+ ((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2y2\\n0)\\n.\\nThus, since E[S(j)\\n2] = 0 andS(j)\\n2is independent of S(j)\\n1, we getES(j)[\\nΦˆθ(j)]\\n=(y1\\n0)\\n, which\\nhappens to be exactly the OLS estimator Φ ˆθOLS= Φ(Φ⊤Φ)−1Φ⊤y=(I0\\n0 0)\\ny. Moreover,\\n2Ifs⩾d, thenS(j)Φ has rank dalmost surely, and thus ˆθ(j)is uniquely deﬁned.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 6}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 291\\nwe have the model y= Φθ∗+εand, if we split εasε=(ε1\\nε2)\\n, we havey=(I\\n0)\\nDV⊤θ∗+ε,\\nand thusy2=ε2. We therefore get\\nES(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=ES(j)[\\n∥((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2ε2∥2\\n2]\\n.\\nTaking the expectation with respect to ε(through E[ε2ε⊤\\n2] =σ2I), using the trace trick,\\nand using expectations for the Wishart and inverse Wishart distribu tions,3this leads to\\nEε,S(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=σ2ES(j)[\\ntr(\\n(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤S(j)\\n2)]\\n=σ2ES(j)[\\ntr(\\nS(j)\\n2(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤)]\\n= (n−d)σ2ES(j)\\n1[\\ntr(\\n((S(j)\\n1)⊤S(j)\\n1)−1)]\\n=d\\ns−d−1(n−d)σ2.\\nWe can now compute the overall expected generalization error:\\n1\\nnEε,S(j)[\\ued79\\ued79\\ued791\\nmm∑\\nj=1Φˆθ(j)−Φθ∗\\ued79\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nΦˆθ(1)]\\n−Φθ∗\\ued79\\ued792\\n2]\\n+1\\nnmEε,S(1)[\\ued79\\ued79Φˆθ(1)−ES(1)Φˆθ(1)\\ued79\\ued792]\\n=1\\nnEε[\\ued79\\ued79ΦˆθOLS−Φθ∗\\ued79\\ued792\\n2]\\n+σ2d\\nnmn−d\\ns−d−1\\n=σ2d\\nn+σ2d\\nnmn−d\\ns−d−1.\\nThus, when morstends to inﬁnity, we recover the traditional OLS behavior, while for m\\nandsﬁnite, the performance degrades gracefully. Moreover, when s=n, even form= 1,\\nwe get essentially twice the performance of the OLS estimator. We n ote that to get the\\nsame performance as OLS (up to a factor of 2), we need m=n−d\\ns−d−1∼n\\nsreplications.\\nAs in section 10.1, there is no statistical gain (here, compared to OLS), but only\\npotentially a computational one (because some computations may b e done in parallel\\nand of reduced storage). See, for example, Dobriban and Liu (2019) for other criteria\\nand sketching matrices.\\nBeyond Gaussian sketching. In this section, we have chosen a Gaussian sketching\\nmatrixS. This made the analysis simple because of the properties of the Gaus sian\\ndistribution (invariance by rotation and availability of exact expecta tions for inverse\\nWishart distributions). The analysis can be extended with more comp lex tools to other\\nrandom sketching matrices with more attractive computational pr operties, such as with\\nmany zeros, leading to subsampling observations or dimensions. See Wang et al. (2018),\\nDobriban and Liu (2019), and the references therein. For the random projections that\\nfollow, our analysis will apply to more general sketches.\\n3IfS∈Ra×bhas independent standard Gaussian components, then E[(S⊤S)−1] =1\\na−b−1Iifa > b+1,\\nandE[SS⊤] =bI; seehttps://en.wikipedia.org/wiki/Inverse-Wishart_distr ibution.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 7}, page_content='292 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.2 Random Projections\\nWe also consider the ﬁxed design setup, with a design matrix Φ ∈Rn×dand a response\\nvector of the form y= Φθ∗+ε. We now assume that d>n (high-dimensional setup) and\\nthe rank of Φ is n. In this high-dimensional setup, we need some form of regularizatio n,\\nwhich will come here from random projections.\\nFor eachj∈{1,...,n}, we consider a sketching matrix S(j)∈Rd×s, fors⩽nsampled\\nindependently from a distribution to be determined (we only assume t hat almost surely,\\nits rank is equal to s). We then consider ˆ η(j)as a minimizer of min η∈Rs∥y−ΦS(j)η∥2\\n2.\\nFor simplicity, we assume that matrix Φ S(j)has ranks, which is the case almost surely\\nfor Gaussian projections; this implies that ˆ η(j)is unique, but our result applies in all\\nsituations, as we are only interested in the denoised response vect or. We now consider\\nthe average ˆθ=1\\nm∑m\\nj=1S(j)ˆη(j).\\nWe thus consider the estimator ˆ η(j)=(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rs, ob-\\ntained from the normal equation ( S(j))⊤Φ⊤ΦS(j)ˆη(j)= (S(j))⊤Φ⊤ywith the denoised\\nresponse vector\\nˆy(j)= ΦS(j)ˆη(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rn.\\nDenoting Π(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤, it takes the form ˆ y(j)= Π(j)y.\\nMatrix Π(j)is almost surely an orthogonal projection matrix into an s-dimensional vector\\nspace, and its expectation is denoted as ∆ ∈Rn×n, which satisﬁes tr(∆) = s. We have,\\nmoreover, 0 ≼∆≼I; that is, all eigenvalues of ∆ are between 0 and 1.\\nWe can then compute expectations and variances as follows:\\nES(j)[\\nˆy(j)]\\n=ES(j)[\\nΠ(j)y]\\n= ∆y= ∆[\\nΦθ∗+ε] = ∆ε+ ∆Φθ∗\\nES(j)[\\nˆy(j)]\\n−Φθ∗= ∆ε+ [∆−I]Φθ∗ (10.1)\\nES(j)[\\ued79\\ued79ˆy(j)−ES(j)[\\nˆy(j)]\\ued79\\ued792\\n2]\\n=ES(j)[\\ued79\\ued79(Π(j)−∆)y\\ued79\\ued792\\n2]\\n=y⊤ES(j)[(\\nΠ(j)−∆)2]\\ny\\n=y⊤ES(j)[\\nΠ(j)−∆Π(j)−Π(j)∆+∆2]\\nysince Π(j)Π(j)=Π(j),\\n=y⊤(\\n∆−∆2)\\ny,sinceE[Π(j)= ∆. (10.2)\\nThus, the overall (ﬁxed design) expected generalization error is e qual to, using that\\nS(1),...,S(m)are i.i.d. matrices,\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nˆy(1)]\\n−Φθ∗\\ued79\\ued792\\n2+1\\nmES(1)[\\ued79\\ued79ˆy(1)−ES(1)[\\nˆy(1)]\\ued79\\ued792\\n2]]\\nby taking expectations with respect to all S(j),\\n=1\\nnEε[\\ued79\\ued79∆ε+ [∆−I]Φθ∗\\ued79\\ued792\\n2+1\\nmy⊤(\\n∆−∆2)\\ny]\\nusing equations ( 10.1) and ( 10.2).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 8}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 293\\nUsing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I, we get\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=σ2\\nntr(∆2) +1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗+1\\nnm[\\nσ2(tr(∆)−tr(∆2)) +θ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗]\\nusing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I,\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[∆−I]2Φθ∗+1\\nnmθ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆ +(1\\nm−1)\\n(∆−∆2)]\\nΦθ∗\\n⩽σ2s\\nn+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗,since ∆2≼∆, (10.3)\\nwhich is the value for m= 1 (single replication). Note that the expectation (before taking\\nthe bound) decreases in m, with a limitσ2tr(∆2)\\nn+1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗whenm→+∞\\n(with improved bias and variance terms). We now follow Kab´ an (2014) and Thanei et al.\\n(2017) to bound the matrix I−∆.\\nSince ∆ is the expectation of a projection matrix, we already know th at 0≼∆≼I.\\nWe omit the superscript 0(j)for clarity, and we consider Π = Φ S(\\nS⊤Φ⊤ΦS)−1S⊤Φ. For\\nany vector z∈Rn, we consider\\nz⊤(I−∆)z=ES[\\nz⊤(I−Π)z]\\n=ES[\\nz⊤z−z⊤ΦS(\\nS⊤Φ⊤ΦS)−1S⊤Φ⊤z]\\n=ES[\\nmin\\nu∈Rs∥z−ΦSu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽ES[\\nmin\\nv∈Rd∥z−ΦSS⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RdES[\\n∥z−ΦSS⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nES[\\n∥z−ΦSS⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤ΦES[\\nSS⊤]\\nv+v⊤ES[\\nSS⊤Φ⊤ΦSS⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤z,\\nz⊤(I−∆)z⩽z⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nz.\\nWe then need to apply to z= Φθ∗and get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗.\\nThus, we get an overall upper bound of\\nσ2s\\nn+1\\nnθ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 9}, page_content='294 CHAPTER 10. ENSEMBLE LEARNING\\ncomposed of expectations that can be readily computed. As shown next for special\\ncases, we obtain a bias-variance trade-oﬀ similar to equation ( 3.6) for ridge regression\\nin section 3.6, but now with random projections. Note that in the ﬁxed design set ting,\\nthere is no explosion of the testing error when s=n, as opposed to the random design\\nsetting studied in section 12.2 in the context of “double descent” (where generalization\\nto unseen inputs is required).\\nGaussian projections. If we assume Gaussian random projections, with S∈Rd×s\\nwith independent standard Gaussian components, we get, from pr operties of the Wishart\\ndistribution,4\\nES[SS⊤] =sIandES[\\nSS⊤Φ⊤ΦSS⊤]\\n=s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I.\\nWe then get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤(\\nI−s2Φ(\\ns(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I)−1Φ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤Φθ∗−sθ∗(Φ⊤Φ)2(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n=θ⊤\\n∗Φ⊤Φ(\\nΦ⊤Φ + tr(Φ⊤Φ)I)(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n⩽2 tr(Φ⊤Φ)·θ⊤\\n∗Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\nusing that Φ⊤Φ + tr(Φ⊤Φ)Ihas eigenvalues less than 2 tr(Φ⊤Φ),\\n⩽2 tr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1,\\nsince Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1has eigenvalues less than 1 /(s+ 1). The overall\\nexcess risk is then less than\\nσ2s\\nn+2\\nntr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1, (10.4)\\nwhich is exactly of the form obtained for ridge regression in equation (3.6) with the\\nidentiﬁcation s∼tr(Φ⊤Φ)\\nλ. We can consider other sketching matrices with additional\\nproperties, such as sparsity (see exercise 10.5).\\nExercise 10.5 We consider a sketching matrix S∈Rd×s, where each column is equal\\nto one of the dcanonical basis vectors of Rd, selected uniformly at random and indepen-\\ndently. Compute E[SS⊤], as well as ES[\\nSS⊤Φ⊤ΦSS⊤]\\n, as well as a bound similar to\\nequation ( 10.4).\\n4IfW=S1S⊤\\n1forS1∈Rn×swith independent standard Gaussian components, then E[W] =sIand\\nfor ann×ndiagonal matrix D, we have E[WD2W] =s(s+1)D2+str(D2)I.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 10}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 295\\nKernel methods. ( ♦)The random projection idea can be extended to kernel meth-\\nods discussed in chapter 7. We consider the kernel matrix K= ΦΦ⊤∈Rn×n, and the\\nassumption y= Φθ∗+εwith∥θ∗∥2bounded is turned into y=y∗+εwithy⊤\\n∗K−1y∗\\nbounded. This corresponds to y∗=Kα, with a reproducing kernel Hilbert space (RKHS)\\nnormα⊤Kα. We then consider a random sketch ˆΦ∈Rn×sand an approximate ker-\\nnel matrix ˆK. We then obtain an estimate ˆ y=ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y. Matrix Π is then\\nΠ = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤, and for the analysis, we need to compute its expectation, ∆ (this\\ncorresponds to replacing Φ Sin earlier developments starting at the beginning of sec-\\ntion 10.2.2 with ˆΦ). We have, following the same reasoning as before, for an arbitra ry\\ndeterministic z∈Rn,\\nz⊤(I−∆)z=EˆΦ[\\nz⊤(I−Π)z]\\n=EˆΦ[\\nz⊤z−z⊤ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤z]\\n=EˆΦ[\\nmin\\nu∈Rs∥z−ˆΦu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽EˆΦ[\\nmin\\nv∈Rn∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RnEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤EˆΦ[ˆΦˆΦ⊤]\\nv+v⊤EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤]\\nz,\\nz⊤(I−∆)z⩽z⊤(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\nz. (10.5)\\nWe then need to apply equation ( 10.5) toz=y∗to get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽y⊤\\n∗(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\ny∗.\\nWe can, for example, consider each column of ˆΦ to be sampled from a Gaussian distri-\\nbution with mean zero and covariance matrix K, for which we have\\nEˆΦ[ˆΦˆΦ⊤]\\n=sKandEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n=s(s+ 1)K2+str(K)·K.\\nUsing equation ( 10.3), with the same derivations that led to equation ( 10.4), this leads\\nto a bound on the expected excess risk equal toσ2s\\nn+2\\nntr(K)y⊤\\n∗K−1y∗\\ns+1, which is exactly\\nthe bound in equation ( 10.4) in the kernel context. However, it is not interesting in\\npractice, as it requires the computation of the kernel matrix Kand typically a square\\nroot to sample from the multivariate Gaussian distribution, which has a running-time\\ncomplexity of O(n3).\\nIn practice, many kernels come with a random feature expansion of the formk(x,x′) =\\nEv[\\nϕ(x,v)ϕ(x′,v)]\\n, such that|ϕ(x,v)|⩽Ralmost surely (as presented in section 7.4).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 11}, page_content='296 CHAPTER 10. ENSEMBLE LEARNING\\nWe can take for each column of ˆΦ the vector ( ϕ(x1,v),...,ϕ (xn,v))⊤∈Rnfor a random\\nindependent v. Then we have E[ˆΦˆΦ⊤]\\n=sKby construction, while a short calculation\\n(left as an exercise) shows that the second-order moment can be bounded as\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n≼s(s−1)K2+sR2K.\\nThis leads to the boundσ2s\\nn+1\\nnR2y⊤\\n∗K−1y∗\\ns−1, which is almost the same as before, but with\\nan eﬃcient practical algorithm (since we now have to solve a least-sq uares regression\\nproblem in dimension s, which is more eﬃcient than using the kernel trick if s<n ).\\nExperiments. In ﬁgure 10.2, we consider a polynomial regression problem in dimen-\\nsiondX= 20, with polynomials of a maximum degree of 2, and thus a feature sp ace\\nof dimension d= 1 +dX+dX(dX+ 1)/2 = 231. We also compare ridge regression with\\nGaussian random projections. We see better performance as mgrows, consistent with our\\nbounds (underﬁtting for small s, overﬁtting with large s). Moreover, when the number\\nmof times the dataset is randomly projected goes from 10 to 100, we obtain almost the\\nsame plot, with better performance than m= 1, highlighting the fact that m= 1 is not\\noptimal but taking mtoo large is not useful.\\nJohnson-Lindenstrauss lemma ( ♦).A related classical result in Gaussian random\\nprojections shows that nfeature vectors ϕ1,...,ϕn∈Rdcan be well represented in di-\\nmensionsby Gaussian random projections, with sgrowing only logarithmically in n, and\\nindependent of the underlying dimension. Lemma 10.1 shows that all pairwise distances\\nare preserved (a small modiﬁcation would lead to the preservation o f all dot products).\\nLemma 10.1 ( Johnson and Lindenstrauss ,1984)Givenϕ1,...,ϕn∈Rd, letS∈\\nRd×sbe a random matrix with independent standard Gaussian rando m variables. Then,\\nfor anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\\nε2logn2\\nδ, with probability greater than 1−δ,\\nwe have\\n∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\\n2⩽\\ued79\\ued79s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\\n2⩽(1+ε)∥ϕi−ϕj∥2\\n2.\\n(10.6)\\nProof (♦) Letψ∈Rdwith theℓ2-norm equal to 1. The random variable y=ψ⊤SS⊤ψ\\nis the sum of srandom variables ψ⊤S·jS⊤\\n·jψ, forS·jthejth column of S,j∈{1,...,s}.\\nEach of these is the square of S⊤\\n·jψ, which is Gaussian with mean zero and variance equal\\nto∥ψ∥2\\n2= 1. Thus, yis a chi-squared random variable.5We can thus apply concentration\\nresults from exercise 8.1, leading to\\nP(\\n|y−s|⩾sε)\\n⩽(1−ε\\nexp(−ε))s/2\\n+(1 +ε\\nexp(ε))s/2\\n.\\nWe can then use the inequality log(1 + u)⩽u−u2\\n3for any|u|⩽1\\n2, applied to εand\\n−ε, leading to the probability bound P(\\n|y−s|⩾sε)\\n⩽2 exp(\\n−s\\n2ε2\\n3)\\n.We then apply\\n5Seehttps://en.wikipedia.org/wiki/Chi-squared_distribut ion.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 12}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 297\\nFigure 10.2. Polynomial regression in dimension 20, with polynomials of a maximum\\ndegree of 2, with n= 100. Top left: training and testing errors for ridge regression in t he\\nﬁxed design setting (the input data are ﬁxed, and only the noise var iables are resampled\\nfor computing the test error). All other plots: training and testin g errors for Gaussian\\nrandom projections, with diﬀerent numbers of random projection s:m= 1 (top right),\\nm= 10 (bottom left), and m= 100 (bottom right). All the curves are averaged over 100\\nreplications of the noise variables and the random projections.\\nthis bound for ψbeing then(n−1)/2 vectorsϕi−ϕj, fori̸=j, leading to, using a\\nunion bound, a probability that equation ( 10.6) is not satisﬁed with a probability less\\nthann2exp(−sε2/6),leading to the desired result.\\nIn our context of least-squares regression, the Johnson-Linde nstrauss lemma shows that\\nthe kernel matrix is preserved by random projections so that pre dictions with the pro-\\njected data should be close to predictions with the original data. Th e results in this\\nsection provide a direct proof that aims to characterize directly th e predictive perfor-\\nmance of such random projections (using the Johnson-Lindenstr auss lemma to obtain\\nsimilar bounds is not straightforward as we consider unregularized r egression, where per-\\nturbations of matrix inverses are harder to control).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 13}, page_content='298 CHAPTER 10. ENSEMBLE LEARNING\\n10.3 Boosting\\nIn sections 10.1 and10.2, we focused on uniformly combining the outputs (e.g., plain av-\\neraging) of estimators obtained by randomly reweighted versions o f the original datasets.\\nReweighting was performed independent of the performance of th e resulting prediction\\nfunctions, and the training procedures for all predictors could be done in parallel. In this\\nsection, we explore sequential reweightings of the training datasets that depend on the\\nmistakes made by the current prediction functions. While the natur al parallelizability is\\nlost, we will see that we get additional statistical beneﬁts.\\nIn the early boosting procedures adapted to binary classiﬁcation, the original learn-\\ning algorithms (going from datasets to prediction functions with bina ry values) were used\\ndirectly on a reweighted version, such as Adaboost (see, e.g., Freund et al. ,1999). Our\\nanalysis will be carried out for boosting procedures, often referr ed to as “gradient boost-\\ning,” which are adapted to real-valued outputs, as done in the rest o f this book (noting\\nthat for classiﬁcation, we can use convex surrogates).\\nThe theory of boosting is rich, with many connections, and in this sec tion, we only\\nprovide a consistency proof in the simplest setting. See Schapire and Freund (2012) for\\nmore details.\\n10.3.1 Problem Setup\\nGiven an input space Xandnobservations ( xi,yi)∈X×R,i= 1,...,n , we are given a\\nset of predictors ϕ(·,w) :X→R, forw∈W, withWtypically being a compact subset of\\na ﬁnite-dimensional vector space.\\nThe main assumption is that given weights α∈Rn, one can reasonably easily ﬁnd\\nthe function ϕ(·,w) that minimizes with respect to w∈W:\\nn∑\\ni=1αiϕ(xi,w); (10.7)\\nthat is, the dot product between αand thenoutputs ofϕ(·,w) on thenobservations. In\\nthis section, for simplicity, we assume that this minimization can be don e exactly. This\\nis often referred to as the “weak learner” assumption. Many exam ples are available, such\\nas the following:\\n•Linear stumps for X=Rd:ϕ(x,w) =±(w⊤\\n0x+w1)+, wherew= (w0,w1)∈Rd×R,\\nwith sometimes the restriction that whas a single nonzero component (where the\\nweak learning tractability assumption is indeed veriﬁed; see exercise 10.6). This\\nwill lead to a predictor, which is a one-hidden-layer neural network a s presented\\nin chapter 9, but learned sequentially rather than by GD on the empirical risk. In\\nthe context of binary classiﬁcation, the weak learners are sometim es thresholded to\\nvalues in{−1,1}by taking their signs.\\nExercise 10.6 For linear stumps with only one nonzero coordinate for the sl ope,\\nshow how to minimize equation ( 10.7) eﬃciently.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('text_summarization/tree.pdf')\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Write a concise and short summary of the following speech,\n",
    "Speech: {text}\n",
    "\"\"\"\n",
    "prompt_stuff = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Write a concise and short summary of the following speech,\n",
      "Speech: 10.1. AVERAGING/BAGGING 285\n",
      "ized situation, we have mindependent datasets of size n, composed of independent and\n",
      "identically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\n",
      "We obtain for each of them an estimator ˆf(j)\n",
      "λ, wherej∈{1,...,m}andλis an associ-\n",
      "ated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\n",
      "regression, the new predictor, ˆfbag\n",
      "λ, is simply the average of all ˆf(j)\n",
      "λ,j= 1,...,m .\n",
      "If we denote bias(j)(x) =E[ˆf(j)\n",
      "λ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\n",
      "λ(x)]\n",
      "(assuming\n",
      "thatxis ﬁxed and only taking expectations with respect to the data), the n they are the\n",
      "same for all j∈{1,...,m}and the bias of ˆfbag\n",
      "λis the same as the base bias for a single\n",
      "dataset (and thus so is the squared bias). At the same time, the va riance is divided by m\n",
      "because the datasets are assumed to be independent.\n",
      "Thus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\n",
      "a higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\n",
      "independent datasets, it is relevant to concatenate all mdatasets into one large dataset\n",
      "withN=nmobservations and learn a single predictor with these: the generaliza tion\n",
      "performance of the average of mpredictors will often be the same as the one of the single\n",
      "predictor on the large dataset, but with potential computational beneﬁts. We now give a\n",
      "few examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\n",
      "Thek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\n",
      "prediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\n",
      "bias was upper-bounded by 8 B2diam(X)2(2k\n",
      "n)2/d(ford⩾2). At the same time, the\n",
      "variance was bounded byσ2\n",
      "k, whereσ2is a bound on the noise variance on top of the\n",
      "target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\n",
      "replications, we get an excess risk upper-bounded by\n",
      "σ2\n",
      "km+ 8B2diam(X)2(2k\n",
      "n)2/d\n",
      ".\n",
      "When optimizing this bound with respect to k, we get that k1+2/d∝n2/d\n",
      "m, leading to\n",
      "k∝1\n",
      "md/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\n",
      "(which is consistent with favoring higher variance estimators). The overall excess risk ends\n",
      "up being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\n",
      "observations.\n",
      "Thus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\n",
      "estimating independently, and combining linearly does not lead to an ov erall improved\n",
      "statistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\n",
      "putational advantages when the mestimators can be computed in parallel (and totally\n",
      "independently). We thus obtain a distributed algorithm with the same worst-case pre-\n",
      "dictive performance as for a single machine.\n",
      "Note here that there is an upper bound on the number of replication s (and thus the\n",
      "ability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\n",
      "and thus,mcannot grow larger than n2/d.\n",
      "\n",
      "286 CHAPTER 10. ENSEMBLE LEARNING\n",
      "Exercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\n",
      "jority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\n",
      "section4.1.4to derive an upper bound on the expected risk, what is the corr esponding\n",
      "optimal choice of mwhen using independent datasets?\n",
      "Ridge regression. Following the analysis from section 7.6.6, the variance of the ridge\n",
      "regression estimator was proportional toσ2\n",
      "nλ−1/αand the bias proportional to λt/s(see\n",
      "precise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\n",
      "proportional toσ2\n",
      "nmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\n",
      "observations (and the same regularization parameter). Again, wit h the proper choice\n",
      "of regularization parameter (lower λthan for the full dataset), there is no statistical\n",
      "advantage. Still, there may be a computational one, not only for pa rallel processing but\n",
      "also with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\n",
      "time for ridge regression is superlinear in the number of observation s with running-time\n",
      "complexities between O(n2) andO(n3) if no low-rank approximations are used.\n",
      "Exercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\n",
      "time complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\n",
      "split of the data into mchunks? What is the optimal value of m?\n",
      "Exercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\n",
      "an estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\n",
      "problem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\n",
      "j=1ˆf(j)(x))\n",
      "the majority vote clas-\n",
      "siﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\n",
      "tion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\n",
      "E[1ˆfbag(x)̸=f∗(x)]⩽exp(\n",
      "−m\n",
      "2(ε(x))2\n",
      "+)\n",
      ".If∀x∈X,ε(x)⩾η >0, show that the expected\n",
      "excess risk is less than exp(\n",
      "−m\n",
      "2η2)\n",
      ".\n",
      "Beyond independent datasets. Having independent datasets may not be possible,\n",
      "and one typically needs to artiﬁcially create such replicated dataset s from a single one,\n",
      "which is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\n",
      "still, but this time with a potentially higher bias.\n",
      "10.1.2 Bagging\n",
      "We consider datasets D(b), obtained with random weights v(b)\n",
      "i∈R+,i= 1,...,n . For\n",
      "the bootstrap,1we consider nsamples from the original ndata points with replacement,\n",
      "which correspond to integer weights v(b)\n",
      "i∈N,i= 1,...,n , that sum to n. Such sets of\n",
      "weights are sampled independently mtimes. We study m=∞for simplicity; that is,\n",
      "inﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\n",
      "moderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\n",
      "1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\n",
      "(1994) for an introduction to bootstrapping methods in statistic s.\n",
      "\n",
      "10.1. AVERAGING/BAGGING 287\n",
      "which is important for highly variable predictors (which usually imply a lar ge estimation\n",
      "variance).\n",
      "For linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\n",
      "square loss, such as kernel ridge regression or local averaging, t his leads to another linear\n",
      "estimator. Therefore, this provides alternative ways of regulariz ing, which typically may\n",
      "not provide a strong statistical gain over existing methods but pro vide a computational\n",
      "gain, in particular when each estimator is very eﬃcient to compute (s ee related examples\n",
      "in section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\n",
      "variance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\n",
      "larizing methods. See also the end of section 10.2 for a short description of “random\n",
      "forests,” which is also partially based on bagging.\n",
      "For simplicity, we will consider averaging estimators obtained by rand omly selecting s\n",
      "observations from the navailable ones, doing this many times (inﬁnitely many for the\n",
      "analysis), and averaging the predictions.\n",
      "Exercise 10.4 Show that when sampling nelements with replacement from nitems, the\n",
      "expected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\n",
      "to inﬁnity.\n",
      "One-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\n",
      "where the strong eﬀect of bagging is striking. The analysis in this sub section follows\n",
      "from Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\n",
      "the pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\n",
      "(ignoring ties), then we can write the bagged estimate as\n",
      "ˆf(x) =n∑\n",
      "i=1Viy(i)(x),\n",
      "where the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\n",
      "probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\n",
      "subsample of size s. We consider sampling without replacement and leave sampling with\n",
      "replacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\n",
      "To select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\n",
      "that theith-nearest neighbor is selected but none of the closer neighbors, which leaves\n",
      "s−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\n",
      "Vi= 0, while otherwise Vi=(n\n",
      "s)−1(n−i\n",
      "s−1)\n",
      ", as the total number of subsets of size sis(n\n",
      "s)\n",
      ",\n",
      "and there are(n−i\n",
      "s−1)\n",
      "relevant ones.\n",
      "We can now use the reasoning from section 6.3.2. Since for any x, the weights given\n",
      "to each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\n",
      "variance term is equal to∑n\n",
      "i=1V2\n",
      "i. To obtain a bound, we note that for i⩽n−s+ 1,\n",
      "Vi=s\n",
      "n−s+ 1∏s−2\n",
      "j=0(n−i−j)\n",
      "∏s−2\n",
      "j=0(n−j)=s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n−j)\n",
      "⩽s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n)\n",
      ",\n",
      "\n",
      "288 CHAPTER 10. ENSEMBLE LEARNING\n",
      "leading to, upper-bounding the sum by an integral,\n",
      "n∑\n",
      "i=1V2\n",
      "i⩽s2\n",
      "(n−s+ 1)2n∑\n",
      "i=1(\n",
      "1−i\n",
      "n)2(s−1)⩽ns2\n",
      "(n−s+ 1)2∫1\n",
      "0(1−t)2(s−1)dt\n",
      "⩽ns2\n",
      "(n−s+ 1)21\n",
      "2s−1⩽ns\n",
      "(n−s+ 1)2=s\n",
      "n1\n",
      "(1 + 1/n−s/n)2.\n",
      "For the bias term, we need to bound∑n\n",
      "i=1Vi·E[\n",
      "∥x−x(i)(x)∥2]\n",
      ", where the expectation\n",
      "is with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\n",
      "conditioning on the data and x, this isB2multiplied by the expectation of the distance\n",
      "to the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\n",
      "on a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\n",
      "s2/difd⩾2 (which we now\n",
      "assume).\n",
      "Thus, the overall excess risk is less than\n",
      "4B2diam(X)21\n",
      "s2/d+s\n",
      "n1\n",
      "(1 + 1/n−s/n)2,\n",
      "which we can balance by choosing s1+2/d∝n, leading to the same performance as the\n",
      "k-nearest neighbor for a well-chosen k, but now with a bagged esimate.\n",
      "In ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\n",
      "eﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\n",
      "neighbor estimate, and when sdecreases, the variance indeed decreases while the bias\n",
      "increases.\n",
      "10.2 Random Projections and Averaging\n",
      "In section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\n",
      "can also be done through random projections of all observations. Such random projections\n",
      "can be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\n",
      "(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\n",
      "and (3) for kernel methods, using random features such as pres ented in section 7.4. Such\n",
      "random projections can also reduce the number of samples while kee ping the dimension\n",
      "ﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\n",
      "reduced size).\n",
      "In this section, we consider random projections for ordinary least -squares (OLS), with\n",
      "the same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\n",
      "design matrix, in two settings:\n",
      "•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\n",
      "2by minθ∈Rd∥Sy−SΦθ∥2\n",
      "2, whereS∈Rs×n\n",
      "is an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\n",
      "ments). This is an idealization of subsampling done in the previous sect ion. Here,\n",
      "we typically have n > s > d (more observations than the feature dimension), and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This passage describes **averaging/bagging methods** in machine learning, focusing on reducing variance and improving predictive performance. \\n\\n**Key Points:**\\n\\n* **Averaging with Independent Datasets:**\\n    *  The technique involves splitting data into multiple independent datasets and training separate models on each. \\n    * The final prediction is the average of the predictions from all individual models.\\n    *  This method reduces variance but can increase bias.\\n* **Bagging (Bootstrap Aggregating):**\\n    *  A specific type of averaging where each dataset is created by randomly sampling (with replacement) from the original dataset.\\n    *  This introduces randomness and helps stabilize models with high variance.\\n* **Examples:**\\n    * **k-nearest neighbor:** Bagging reduces variance by averaging predictions from slightly different subsets of data points.\\n    * **Ridge Regression:** Bagging can improve computational efficiency while maintaining similar predictive performance.\\n\\n* **Random Projections:**\\n    * An alternative to bagging that involves randomly projecting the data onto a lower-dimensional space before training.\\n    * This can reduce both dimensionality and variance, making it useful for large datasets.\\n\\n**Benefits of Averaging/Bagging:**\\n\\n* Reduces variance in model predictions, leading to more stable and reliable performance.\\n* Can improve performance for models with high variance, such as k-nearest neighbors.\\n* Can be computationally efficient, especially when individual models are fast to train.\\n\\n**Limitations:**\\n\\n*  Can increase bias compared to a single, well-trained model.\\n*  May not be as effective for models with low variance.\\n\\n\\n\\nLet me know if you'd like more details on any specific aspect of averaging/bagging!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the file to large for stuff method so I just get 4 elements\n",
    "chain = load_summarize_chain(llm, chain_type='stuff', prompt=prompt_stuff, verbose=True)\n",
    "output_summary = chain.run(docs[:4])\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map reduce to Summarize Large documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 0}, page_content='10.1. AVERAGING/BAGGING 285\\nized situation, we have mindependent datasets of size n, composed of independent and\\nidentically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\\nWe obtain for each of them an estimator ˆf(j)\\nλ, wherej∈{1,...,m}andλis an associ-\\nated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\\nregression, the new predictor, ˆfbag\\nλ, is simply the average of all ˆf(j)\\nλ,j= 1,...,m .\\nIf we denote bias(j)(x) =E[ˆf(j)\\nλ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\\nλ(x)]\\n(assuming\\nthatxis ﬁxed and only taking expectations with respect to the data), the n they are the\\nsame for all j∈{1,...,m}and the bias of ˆfbag\\nλis the same as the base bias for a single\\ndataset (and thus so is the squared bias). At the same time, the va riance is divided by m\\nbecause the datasets are assumed to be independent.\\nThus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\\na higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\\nindependent datasets, it is relevant to concatenate all mdatasets into one large dataset\\nwithN=nmobservations and learn a single predictor with these: the generaliza tion\\nperformance of the average of mpredictors will often be the same as the one of the single\\npredictor on the large dataset, but with potential computational beneﬁts. We now give a\\nfew examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\\nThek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\\nprediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\\nbias was upper-bounded by 8 B2diam(X)2(2k\\nn)2/d(ford⩾2). At the same time, the\\nvariance was bounded byσ2\\nk, whereσ2is a bound on the noise variance on top of the\\ntarget function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\\nreplications, we get an excess risk upper-bounded by\\nσ2\\nkm+ 8B2diam(X)2(2k\\nn)2/d\\n.\\nWhen optimizing this bound with respect to k, we get that k1+2/d∝n2/d\\nm, leading to\\nk∝1\\nmd/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\\n(which is consistent with favoring higher variance estimators). The overall excess risk ends\\nup being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\\nobservations.\\nThus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\\nestimating independently, and combining linearly does not lead to an ov erall improved\\nstatistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\\nputational advantages when the mestimators can be computed in parallel (and totally\\nindependently). We thus obtain a distributed algorithm with the same worst-case pre-\\ndictive performance as for a single machine.\\nNote here that there is an upper bound on the number of replication s (and thus the\\nability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\\nand thus,mcannot grow larger than n2/d.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 1}, page_content='286 CHAPTER 10. ENSEMBLE LEARNING\\nExercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\\njority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\\nsection4.1.4to derive an upper bound on the expected risk, what is the corr esponding\\noptimal choice of mwhen using independent datasets?\\nRidge regression. Following the analysis from section 7.6.6, the variance of the ridge\\nregression estimator was proportional toσ2\\nnλ−1/αand the bias proportional to λt/s(see\\nprecise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\\nproportional toσ2\\nnmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\\nobservations (and the same regularization parameter). Again, wit h the proper choice\\nof regularization parameter (lower λthan for the full dataset), there is no statistical\\nadvantage. Still, there may be a computational one, not only for pa rallel processing but\\nalso with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\\ntime for ridge regression is superlinear in the number of observation s with running-time\\ncomplexities between O(n2) andO(n3) if no low-rank approximations are used.\\nExercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\\ntime complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\\nsplit of the data into mchunks? What is the optimal value of m?\\nExercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\\nan estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\\nproblem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\\nj=1ˆf(j)(x))\\nthe majority vote clas-\\nsiﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\\ntion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\\nE[1ˆfbag(x)̸=f∗(x)]⩽exp(\\n−m\\n2(ε(x))2\\n+)\\n.If∀x∈X,ε(x)⩾η >0, show that the expected\\nexcess risk is less than exp(\\n−m\\n2η2)\\n.\\nBeyond independent datasets. Having independent datasets may not be possible,\\nand one typically needs to artiﬁcially create such replicated dataset s from a single one,\\nwhich is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\\nstill, but this time with a potentially higher bias.\\n10.1.2 Bagging\\nWe consider datasets D(b), obtained with random weights v(b)\\ni∈R+,i= 1,...,n . For\\nthe bootstrap,1we consider nsamples from the original ndata points with replacement,\\nwhich correspond to integer weights v(b)\\ni∈N,i= 1,...,n , that sum to n. Such sets of\\nweights are sampled independently mtimes. We study m=∞for simplicity; that is,\\ninﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\\nmoderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\\n1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\\n(1994) for an introduction to bootstrapping methods in statistic s.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 2}, page_content='10.1. AVERAGING/BAGGING 287\\nwhich is important for highly variable predictors (which usually imply a lar ge estimation\\nvariance).\\nFor linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\\nsquare loss, such as kernel ridge regression or local averaging, t his leads to another linear\\nestimator. Therefore, this provides alternative ways of regulariz ing, which typically may\\nnot provide a strong statistical gain over existing methods but pro vide a computational\\ngain, in particular when each estimator is very eﬃcient to compute (s ee related examples\\nin section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\\nvariance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\\nlarizing methods. See also the end of section 10.2 for a short description of “random\\nforests,” which is also partially based on bagging.\\nFor simplicity, we will consider averaging estimators obtained by rand omly selecting s\\nobservations from the navailable ones, doing this many times (inﬁnitely many for the\\nanalysis), and averaging the predictions.\\nExercise 10.4 Show that when sampling nelements with replacement from nitems, the\\nexpected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\\nto inﬁnity.\\nOne-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\\nwhere the strong eﬀect of bagging is striking. The analysis in this sub section follows\\nfrom Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\\nthe pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\\n(ignoring ties), then we can write the bagged estimate as\\nˆf(x) =n∑\\ni=1Viy(i)(x),\\nwhere the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\\nprobability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\\nsubsample of size s. We consider sampling without replacement and leave sampling with\\nreplacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\\nTo select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\\nthat theith-nearest neighbor is selected but none of the closer neighbors, which leaves\\ns−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\\nVi= 0, while otherwise Vi=(n\\ns)−1(n−i\\ns−1)\\n, as the total number of subsets of size sis(n\\ns)\\n,\\nand there are(n−i\\ns−1)\\nrelevant ones.\\nWe can now use the reasoning from section 6.3.2. Since for any x, the weights given\\nto each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\\nvariance term is equal to∑n\\ni=1V2\\ni. To obtain a bound, we note that for i⩽n−s+ 1,\\nVi=s\\nn−s+ 1∏s−2\\nj=0(n−i−j)\\n∏s−2\\nj=0(n−j)=s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn−j)\\n⩽s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn)\\n,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 3}, page_content='288 CHAPTER 10. ENSEMBLE LEARNING\\nleading to, upper-bounding the sum by an integral,\\nn∑\\ni=1V2\\ni⩽s2\\n(n−s+ 1)2n∑\\ni=1(\\n1−i\\nn)2(s−1)⩽ns2\\n(n−s+ 1)2∫1\\n0(1−t)2(s−1)dt\\n⩽ns2\\n(n−s+ 1)21\\n2s−1⩽ns\\n(n−s+ 1)2=s\\nn1\\n(1 + 1/n−s/n)2.\\nFor the bias term, we need to bound∑n\\ni=1Vi·E[\\n∥x−x(i)(x)∥2]\\n, where the expectation\\nis with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\\nconditioning on the data and x, this isB2multiplied by the expectation of the distance\\nto the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\\non a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\\ns2/difd⩾2 (which we now\\nassume).\\nThus, the overall excess risk is less than\\n4B2diam(X)21\\ns2/d+s\\nn1\\n(1 + 1/n−s/n)2,\\nwhich we can balance by choosing s1+2/d∝n, leading to the same performance as the\\nk-nearest neighbor for a well-chosen k, but now with a bagged esimate.\\nIn ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\\neﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\\nneighbor estimate, and when sdecreases, the variance indeed decreases while the bias\\nincreases.\\n10.2 Random Projections and Averaging\\nIn section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\\ncan also be done through random projections of all observations. Such random projections\\ncan be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\\n(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\\nand (3) for kernel methods, using random features such as pres ented in section 7.4. Such\\nrandom projections can also reduce the number of samples while kee ping the dimension\\nﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\\nreduced size).\\nIn this section, we consider random projections for ordinary least -squares (OLS), with\\nthe same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\\ndesign matrix, in two settings:\\n•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minθ∈Rd∥Sy−SΦθ∥2\\n2, whereS∈Rs×n\\nis an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\\nments). This is an idealization of subsampling done in the previous sect ion. Here,\\nwe typically have n > s > d (more observations than the feature dimension), and'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 4}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 289\\nFigure 10.1. Subsampling estimates with m= 20 subsampled datasets, for varying sub-\\nsampling ratios n/s, with an estimation of the testing error. When n/s= 1, we recover\\nthe 1-nearest neighbor classiﬁer (which overﬁts), and when n/sgrows, we get better ﬁts\\nuntil underﬁtting kicks in. Optimal testing error is obtained for n/s= 8.\\none of the beneﬁts of sketching is to be able to store a reduced rep resentation of\\nthe data ( Rs×dinstead of Rn×d).\\n•Random projection : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minη∈Rs∥y−ΦSη∥2\\n2, where\\nS∈Rd×sis a more general sketching matrix. Here, we typically have d > n >\\ns(high-dimensional situation). The beneﬁts of random projection a re twofold:\\nreduction in computation time and regularization. This corresponds to replacing\\nthe corresponding feature vectors ϕ(x)∈RdbyS⊤ϕ(x)∈Rs. We will consider\\nGaussian matrices, but also subsampling matrices, and draw connec tions with kernel\\nmethods.\\nIn the following sections, we study these precisely for the OLS fram ework (it could\\nalso be done for ridge regression). We ﬁrst brieﬂy mention a common ly used and related\\napproach.\\nRandom forests. A popular algorithm called “random forests” ( Breiman ,2001) mixes\\nboth dimension reduction by projection and bagging: decision trees are learned on a\\nbootstrapped sample of the data, while selecting a random subset o f features at every\\nsplitting decision. This algorithm has nice properties (invariance to re scaling of the\\nvariables and robustness in high dimension due to the random featur e selection) and can\\nbe extended in many ways. See Biau and Scornet (2016) for details.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 5}, page_content='290 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.1 Gaussian Sketching\\nFollowing section 3.3on OLS, we consider a design matrix Φ ∈Rn×dwith rankd(i.e.,\\nΦ⊤Φ∈Rd×dinvertible), which implies n⩾d. We consider s > d Gaussian random\\nprojections, with typically s⩽n, but this is not necessary in the analysis that follows.\\nThe estimator ˆθ(j)is obtained by using S(j)∈Rs×n, withj= 1,...,m , wherem\\ndenotes the number of replications. We then consider ˆθ=1\\nm∑m\\nj=1ˆθ(j). Whenm= 1,\\nthis is a single sketch.\\nWe will consider the same ﬁxed design assumptions as in section 3.5; that is,y=\\nΦθ∗+ε, whereε∈Rnhas independent zero-mean components with variance σ2, and\\nθ∗∈Rd. Our goal is to compute the ﬁxed design error1\\nnEε,S∥Φˆθ−Φθ∗∥2\\n2, where we take\\nexpectations with respect to both the learning problem (in the ﬁxed design setting, the\\nnoise vector ε) and the added randomization (the sketching matrices S(j),j= 1,...,m ).\\nTo compute this error, we ﬁrst need to compute expectations and variances with\\nrespect to the random projections, assuming that εis ﬁxed.\\nWe ﬁrst introduce a representation tool that will allow simple expres sions of all predic-\\ntion vectors S(j)Φ. Since the Gaussian matrices S(j)are invariant by left and right multi-\\nplication by an orthogonal matrix, we can assume that the singular v alue decomposition\\n(SVD) of Φ = UDV⊤, whereV∈Rd×dis orthogonal (i.e., V⊤V=VV⊤=I),D∈Rd×d\\nis an invertible diagonal matrix, and U∈Rn×dhas orthonormal columns (i.e., U⊤U=I;\\nremember than n⩾d), is such that U=(I\\n0)\\n, and we can write S(j)=(\\nS(j)\\n1S(j)\\n2)\\n, with\\nS(j)\\n1∈Rs×dandS(j)\\n2∈Rs×(n−d). We can also split yasy=(y1\\ny2)\\nfory1∈Rdand\\ny2∈Rn−d.\\nWe can write the normal equation that deﬁnes ˆθ(j)∈Rd, for eachj∈{1,...,m}(i.e.,\\n(Φ⊤(S(j))⊤S(j)Φ)ˆθ(j)= Φ⊤(S(j))⊤S(j)y), leading to the following closed-form estimators\\nˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\\nregarding the SVD of Φ, we have S(j)Φ =S(j)\\n1DV⊤. We can then expand the prediction\\nvector in Rnas\\nΦˆθ(j)= Φ(Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\\n=(I\\n0)\\nDV⊤(VD(S(j)\\n1)⊤S(j)\\n1DV⊤)−1VD(S(j)\\n1)⊤S(j)y\\n=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)y=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤(S(j)\\n1y1+S(j)\\n2y2)\\n=(y1+ ((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2y2\\n0)\\n.\\nThus, since E[S(j)\\n2] = 0 andS(j)\\n2is independent of S(j)\\n1, we getES(j)[\\nΦˆθ(j)]\\n=(y1\\n0)\\n, which\\nhappens to be exactly the OLS estimator Φ ˆθOLS= Φ(Φ⊤Φ)−1Φ⊤y=(I0\\n0 0)\\ny. Moreover,\\n2Ifs⩾d, thenS(j)Φ has rank dalmost surely, and thus ˆθ(j)is uniquely deﬁned.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 6}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 291\\nwe have the model y= Φθ∗+εand, if we split εasε=(ε1\\nε2)\\n, we havey=(I\\n0)\\nDV⊤θ∗+ε,\\nand thusy2=ε2. We therefore get\\nES(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=ES(j)[\\n∥((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2ε2∥2\\n2]\\n.\\nTaking the expectation with respect to ε(through E[ε2ε⊤\\n2] =σ2I), using the trace trick,\\nand using expectations for the Wishart and inverse Wishart distribu tions,3this leads to\\nEε,S(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=σ2ES(j)[\\ntr(\\n(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤S(j)\\n2)]\\n=σ2ES(j)[\\ntr(\\nS(j)\\n2(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤)]\\n= (n−d)σ2ES(j)\\n1[\\ntr(\\n((S(j)\\n1)⊤S(j)\\n1)−1)]\\n=d\\ns−d−1(n−d)σ2.\\nWe can now compute the overall expected generalization error:\\n1\\nnEε,S(j)[\\ued79\\ued79\\ued791\\nmm∑\\nj=1Φˆθ(j)−Φθ∗\\ued79\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nΦˆθ(1)]\\n−Φθ∗\\ued79\\ued792\\n2]\\n+1\\nnmEε,S(1)[\\ued79\\ued79Φˆθ(1)−ES(1)Φˆθ(1)\\ued79\\ued792]\\n=1\\nnEε[\\ued79\\ued79ΦˆθOLS−Φθ∗\\ued79\\ued792\\n2]\\n+σ2d\\nnmn−d\\ns−d−1\\n=σ2d\\nn+σ2d\\nnmn−d\\ns−d−1.\\nThus, when morstends to inﬁnity, we recover the traditional OLS behavior, while for m\\nandsﬁnite, the performance degrades gracefully. Moreover, when s=n, even form= 1,\\nwe get essentially twice the performance of the OLS estimator. We n ote that to get the\\nsame performance as OLS (up to a factor of 2), we need m=n−d\\ns−d−1∼n\\nsreplications.\\nAs in section 10.1, there is no statistical gain (here, compared to OLS), but only\\npotentially a computational one (because some computations may b e done in parallel\\nand of reduced storage). See, for example, Dobriban and Liu (2019) for other criteria\\nand sketching matrices.\\nBeyond Gaussian sketching. In this section, we have chosen a Gaussian sketching\\nmatrixS. This made the analysis simple because of the properties of the Gaus sian\\ndistribution (invariance by rotation and availability of exact expecta tions for inverse\\nWishart distributions). The analysis can be extended with more comp lex tools to other\\nrandom sketching matrices with more attractive computational pr operties, such as with\\nmany zeros, leading to subsampling observations or dimensions. See Wang et al. (2018),\\nDobriban and Liu (2019), and the references therein. For the random projections that\\nfollow, our analysis will apply to more general sketches.\\n3IfS∈Ra×bhas independent standard Gaussian components, then E[(S⊤S)−1] =1\\na−b−1Iifa > b+1,\\nandE[SS⊤] =bI; seehttps://en.wikipedia.org/wiki/Inverse-Wishart_distr ibution.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 7}, page_content='292 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.2 Random Projections\\nWe also consider the ﬁxed design setup, with a design matrix Φ ∈Rn×dand a response\\nvector of the form y= Φθ∗+ε. We now assume that d>n (high-dimensional setup) and\\nthe rank of Φ is n. In this high-dimensional setup, we need some form of regularizatio n,\\nwhich will come here from random projections.\\nFor eachj∈{1,...,n}, we consider a sketching matrix S(j)∈Rd×s, fors⩽nsampled\\nindependently from a distribution to be determined (we only assume t hat almost surely,\\nits rank is equal to s). We then consider ˆ η(j)as a minimizer of min η∈Rs∥y−ΦS(j)η∥2\\n2.\\nFor simplicity, we assume that matrix Φ S(j)has ranks, which is the case almost surely\\nfor Gaussian projections; this implies that ˆ η(j)is unique, but our result applies in all\\nsituations, as we are only interested in the denoised response vect or. We now consider\\nthe average ˆθ=1\\nm∑m\\nj=1S(j)ˆη(j).\\nWe thus consider the estimator ˆ η(j)=(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rs, ob-\\ntained from the normal equation ( S(j))⊤Φ⊤ΦS(j)ˆη(j)= (S(j))⊤Φ⊤ywith the denoised\\nresponse vector\\nˆy(j)= ΦS(j)ˆη(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rn.\\nDenoting Π(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤, it takes the form ˆ y(j)= Π(j)y.\\nMatrix Π(j)is almost surely an orthogonal projection matrix into an s-dimensional vector\\nspace, and its expectation is denoted as ∆ ∈Rn×n, which satisﬁes tr(∆) = s. We have,\\nmoreover, 0 ≼∆≼I; that is, all eigenvalues of ∆ are between 0 and 1.\\nWe can then compute expectations and variances as follows:\\nES(j)[\\nˆy(j)]\\n=ES(j)[\\nΠ(j)y]\\n= ∆y= ∆[\\nΦθ∗+ε] = ∆ε+ ∆Φθ∗\\nES(j)[\\nˆy(j)]\\n−Φθ∗= ∆ε+ [∆−I]Φθ∗ (10.1)\\nES(j)[\\ued79\\ued79ˆy(j)−ES(j)[\\nˆy(j)]\\ued79\\ued792\\n2]\\n=ES(j)[\\ued79\\ued79(Π(j)−∆)y\\ued79\\ued792\\n2]\\n=y⊤ES(j)[(\\nΠ(j)−∆)2]\\ny\\n=y⊤ES(j)[\\nΠ(j)−∆Π(j)−Π(j)∆+∆2]\\nysince Π(j)Π(j)=Π(j),\\n=y⊤(\\n∆−∆2)\\ny,sinceE[Π(j)= ∆. (10.2)\\nThus, the overall (ﬁxed design) expected generalization error is e qual to, using that\\nS(1),...,S(m)are i.i.d. matrices,\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nˆy(1)]\\n−Φθ∗\\ued79\\ued792\\n2+1\\nmES(1)[\\ued79\\ued79ˆy(1)−ES(1)[\\nˆy(1)]\\ued79\\ued792\\n2]]\\nby taking expectations with respect to all S(j),\\n=1\\nnEε[\\ued79\\ued79∆ε+ [∆−I]Φθ∗\\ued79\\ued792\\n2+1\\nmy⊤(\\n∆−∆2)\\ny]\\nusing equations ( 10.1) and ( 10.2).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 8}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 293\\nUsing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I, we get\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=σ2\\nntr(∆2) +1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗+1\\nnm[\\nσ2(tr(∆)−tr(∆2)) +θ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗]\\nusing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I,\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[∆−I]2Φθ∗+1\\nnmθ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆ +(1\\nm−1)\\n(∆−∆2)]\\nΦθ∗\\n⩽σ2s\\nn+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗,since ∆2≼∆, (10.3)\\nwhich is the value for m= 1 (single replication). Note that the expectation (before taking\\nthe bound) decreases in m, with a limitσ2tr(∆2)\\nn+1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗whenm→+∞\\n(with improved bias and variance terms). We now follow Kab´ an (2014) and Thanei et al.\\n(2017) to bound the matrix I−∆.\\nSince ∆ is the expectation of a projection matrix, we already know th at 0≼∆≼I.\\nWe omit the superscript 0(j)for clarity, and we consider Π = Φ S(\\nS⊤Φ⊤ΦS)−1S⊤Φ. For\\nany vector z∈Rn, we consider\\nz⊤(I−∆)z=ES[\\nz⊤(I−Π)z]\\n=ES[\\nz⊤z−z⊤ΦS(\\nS⊤Φ⊤ΦS)−1S⊤Φ⊤z]\\n=ES[\\nmin\\nu∈Rs∥z−ΦSu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽ES[\\nmin\\nv∈Rd∥z−ΦSS⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RdES[\\n∥z−ΦSS⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nES[\\n∥z−ΦSS⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤ΦES[\\nSS⊤]\\nv+v⊤ES[\\nSS⊤Φ⊤ΦSS⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤z,\\nz⊤(I−∆)z⩽z⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nz.\\nWe then need to apply to z= Φθ∗and get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗.\\nThus, we get an overall upper bound of\\nσ2s\\nn+1\\nnθ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 9}, page_content='294 CHAPTER 10. ENSEMBLE LEARNING\\ncomposed of expectations that can be readily computed. As shown next for special\\ncases, we obtain a bias-variance trade-oﬀ similar to equation ( 3.6) for ridge regression\\nin section 3.6, but now with random projections. Note that in the ﬁxed design set ting,\\nthere is no explosion of the testing error when s=n, as opposed to the random design\\nsetting studied in section 12.2 in the context of “double descent” (where generalization\\nto unseen inputs is required).\\nGaussian projections. If we assume Gaussian random projections, with S∈Rd×s\\nwith independent standard Gaussian components, we get, from pr operties of the Wishart\\ndistribution,4\\nES[SS⊤] =sIandES[\\nSS⊤Φ⊤ΦSS⊤]\\n=s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I.\\nWe then get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤(\\nI−s2Φ(\\ns(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I)−1Φ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤Φθ∗−sθ∗(Φ⊤Φ)2(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n=θ⊤\\n∗Φ⊤Φ(\\nΦ⊤Φ + tr(Φ⊤Φ)I)(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n⩽2 tr(Φ⊤Φ)·θ⊤\\n∗Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\nusing that Φ⊤Φ + tr(Φ⊤Φ)Ihas eigenvalues less than 2 tr(Φ⊤Φ),\\n⩽2 tr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1,\\nsince Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1has eigenvalues less than 1 /(s+ 1). The overall\\nexcess risk is then less than\\nσ2s\\nn+2\\nntr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1, (10.4)\\nwhich is exactly of the form obtained for ridge regression in equation (3.6) with the\\nidentiﬁcation s∼tr(Φ⊤Φ)\\nλ. We can consider other sketching matrices with additional\\nproperties, such as sparsity (see exercise 10.5).\\nExercise 10.5 We consider a sketching matrix S∈Rd×s, where each column is equal\\nto one of the dcanonical basis vectors of Rd, selected uniformly at random and indepen-\\ndently. Compute E[SS⊤], as well as ES[\\nSS⊤Φ⊤ΦSS⊤]\\n, as well as a bound similar to\\nequation ( 10.4).\\n4IfW=S1S⊤\\n1forS1∈Rn×swith independent standard Gaussian components, then E[W] =sIand\\nfor ann×ndiagonal matrix D, we have E[WD2W] =s(s+1)D2+str(D2)I.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 10}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 295\\nKernel methods. ( ♦)The random projection idea can be extended to kernel meth-\\nods discussed in chapter 7. We consider the kernel matrix K= ΦΦ⊤∈Rn×n, and the\\nassumption y= Φθ∗+εwith∥θ∗∥2bounded is turned into y=y∗+εwithy⊤\\n∗K−1y∗\\nbounded. This corresponds to y∗=Kα, with a reproducing kernel Hilbert space (RKHS)\\nnormα⊤Kα. We then consider a random sketch ˆΦ∈Rn×sand an approximate ker-\\nnel matrix ˆK. We then obtain an estimate ˆ y=ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y. Matrix Π is then\\nΠ = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤, and for the analysis, we need to compute its expectation, ∆ (this\\ncorresponds to replacing Φ Sin earlier developments starting at the beginning of sec-\\ntion 10.2.2 with ˆΦ). We have, following the same reasoning as before, for an arbitra ry\\ndeterministic z∈Rn,\\nz⊤(I−∆)z=EˆΦ[\\nz⊤(I−Π)z]\\n=EˆΦ[\\nz⊤z−z⊤ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤z]\\n=EˆΦ[\\nmin\\nu∈Rs∥z−ˆΦu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽EˆΦ[\\nmin\\nv∈Rn∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RnEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤EˆΦ[ˆΦˆΦ⊤]\\nv+v⊤EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤]\\nz,\\nz⊤(I−∆)z⩽z⊤(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\nz. (10.5)\\nWe then need to apply equation ( 10.5) toz=y∗to get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽y⊤\\n∗(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\ny∗.\\nWe can, for example, consider each column of ˆΦ to be sampled from a Gaussian distri-\\nbution with mean zero and covariance matrix K, for which we have\\nEˆΦ[ˆΦˆΦ⊤]\\n=sKandEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n=s(s+ 1)K2+str(K)·K.\\nUsing equation ( 10.3), with the same derivations that led to equation ( 10.4), this leads\\nto a bound on the expected excess risk equal toσ2s\\nn+2\\nntr(K)y⊤\\n∗K−1y∗\\ns+1, which is exactly\\nthe bound in equation ( 10.4) in the kernel context. However, it is not interesting in\\npractice, as it requires the computation of the kernel matrix Kand typically a square\\nroot to sample from the multivariate Gaussian distribution, which has a running-time\\ncomplexity of O(n3).\\nIn practice, many kernels come with a random feature expansion of the formk(x,x′) =\\nEv[\\nϕ(x,v)ϕ(x′,v)]\\n, such that|ϕ(x,v)|⩽Ralmost surely (as presented in section 7.4).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 11}, page_content='296 CHAPTER 10. ENSEMBLE LEARNING\\nWe can take for each column of ˆΦ the vector ( ϕ(x1,v),...,ϕ (xn,v))⊤∈Rnfor a random\\nindependent v. Then we have E[ˆΦˆΦ⊤]\\n=sKby construction, while a short calculation\\n(left as an exercise) shows that the second-order moment can be bounded as\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n≼s(s−1)K2+sR2K.\\nThis leads to the boundσ2s\\nn+1\\nnR2y⊤\\n∗K−1y∗\\ns−1, which is almost the same as before, but with\\nan eﬃcient practical algorithm (since we now have to solve a least-sq uares regression\\nproblem in dimension s, which is more eﬃcient than using the kernel trick if s<n ).\\nExperiments. In ﬁgure 10.2, we consider a polynomial regression problem in dimen-\\nsiondX= 20, with polynomials of a maximum degree of 2, and thus a feature sp ace\\nof dimension d= 1 +dX+dX(dX+ 1)/2 = 231. We also compare ridge regression with\\nGaussian random projections. We see better performance as mgrows, consistent with our\\nbounds (underﬁtting for small s, overﬁtting with large s). Moreover, when the number\\nmof times the dataset is randomly projected goes from 10 to 100, we obtain almost the\\nsame plot, with better performance than m= 1, highlighting the fact that m= 1 is not\\noptimal but taking mtoo large is not useful.\\nJohnson-Lindenstrauss lemma ( ♦).A related classical result in Gaussian random\\nprojections shows that nfeature vectors ϕ1,...,ϕn∈Rdcan be well represented in di-\\nmensionsby Gaussian random projections, with sgrowing only logarithmically in n, and\\nindependent of the underlying dimension. Lemma 10.1 shows that all pairwise distances\\nare preserved (a small modiﬁcation would lead to the preservation o f all dot products).\\nLemma 10.1 ( Johnson and Lindenstrauss ,1984)Givenϕ1,...,ϕn∈Rd, letS∈\\nRd×sbe a random matrix with independent standard Gaussian rando m variables. Then,\\nfor anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\\nε2logn2\\nδ, with probability greater than 1−δ,\\nwe have\\n∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\\n2⩽\\ued79\\ued79s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\\n2⩽(1+ε)∥ϕi−ϕj∥2\\n2.\\n(10.6)\\nProof (♦) Letψ∈Rdwith theℓ2-norm equal to 1. The random variable y=ψ⊤SS⊤ψ\\nis the sum of srandom variables ψ⊤S·jS⊤\\n·jψ, forS·jthejth column of S,j∈{1,...,s}.\\nEach of these is the square of S⊤\\n·jψ, which is Gaussian with mean zero and variance equal\\nto∥ψ∥2\\n2= 1. Thus, yis a chi-squared random variable.5We can thus apply concentration\\nresults from exercise 8.1, leading to\\nP(\\n|y−s|⩾sε)\\n⩽(1−ε\\nexp(−ε))s/2\\n+(1 +ε\\nexp(ε))s/2\\n.\\nWe can then use the inequality log(1 + u)⩽u−u2\\n3for any|u|⩽1\\n2, applied to εand\\n−ε, leading to the probability bound P(\\n|y−s|⩾sε)\\n⩽2 exp(\\n−s\\n2ε2\\n3)\\n.We then apply\\n5Seehttps://en.wikipedia.org/wiki/Chi-squared_distribut ion.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 12}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 297\\nFigure 10.2. Polynomial regression in dimension 20, with polynomials of a maximum\\ndegree of 2, with n= 100. Top left: training and testing errors for ridge regression in t he\\nﬁxed design setting (the input data are ﬁxed, and only the noise var iables are resampled\\nfor computing the test error). All other plots: training and testin g errors for Gaussian\\nrandom projections, with diﬀerent numbers of random projection s:m= 1 (top right),\\nm= 10 (bottom left), and m= 100 (bottom right). All the curves are averaged over 100\\nreplications of the noise variables and the random projections.\\nthis bound for ψbeing then(n−1)/2 vectorsϕi−ϕj, fori̸=j, leading to, using a\\nunion bound, a probability that equation ( 10.6) is not satisﬁed with a probability less\\nthann2exp(−sε2/6),leading to the desired result.\\nIn our context of least-squares regression, the Johnson-Linde nstrauss lemma shows that\\nthe kernel matrix is preserved by random projections so that pre dictions with the pro-\\njected data should be close to predictions with the original data. Th e results in this\\nsection provide a direct proof that aims to characterize directly th e predictive perfor-\\nmance of such random projections (using the Johnson-Lindenstr auss lemma to obtain\\nsimilar bounds is not straightforward as we consider unregularized r egression, where per-\\nturbations of matrix inverses are harder to control).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 13}, page_content='298 CHAPTER 10. ENSEMBLE LEARNING\\n10.3 Boosting\\nIn sections 10.1 and10.2, we focused on uniformly combining the outputs (e.g., plain av-\\neraging) of estimators obtained by randomly reweighted versions o f the original datasets.\\nReweighting was performed independent of the performance of th e resulting prediction\\nfunctions, and the training procedures for all predictors could be done in parallel. In this\\nsection, we explore sequential reweightings of the training datasets that depend on the\\nmistakes made by the current prediction functions. While the natur al parallelizability is\\nlost, we will see that we get additional statistical beneﬁts.\\nIn the early boosting procedures adapted to binary classiﬁcation, the original learn-\\ning algorithms (going from datasets to prediction functions with bina ry values) were used\\ndirectly on a reweighted version, such as Adaboost (see, e.g., Freund et al. ,1999). Our\\nanalysis will be carried out for boosting procedures, often referr ed to as “gradient boost-\\ning,” which are adapted to real-valued outputs, as done in the rest o f this book (noting\\nthat for classiﬁcation, we can use convex surrogates).\\nThe theory of boosting is rich, with many connections, and in this sec tion, we only\\nprovide a consistency proof in the simplest setting. See Schapire and Freund (2012) for\\nmore details.\\n10.3.1 Problem Setup\\nGiven an input space Xandnobservations ( xi,yi)∈X×R,i= 1,...,n , we are given a\\nset of predictors ϕ(·,w) :X→R, forw∈W, withWtypically being a compact subset of\\na ﬁnite-dimensional vector space.\\nThe main assumption is that given weights α∈Rn, one can reasonably easily ﬁnd\\nthe function ϕ(·,w) that minimizes with respect to w∈W:\\nn∑\\ni=1αiϕ(xi,w); (10.7)\\nthat is, the dot product between αand thenoutputs ofϕ(·,w) on thenobservations. In\\nthis section, for simplicity, we assume that this minimization can be don e exactly. This\\nis often referred to as the “weak learner” assumption. Many exam ples are available, such\\nas the following:\\n•Linear stumps for X=Rd:ϕ(x,w) =±(w⊤\\n0x+w1)+, wherew= (w0,w1)∈Rd×R,\\nwith sometimes the restriction that whas a single nonzero component (where the\\nweak learning tractability assumption is indeed veriﬁed; see exercise 10.6). This\\nwill lead to a predictor, which is a one-hidden-layer neural network a s presented\\nin chapter 9, but learned sequentially rather than by GD on the empirical risk. In\\nthe context of binary classiﬁcation, the weak learners are sometim es thresholded to\\nvalues in{−1,1}by taking their signs.\\nExercise 10.6 For linear stumps with only one nonzero coordinate for the sl ope,\\nshow how to minimize equation ( 10.7) eﬃciently.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 0}, page_content='10.1. AVERAGING/BAGGING 285\\nized situation, we have mindependent datasets of size n, composed of independent and\\nidentically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\\nWe obtain for each of them an estimator ˆf(j)\\nλ, wherej∈{1,...,m}andλis an associ-\\nated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\\nregression, the new predictor, ˆfbag\\nλ, is simply the average of all ˆf(j)\\nλ,j= 1,...,m .\\nIf we denote bias(j)(x) =E[ˆf(j)\\nλ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\\nλ(x)]\\n(assuming\\nthatxis ﬁxed and only taking expectations with respect to the data), the n they are the\\nsame for all j∈{1,...,m}and the bias of ˆfbag\\nλis the same as the base bias for a single\\ndataset (and thus so is the squared bias). At the same time, the va riance is divided by m\\nbecause the datasets are assumed to be independent.\\nThus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\\na higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\\nindependent datasets, it is relevant to concatenate all mdatasets into one large dataset\\nwithN=nmobservations and learn a single predictor with these: the generaliza tion\\nperformance of the average of mpredictors will often be the same as the one of the single\\npredictor on the large dataset, but with potential computational beneﬁts. We now give a\\nfew examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\\nThek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\\nprediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\\nbias was upper-bounded by 8 B2diam(X)2(2k\\nn)2/d(ford⩾2). At the same time, the\\nvariance was bounded byσ2\\nk, whereσ2is a bound on the noise variance on top of the\\ntarget function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\\nreplications, we get an excess risk upper-bounded by\\nσ2\\nkm+ 8B2diam(X)2(2k\\nn)2/d\\n.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 0}, page_content='target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\\nreplications, we get an excess risk upper-bounded by\\nσ2\\nkm+ 8B2diam(X)2(2k\\nn)2/d\\n.\\nWhen optimizing this bound with respect to k, we get that k1+2/d∝n2/d\\nm, leading to\\nk∝1\\nmd/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\\n(which is consistent with favoring higher variance estimators). The overall excess risk ends\\nup being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\\nobservations.\\nThus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\\nestimating independently, and combining linearly does not lead to an ov erall improved\\nstatistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\\nputational advantages when the mestimators can be computed in parallel (and totally\\nindependently). We thus obtain a distributed algorithm with the same worst-case pre-\\ndictive performance as for a single machine.\\nNote here that there is an upper bound on the number of replication s (and thus the\\nability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\\nand thus,mcannot grow larger than n2/d.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 1}, page_content='286 CHAPTER 10. ENSEMBLE LEARNING\\nExercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\\njority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\\nsection4.1.4to derive an upper bound on the expected risk, what is the corr esponding\\noptimal choice of mwhen using independent datasets?\\nRidge regression. Following the analysis from section 7.6.6, the variance of the ridge\\nregression estimator was proportional toσ2\\nnλ−1/αand the bias proportional to λt/s(see\\nprecise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\\nproportional toσ2\\nnmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\\nobservations (and the same regularization parameter). Again, wit h the proper choice\\nof regularization parameter (lower λthan for the full dataset), there is no statistical\\nadvantage. Still, there may be a computational one, not only for pa rallel processing but\\nalso with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\\ntime for ridge regression is superlinear in the number of observation s with running-time\\ncomplexities between O(n2) andO(n3) if no low-rank approximations are used.\\nExercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\\ntime complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\\nsplit of the data into mchunks? What is the optimal value of m?\\nExercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\\nan estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\\nproblem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\\nj=1ˆf(j)(x))\\nthe majority vote clas-\\nsiﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\\ntion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\\nE[1ˆfbag(x)̸=f∗(x)]⩽exp(\\n−m\\n2(ε(x))2\\n+)\\n.If∀x∈X,ε(x)⩾η >0, show that the expected\\nexcess risk is less than exp(\\n−m\\n2η2)\\n.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 1}, page_content='E[1ˆfbag(x)̸=f∗(x)]⩽exp(\\n−m\\n2(ε(x))2\\n+)\\n.If∀x∈X,ε(x)⩾η >0, show that the expected\\nexcess risk is less than exp(\\n−m\\n2η2)\\n.\\nBeyond independent datasets. Having independent datasets may not be possible,\\nand one typically needs to artiﬁcially create such replicated dataset s from a single one,\\nwhich is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\\nstill, but this time with a potentially higher bias.\\n10.1.2 Bagging\\nWe consider datasets D(b), obtained with random weights v(b)\\ni∈R+,i= 1,...,n . For\\nthe bootstrap,1we consider nsamples from the original ndata points with replacement,\\nwhich correspond to integer weights v(b)\\ni∈N,i= 1,...,n , that sum to n. Such sets of\\nweights are sampled independently mtimes. We study m=∞for simplicity; that is,\\ninﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\\nmoderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\\n1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\\n(1994) for an introduction to bootstrapping methods in statistic s.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 2}, page_content='10.1. AVERAGING/BAGGING 287\\nwhich is important for highly variable predictors (which usually imply a lar ge estimation\\nvariance).\\nFor linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\\nsquare loss, such as kernel ridge regression or local averaging, t his leads to another linear\\nestimator. Therefore, this provides alternative ways of regulariz ing, which typically may\\nnot provide a strong statistical gain over existing methods but pro vide a computational\\ngain, in particular when each estimator is very eﬃcient to compute (s ee related examples\\nin section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\\nvariance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\\nlarizing methods. See also the end of section 10.2 for a short description of “random\\nforests,” which is also partially based on bagging.\\nFor simplicity, we will consider averaging estimators obtained by rand omly selecting s\\nobservations from the navailable ones, doing this many times (inﬁnitely many for the\\nanalysis), and averaging the predictions.\\nExercise 10.4 Show that when sampling nelements with replacement from nitems, the\\nexpected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\\nto inﬁnity.\\nOne-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\\nwhere the strong eﬀect of bagging is striking. The analysis in this sub section follows\\nfrom Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\\nthe pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\\n(ignoring ties), then we can write the bagged estimate as\\nˆf(x) =n∑\\ni=1Viy(i)(x),\\nwhere the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\\nprobability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\\nsubsample of size s. We consider sampling without replacement and leave sampling with'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 2}, page_content='probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\\nsubsample of size s. We consider sampling without replacement and leave sampling with\\nreplacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\\nTo select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\\nthat theith-nearest neighbor is selected but none of the closer neighbors, which leaves\\ns−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\\nVi= 0, while otherwise Vi=(n\\ns)−1(n−i\\ns−1)\\n, as the total number of subsets of size sis(n\\ns)\\n,\\nand there are(n−i\\ns−1)\\nrelevant ones.\\nWe can now use the reasoning from section 6.3.2. Since for any x, the weights given\\nto each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\\nvariance term is equal to∑n\\ni=1V2\\ni. To obtain a bound, we note that for i⩽n−s+ 1,\\nVi=s\\nn−s+ 1∏s−2\\nj=0(n−i−j)\\n∏s−2\\nj=0(n−j)=s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn−j)\\n⩽s\\nn−s+ 1s−2∏\\nj=0(\\n1−i\\nn)\\n,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 3}, page_content='288 CHAPTER 10. ENSEMBLE LEARNING\\nleading to, upper-bounding the sum by an integral,\\nn∑\\ni=1V2\\ni⩽s2\\n(n−s+ 1)2n∑\\ni=1(\\n1−i\\nn)2(s−1)⩽ns2\\n(n−s+ 1)2∫1\\n0(1−t)2(s−1)dt\\n⩽ns2\\n(n−s+ 1)21\\n2s−1⩽ns\\n(n−s+ 1)2=s\\nn1\\n(1 + 1/n−s/n)2.\\nFor the bias term, we need to bound∑n\\ni=1Vi·E[\\n∥x−x(i)(x)∥2]\\n, where the expectation\\nis with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\\nconditioning on the data and x, this isB2multiplied by the expectation of the distance\\nto the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\\non a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\\ns2/difd⩾2 (which we now\\nassume).\\nThus, the overall excess risk is less than\\n4B2diam(X)21\\ns2/d+s\\nn1\\n(1 + 1/n−s/n)2,\\nwhich we can balance by choosing s1+2/d∝n, leading to the same performance as the\\nk-nearest neighbor for a well-chosen k, but now with a bagged esimate.\\nIn ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\\neﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\\nneighbor estimate, and when sdecreases, the variance indeed decreases while the bias\\nincreases.\\n10.2 Random Projections and Averaging\\nIn section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\\ncan also be done through random projections of all observations. Such random projections\\ncan be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\\n(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\\nand (3) for kernel methods, using random features such as pres ented in section 7.4. Such\\nrandom projections can also reduce the number of samples while kee ping the dimension\\nﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\\nreduced size).\\nIn this section, we consider random projections for ordinary least -squares (OLS), with\\nthe same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 3}, page_content='reduced size).\\nIn this section, we consider random projections for ordinary least -squares (OLS), with\\nthe same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\\ndesign matrix, in two settings:\\n•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minθ∈Rd∥Sy−SΦθ∥2\\n2, whereS∈Rs×n\\nis an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\\nments). This is an idealization of subsampling done in the previous sect ion. Here,\\nwe typically have n > s > d (more observations than the feature dimension), and'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 4}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 289\\nFigure 10.1. Subsampling estimates with m= 20 subsampled datasets, for varying sub-\\nsampling ratios n/s, with an estimation of the testing error. When n/s= 1, we recover\\nthe 1-nearest neighbor classiﬁer (which overﬁts), and when n/sgrows, we get better ﬁts\\nuntil underﬁtting kicks in. Optimal testing error is obtained for n/s= 8.\\none of the beneﬁts of sketching is to be able to store a reduced rep resentation of\\nthe data ( Rs×dinstead of Rn×d).\\n•Random projection : Replacing min θ∈Rd∥y−Φθ∥2\\n2by minη∈Rs∥y−ΦSη∥2\\n2, where\\nS∈Rd×sis a more general sketching matrix. Here, we typically have d > n >\\ns(high-dimensional situation). The beneﬁts of random projection a re twofold:\\nreduction in computation time and regularization. This corresponds to replacing\\nthe corresponding feature vectors ϕ(x)∈RdbyS⊤ϕ(x)∈Rs. We will consider\\nGaussian matrices, but also subsampling matrices, and draw connec tions with kernel\\nmethods.\\nIn the following sections, we study these precisely for the OLS fram ework (it could\\nalso be done for ridge regression). We ﬁrst brieﬂy mention a common ly used and related\\napproach.\\nRandom forests. A popular algorithm called “random forests” ( Breiman ,2001) mixes\\nboth dimension reduction by projection and bagging: decision trees are learned on a\\nbootstrapped sample of the data, while selecting a random subset o f features at every\\nsplitting decision. This algorithm has nice properties (invariance to re scaling of the\\nvariables and robustness in high dimension due to the random featur e selection) and can\\nbe extended in many ways. See Biau and Scornet (2016) for details.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 5}, page_content='290 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.1 Gaussian Sketching\\nFollowing section 3.3on OLS, we consider a design matrix Φ ∈Rn×dwith rankd(i.e.,\\nΦ⊤Φ∈Rd×dinvertible), which implies n⩾d. We consider s > d Gaussian random\\nprojections, with typically s⩽n, but this is not necessary in the analysis that follows.\\nThe estimator ˆθ(j)is obtained by using S(j)∈Rs×n, withj= 1,...,m , wherem\\ndenotes the number of replications. We then consider ˆθ=1\\nm∑m\\nj=1ˆθ(j). Whenm= 1,\\nthis is a single sketch.\\nWe will consider the same ﬁxed design assumptions as in section 3.5; that is,y=\\nΦθ∗+ε, whereε∈Rnhas independent zero-mean components with variance σ2, and\\nθ∗∈Rd. Our goal is to compute the ﬁxed design error1\\nnEε,S∥Φˆθ−Φθ∗∥2\\n2, where we take\\nexpectations with respect to both the learning problem (in the ﬁxed design setting, the\\nnoise vector ε) and the added randomization (the sketching matrices S(j),j= 1,...,m ).\\nTo compute this error, we ﬁrst need to compute expectations and variances with\\nrespect to the random projections, assuming that εis ﬁxed.\\nWe ﬁrst introduce a representation tool that will allow simple expres sions of all predic-\\ntion vectors S(j)Φ. Since the Gaussian matrices S(j)are invariant by left and right multi-\\nplication by an orthogonal matrix, we can assume that the singular v alue decomposition\\n(SVD) of Φ = UDV⊤, whereV∈Rd×dis orthogonal (i.e., V⊤V=VV⊤=I),D∈Rd×d\\nis an invertible diagonal matrix, and U∈Rn×dhas orthonormal columns (i.e., U⊤U=I;\\nremember than n⩾d), is such that U=(I\\n0)\\n, and we can write S(j)=(\\nS(j)\\n1S(j)\\n2)\\n, with\\nS(j)\\n1∈Rs×dandS(j)\\n2∈Rs×(n−d). We can also split yasy=(y1\\ny2)\\nfory1∈Rdand\\ny2∈Rn−d.\\nWe can write the normal equation that deﬁnes ˆθ(j)∈Rd, for eachj∈{1,...,m}(i.e.,\\n(Φ⊤(S(j))⊤S(j)Φ)ˆθ(j)= Φ⊤(S(j))⊤S(j)y), leading to the following closed-form estimators\\nˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\\nregarding the SVD of Φ, we have S(j)Φ =S(j)\\n1DV⊤. We can then expand the prediction\\nvector in Rnas'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 5}, page_content='ˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\\nregarding the SVD of Φ, we have S(j)Φ =S(j)\\n1DV⊤. We can then expand the prediction\\nvector in Rnas\\nΦˆθ(j)= Φ(Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\\n=(I\\n0)\\nDV⊤(VD(S(j)\\n1)⊤S(j)\\n1DV⊤)−1VD(S(j)\\n1)⊤S(j)y\\n=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)y=(I\\n0)\\n((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤(S(j)\\n1y1+S(j)\\n2y2)\\n=(y1+ ((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2y2\\n0)\\n.\\nThus, since E[S(j)\\n2] = 0 andS(j)\\n2is independent of S(j)\\n1, we getES(j)[\\nΦˆθ(j)]\\n=(y1\\n0)\\n, which\\nhappens to be exactly the OLS estimator Φ ˆθOLS= Φ(Φ⊤Φ)−1Φ⊤y=(I0\\n0 0)\\ny. Moreover,\\n2Ifs⩾d, thenS(j)Φ has rank dalmost surely, and thus ˆθ(j)is uniquely deﬁned.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 6}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 291\\nwe have the model y= Φθ∗+εand, if we split εasε=(ε1\\nε2)\\n, we havey=(I\\n0)\\nDV⊤θ∗+ε,\\nand thusy2=ε2. We therefore get\\nES(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=ES(j)[\\n∥((S(j)\\n1)⊤S(j)\\n1)−1(S(j)\\n1)⊤S(j)\\n2ε2∥2\\n2]\\n.\\nTaking the expectation with respect to ε(through E[ε2ε⊤\\n2] =σ2I), using the trace trick,\\nand using expectations for the Wishart and inverse Wishart distribu tions,3this leads to\\nEε,S(j)[\\ued79\\ued79Φˆθ(j)−ES(j)Φˆθ(j)\\ued79\\ued792\\n2]\\n=σ2ES(j)[\\ntr(\\n(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤S(j)\\n2)]\\n=σ2ES(j)[\\ntr(\\nS(j)\\n2(S(j)\\n2)⊤S(j)\\n1((S(j)\\n1)⊤S(j)\\n1)−2(S(j)\\n1)⊤)]\\n= (n−d)σ2ES(j)\\n1[\\ntr(\\n((S(j)\\n1)⊤S(j)\\n1)−1)]\\n=d\\ns−d−1(n−d)σ2.\\nWe can now compute the overall expected generalization error:\\n1\\nnEε,S(j)[\\ued79\\ued79\\ued791\\nmm∑\\nj=1Φˆθ(j)−Φθ∗\\ued79\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nΦˆθ(1)]\\n−Φθ∗\\ued79\\ued792\\n2]\\n+1\\nnmEε,S(1)[\\ued79\\ued79Φˆθ(1)−ES(1)Φˆθ(1)\\ued79\\ued792]\\n=1\\nnEε[\\ued79\\ued79ΦˆθOLS−Φθ∗\\ued79\\ued792\\n2]\\n+σ2d\\nnmn−d\\ns−d−1\\n=σ2d\\nn+σ2d\\nnmn−d\\ns−d−1.\\nThus, when morstends to inﬁnity, we recover the traditional OLS behavior, while for m\\nandsﬁnite, the performance degrades gracefully. Moreover, when s=n, even form= 1,\\nwe get essentially twice the performance of the OLS estimator. We n ote that to get the\\nsame performance as OLS (up to a factor of 2), we need m=n−d\\ns−d−1∼n\\nsreplications.\\nAs in section 10.1, there is no statistical gain (here, compared to OLS), but only\\npotentially a computational one (because some computations may b e done in parallel\\nand of reduced storage). See, for example, Dobriban and Liu (2019) for other criteria\\nand sketching matrices.\\nBeyond Gaussian sketching. In this section, we have chosen a Gaussian sketching\\nmatrixS. This made the analysis simple because of the properties of the Gaus sian\\ndistribution (invariance by rotation and availability of exact expecta tions for inverse\\nWishart distributions). The analysis can be extended with more comp lex tools to other\\nrandom sketching matrices with more attractive computational pr operties, such as with'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 6}, page_content='Wishart distributions). The analysis can be extended with more comp lex tools to other\\nrandom sketching matrices with more attractive computational pr operties, such as with\\nmany zeros, leading to subsampling observations or dimensions. See Wang et al. (2018),\\nDobriban and Liu (2019), and the references therein. For the random projections that\\nfollow, our analysis will apply to more general sketches.\\n3IfS∈Ra×bhas independent standard Gaussian components, then E[(S⊤S)−1] =1\\na−b−1Iifa > b+1,\\nandE[SS⊤] =bI; seehttps://en.wikipedia.org/wiki/Inverse-Wishart_distr ibution.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 7}, page_content='292 CHAPTER 10. ENSEMBLE LEARNING\\n10.2.2 Random Projections\\nWe also consider the ﬁxed design setup, with a design matrix Φ ∈Rn×dand a response\\nvector of the form y= Φθ∗+ε. We now assume that d>n (high-dimensional setup) and\\nthe rank of Φ is n. In this high-dimensional setup, we need some form of regularizatio n,\\nwhich will come here from random projections.\\nFor eachj∈{1,...,n}, we consider a sketching matrix S(j)∈Rd×s, fors⩽nsampled\\nindependently from a distribution to be determined (we only assume t hat almost surely,\\nits rank is equal to s). We then consider ˆ η(j)as a minimizer of min η∈Rs∥y−ΦS(j)η∥2\\n2.\\nFor simplicity, we assume that matrix Φ S(j)has ranks, which is the case almost surely\\nfor Gaussian projections; this implies that ˆ η(j)is unique, but our result applies in all\\nsituations, as we are only interested in the denoised response vect or. We now consider\\nthe average ˆθ=1\\nm∑m\\nj=1S(j)ˆη(j).\\nWe thus consider the estimator ˆ η(j)=(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rs, ob-\\ntained from the normal equation ( S(j))⊤Φ⊤ΦS(j)ˆη(j)= (S(j))⊤Φ⊤ywith the denoised\\nresponse vector\\nˆy(j)= ΦS(j)ˆη(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rn.\\nDenoting Π(j)= ΦS(j)(\\n(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤, it takes the form ˆ y(j)= Π(j)y.\\nMatrix Π(j)is almost surely an orthogonal projection matrix into an s-dimensional vector\\nspace, and its expectation is denoted as ∆ ∈Rn×n, which satisﬁes tr(∆) = s. We have,\\nmoreover, 0 ≼∆≼I; that is, all eigenvalues of ∆ are between 0 and 1.\\nWe can then compute expectations and variances as follows:\\nES(j)[\\nˆy(j)]\\n=ES(j)[\\nΠ(j)y]\\n= ∆y= ∆[\\nΦθ∗+ε] = ∆ε+ ∆Φθ∗\\nES(j)[\\nˆy(j)]\\n−Φθ∗= ∆ε+ [∆−I]Φθ∗ (10.1)\\nES(j)[\\ued79\\ued79ˆy(j)−ES(j)[\\nˆy(j)]\\ued79\\ued792\\n2]\\n=ES(j)[\\ued79\\ued79(Π(j)−∆)y\\ued79\\ued792\\n2]\\n=y⊤ES(j)[(\\nΠ(j)−∆)2]\\ny\\n=y⊤ES(j)[\\nΠ(j)−∆Π(j)−Π(j)∆+∆2]\\nysince Π(j)Π(j)=Π(j),\\n=y⊤(\\n∆−∆2)\\ny,sinceE[Π(j)= ∆. (10.2)\\nThus, the overall (ﬁxed design) expected generalization error is e qual to, using that\\nS(1),...,S(m)are i.i.d. matrices,\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nˆy(1)]\\n−Φθ∗\\ued79\\ued792\\n2+1'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 7}, page_content='Thus, the overall (ﬁxed design) expected generalization error is e qual to, using that\\nS(1),...,S(m)are i.i.d. matrices,\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=1\\nnEε[\\ued79\\ued79ES(1)[\\nˆy(1)]\\n−Φθ∗\\ued79\\ued792\\n2+1\\nmES(1)[\\ued79\\ued79ˆy(1)−ES(1)[\\nˆy(1)]\\ued79\\ued792\\n2]]\\nby taking expectations with respect to all S(j),\\n=1\\nnEε[\\ued79\\ued79∆ε+ [∆−I]Φθ∗\\ued79\\ued792\\n2+1\\nmy⊤(\\n∆−∆2)\\ny]\\nusing equations ( 10.1) and ( 10.2).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 8}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 293\\nUsing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I, we get\\n1\\nnEε,S[\\ued79\\ued791\\nm∑m\\nj=1ˆy(j)−Φθ∗\\ued79\\ued792\\n2]\\n=σ2\\nntr(∆2) +1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗+1\\nnm[\\nσ2(tr(∆)−tr(∆2)) +θ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗]\\nusing the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I,\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[∆−I]2Φθ∗+1\\nnmθ⊤\\n∗Φ⊤(\\n∆−∆2)\\nΦθ∗\\n=σ2\\nn(\\n1−1\\nm)\\ntr(∆2) +σ2s\\nnm+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆ +(1\\nm−1)\\n(∆−∆2)]\\nΦθ∗\\n⩽σ2s\\nn+1\\nnθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗,since ∆2≼∆, (10.3)\\nwhich is the value for m= 1 (single replication). Note that the expectation (before taking\\nthe bound) decreases in m, with a limitσ2tr(∆2)\\nn+1\\nnθ⊤\\n∗Φ⊤[I−∆]2Φθ∗whenm→+∞\\n(with improved bias and variance terms). We now follow Kab´ an (2014) and Thanei et al.\\n(2017) to bound the matrix I−∆.\\nSince ∆ is the expectation of a projection matrix, we already know th at 0≼∆≼I.\\nWe omit the superscript 0(j)for clarity, and we consider Π = Φ S(\\nS⊤Φ⊤ΦS)−1S⊤Φ. For\\nany vector z∈Rn, we consider\\nz⊤(I−∆)z=ES[\\nz⊤(I−Π)z]\\n=ES[\\nz⊤z−z⊤ΦS(\\nS⊤Φ⊤ΦS)−1S⊤Φ⊤z]\\n=ES[\\nmin\\nu∈Rs∥z−ΦSu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽ES[\\nmin\\nv∈Rd∥z−ΦSS⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RdES[\\n∥z−ΦSS⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nES[\\n∥z−ΦSS⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤ΦES[\\nSS⊤]\\nv+v⊤ES[\\nSS⊤Φ⊤ΦSS⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤z,\\nz⊤(I−∆)z⩽z⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nz.\\nWe then need to apply to z= Φθ∗and get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗.\\nThus, we get an overall upper bound of\\nσ2s\\nn+1\\nnθ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗,'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 9}, page_content='294 CHAPTER 10. ENSEMBLE LEARNING\\ncomposed of expectations that can be readily computed. As shown next for special\\ncases, we obtain a bias-variance trade-oﬀ similar to equation ( 3.6) for ridge regression\\nin section 3.6, but now with random projections. Note that in the ﬁxed design set ting,\\nthere is no explosion of the testing error when s=n, as opposed to the random design\\nsetting studied in section 12.2 in the context of “double descent” (where generalization\\nto unseen inputs is required).\\nGaussian projections. If we assume Gaussian random projections, with S∈Rd×s\\nwith independent standard Gaussian components, we get, from pr operties of the Wishart\\ndistribution,4\\nES[SS⊤] =sIandES[\\nSS⊤Φ⊤ΦSS⊤]\\n=s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I.\\nWe then get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽θ⊤\\n∗Φ⊤(\\nI−ΦES[\\nSS⊤](\\nES[\\nSS⊤Φ⊤ΦSS⊤])−1ES[\\nSS⊤]\\nΦ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤(\\nI−s2Φ(\\ns(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I)−1Φ⊤)\\nΦθ∗\\n=θ⊤\\n∗Φ⊤Φθ∗−sθ∗(Φ⊤Φ)2(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n=θ⊤\\n∗Φ⊤Φ(\\nΦ⊤Φ + tr(Φ⊤Φ)I)(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\n⩽2 tr(Φ⊤Φ)·θ⊤\\n∗Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\\nusing that Φ⊤Φ + tr(Φ⊤Φ)Ihas eigenvalues less than 2 tr(Φ⊤Φ),\\n⩽2 tr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1,\\nsince Φ⊤Φ(\\n(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1has eigenvalues less than 1 /(s+ 1). The overall\\nexcess risk is then less than\\nσ2s\\nn+2\\nntr(Φ⊤Φ)∥θ∗∥2\\n2\\ns+ 1, (10.4)\\nwhich is exactly of the form obtained for ridge regression in equation (3.6) with the\\nidentiﬁcation s∼tr(Φ⊤Φ)\\nλ. We can consider other sketching matrices with additional\\nproperties, such as sparsity (see exercise 10.5).\\nExercise 10.5 We consider a sketching matrix S∈Rd×s, where each column is equal\\nto one of the dcanonical basis vectors of Rd, selected uniformly at random and indepen-\\ndently. Compute E[SS⊤], as well as ES[\\nSS⊤Φ⊤ΦSS⊤]\\n, as well as a bound similar to\\nequation ( 10.4).\\n4IfW=S1S⊤\\n1forS1∈Rn×swith independent standard Gaussian components, then E[W] =sIand\\nfor ann×ndiagonal matrix D, we have E[WD2W] =s(s+1)D2+str(D2)I.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 10}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 295\\nKernel methods. ( ♦)The random projection idea can be extended to kernel meth-\\nods discussed in chapter 7. We consider the kernel matrix K= ΦΦ⊤∈Rn×n, and the\\nassumption y= Φθ∗+εwith∥θ∗∥2bounded is turned into y=y∗+εwithy⊤\\n∗K−1y∗\\nbounded. This corresponds to y∗=Kα, with a reproducing kernel Hilbert space (RKHS)\\nnormα⊤Kα. We then consider a random sketch ˆΦ∈Rn×sand an approximate ker-\\nnel matrix ˆK. We then obtain an estimate ˆ y=ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y. Matrix Π is then\\nΠ = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤, and for the analysis, we need to compute its expectation, ∆ (this\\ncorresponds to replacing Φ Sin earlier developments starting at the beginning of sec-\\ntion 10.2.2 with ˆΦ). We have, following the same reasoning as before, for an arbitra ry\\ndeterministic z∈Rn,\\nz⊤(I−∆)z=EˆΦ[\\nz⊤(I−Π)z]\\n=EˆΦ[\\nz⊤z−z⊤ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤z]\\n=EˆΦ[\\nmin\\nu∈Rs∥z−ˆΦu∥2\\n2]\\nby deﬁnition of projections ,\\n⩽EˆΦ[\\nmin\\nv∈Rn∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby minimizing over a smaller subspace,\\n⩽min\\nv∈RnEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\nby properties of the expectation.\\nWe can expand this to get\\nEˆΦ[\\n∥z−ˆΦˆΦ⊤v∥2\\n2]\\n=∥z∥2\\n2−2z⊤EˆΦ[ˆΦˆΦ⊤]\\nv+v⊤EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\nv,\\nleading to, after selecting the optimal vasv=(\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤]\\nz,\\nz⊤(I−∆)z⩽z⊤(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\nz. (10.5)\\nWe then need to apply equation ( 10.5) toz=y∗to get\\nθ⊤\\n∗Φ⊤[\\nI−∆]\\nΦθ∗⩽y⊤\\n∗(\\nI−EˆΦ[ˆΦˆΦ⊤](\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\\ny∗.\\nWe can, for example, consider each column of ˆΦ to be sampled from a Gaussian distri-\\nbution with mean zero and covariance matrix K, for which we have\\nEˆΦ[ˆΦˆΦ⊤]\\n=sKandEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n=s(s+ 1)K2+str(K)·K.\\nUsing equation ( 10.3), with the same derivations that led to equation ( 10.4), this leads\\nto a bound on the expected excess risk equal toσ2s\\nn+2\\nntr(K)y⊤\\n∗K−1y∗\\ns+1, which is exactly\\nthe bound in equation ( 10.4) in the kernel context. However, it is not interesting in\\npractice, as it requires the computation of the kernel matrix Kand typically a square'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 10}, page_content='s+1, which is exactly\\nthe bound in equation ( 10.4) in the kernel context. However, it is not interesting in\\npractice, as it requires the computation of the kernel matrix Kand typically a square\\nroot to sample from the multivariate Gaussian distribution, which has a running-time\\ncomplexity of O(n3).\\nIn practice, many kernels come with a random feature expansion of the formk(x,x′) =\\nEv[\\nϕ(x,v)ϕ(x′,v)]\\n, such that|ϕ(x,v)|⩽Ralmost surely (as presented in section 7.4).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 11}, page_content='296 CHAPTER 10. ENSEMBLE LEARNING\\nWe can take for each column of ˆΦ the vector ( ϕ(x1,v),...,ϕ (xn,v))⊤∈Rnfor a random\\nindependent v. Then we have E[ˆΦˆΦ⊤]\\n=sKby construction, while a short calculation\\n(left as an exercise) shows that the second-order moment can be bounded as\\nEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\\n≼s(s−1)K2+sR2K.\\nThis leads to the boundσ2s\\nn+1\\nnR2y⊤\\n∗K−1y∗\\ns−1, which is almost the same as before, but with\\nan eﬃcient practical algorithm (since we now have to solve a least-sq uares regression\\nproblem in dimension s, which is more eﬃcient than using the kernel trick if s<n ).\\nExperiments. In ﬁgure 10.2, we consider a polynomial regression problem in dimen-\\nsiondX= 20, with polynomials of a maximum degree of 2, and thus a feature sp ace\\nof dimension d= 1 +dX+dX(dX+ 1)/2 = 231. We also compare ridge regression with\\nGaussian random projections. We see better performance as mgrows, consistent with our\\nbounds (underﬁtting for small s, overﬁtting with large s). Moreover, when the number\\nmof times the dataset is randomly projected goes from 10 to 100, we obtain almost the\\nsame plot, with better performance than m= 1, highlighting the fact that m= 1 is not\\noptimal but taking mtoo large is not useful.\\nJohnson-Lindenstrauss lemma ( ♦).A related classical result in Gaussian random\\nprojections shows that nfeature vectors ϕ1,...,ϕn∈Rdcan be well represented in di-\\nmensionsby Gaussian random projections, with sgrowing only logarithmically in n, and\\nindependent of the underlying dimension. Lemma 10.1 shows that all pairwise distances\\nare preserved (a small modiﬁcation would lead to the preservation o f all dot products).\\nLemma 10.1 ( Johnson and Lindenstrauss ,1984)Givenϕ1,...,ϕn∈Rd, letS∈\\nRd×sbe a random matrix with independent standard Gaussian rando m variables. Then,\\nfor anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\\nε2logn2\\nδ, with probability greater than 1−δ,\\nwe have\\n∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\\n2⩽\\ued79\\ued79s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\\n2⩽(1+ε)∥ϕi−ϕj∥2\\n2.\\n(10.6)'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 11}, page_content='for anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\\nε2logn2\\nδ, with probability greater than 1−δ,\\nwe have\\n∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\\n2⩽\\ued79\\ued79s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\\n2⩽(1+ε)∥ϕi−ϕj∥2\\n2.\\n(10.6)\\nProof (♦) Letψ∈Rdwith theℓ2-norm equal to 1. The random variable y=ψ⊤SS⊤ψ\\nis the sum of srandom variables ψ⊤S·jS⊤\\n·jψ, forS·jthejth column of S,j∈{1,...,s}.\\nEach of these is the square of S⊤\\n·jψ, which is Gaussian with mean zero and variance equal\\nto∥ψ∥2\\n2= 1. Thus, yis a chi-squared random variable.5We can thus apply concentration\\nresults from exercise 8.1, leading to\\nP(\\n|y−s|⩾sε)\\n⩽(1−ε\\nexp(−ε))s/2\\n+(1 +ε\\nexp(ε))s/2\\n.\\nWe can then use the inequality log(1 + u)⩽u−u2\\n3for any|u|⩽1\\n2, applied to εand\\n−ε, leading to the probability bound P(\\n|y−s|⩾sε)\\n⩽2 exp(\\n−s\\n2ε2\\n3)\\n.We then apply\\n5Seehttps://en.wikipedia.org/wiki/Chi-squared_distribut ion.'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 12}, page_content='10.2. RANDOM PROJECTIONS AND AVERAGING 297\\nFigure 10.2. Polynomial regression in dimension 20, with polynomials of a maximum\\ndegree of 2, with n= 100. Top left: training and testing errors for ridge regression in t he\\nﬁxed design setting (the input data are ﬁxed, and only the noise var iables are resampled\\nfor computing the test error). All other plots: training and testin g errors for Gaussian\\nrandom projections, with diﬀerent numbers of random projection s:m= 1 (top right),\\nm= 10 (bottom left), and m= 100 (bottom right). All the curves are averaged over 100\\nreplications of the noise variables and the random projections.\\nthis bound for ψbeing then(n−1)/2 vectorsϕi−ϕj, fori̸=j, leading to, using a\\nunion bound, a probability that equation ( 10.6) is not satisﬁed with a probability less\\nthann2exp(−sε2/6),leading to the desired result.\\nIn our context of least-squares regression, the Johnson-Linde nstrauss lemma shows that\\nthe kernel matrix is preserved by random projections so that pre dictions with the pro-\\njected data should be close to predictions with the original data. Th e results in this\\nsection provide a direct proof that aims to characterize directly th e predictive perfor-\\nmance of such random projections (using the Johnson-Lindenstr auss lemma to obtain\\nsimilar bounds is not straightforward as we consider unregularized r egression, where per-\\nturbations of matrix inverses are harder to control).'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 13}, page_content='298 CHAPTER 10. ENSEMBLE LEARNING\\n10.3 Boosting\\nIn sections 10.1 and10.2, we focused on uniformly combining the outputs (e.g., plain av-\\neraging) of estimators obtained by randomly reweighted versions o f the original datasets.\\nReweighting was performed independent of the performance of th e resulting prediction\\nfunctions, and the training procedures for all predictors could be done in parallel. In this\\nsection, we explore sequential reweightings of the training datasets that depend on the\\nmistakes made by the current prediction functions. While the natur al parallelizability is\\nlost, we will see that we get additional statistical beneﬁts.\\nIn the early boosting procedures adapted to binary classiﬁcation, the original learn-\\ning algorithms (going from datasets to prediction functions with bina ry values) were used\\ndirectly on a reweighted version, such as Adaboost (see, e.g., Freund et al. ,1999). Our\\nanalysis will be carried out for boosting procedures, often referr ed to as “gradient boost-\\ning,” which are adapted to real-valued outputs, as done in the rest o f this book (noting\\nthat for classiﬁcation, we can use convex surrogates).\\nThe theory of boosting is rich, with many connections, and in this sec tion, we only\\nprovide a consistency proof in the simplest setting. See Schapire and Freund (2012) for\\nmore details.\\n10.3.1 Problem Setup\\nGiven an input space Xandnobservations ( xi,yi)∈X×R,i= 1,...,n , we are given a\\nset of predictors ϕ(·,w) :X→R, forw∈W, withWtypically being a compact subset of\\na ﬁnite-dimensional vector space.\\nThe main assumption is that given weights α∈Rn, one can reasonably easily ﬁnd\\nthe function ϕ(·,w) that minimizes with respect to w∈W:\\nn∑\\ni=1αiϕ(xi,w); (10.7)\\nthat is, the dot product between αand thenoutputs ofϕ(·,w) on thenobservations. In\\nthis section, for simplicity, we assume that this minimization can be don e exactly. This\\nis often referred to as the “weak learner” assumption. Many exam ples are available, such\\nas the following:'),\n",
       " Document(metadata={'source': 'text_summarization/tree.pdf', 'page': 13}, page_content='this section, for simplicity, we assume that this minimization can be don e exactly. This\\nis often referred to as the “weak learner” assumption. Many exam ples are available, such\\nas the following:\\n•Linear stumps for X=Rd:ϕ(x,w) =±(w⊤\\n0x+w1)+, wherew= (w0,w1)∈Rd×R,\\nwith sometimes the restriction that whas a single nonzero component (where the\\nweak learning tractability assumption is indeed veriﬁed; see exercise 10.6). This\\nwill lead to a predictor, which is a one-hidden-layer neural network a s presented\\nin chapter 9, but learned sequentially rather than by GD on the empirical risk. In\\nthe context of binary classiﬁcation, the weak learners are sometim es thresholded to\\nvalues in{−1,1}by taking their signs.\\nExercise 10.6 For linear stumps with only one nonzero coordinate for the sl ope,\\nshow how to minimize equation ( 10.7) eﬃciently.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200).split_documents(docs)\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_prompt = \"\"\"\n",
    "Please summarize the below speech:\n",
    "Speech:'{text}'\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=chunks_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\nProvide the final summary of the entire speech with these important points.\\nAdd a Motivation Title,Start the precise summary with an introduction and provide the summary in number \\npoints for the speech.\\nSpeech:{text}\\n')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = \"\"\"\n",
    "Provide the final summary of the entire speech with these important points.\n",
    "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
    "points for the speech.\n",
    "Speech:{text}\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_template = PromptTemplate(\n",
    "    input_variable=['text'],\n",
    "    template=final_prompt\n",
    ")\n",
    "final_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.1. AVERAGING/BAGGING 285\n",
      "ized situation, we have mindependent datasets of size n, composed of independent and\n",
      "identically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\n",
      "We obtain for each of them an estimator ˆf(j)\n",
      "λ, wherej∈{1,...,m}andλis an associ-\n",
      "ated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\n",
      "regression, the new predictor, ˆfbag\n",
      "λ, is simply the average of all ˆf(j)\n",
      "λ,j= 1,...,m .\n",
      "If we denote bias(j)(x) =E[ˆf(j)\n",
      "λ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\n",
      "λ(x)]\n",
      "(assuming\n",
      "thatxis ﬁxed and only taking expectations with respect to the data), the n they are the\n",
      "same for all j∈{1,...,m}and the bias of ˆfbag\n",
      "λis the same as the base bias for a single\n",
      "dataset (and thus so is the squared bias). At the same time, the va riance is divided by m\n",
      "because the datasets are assumed to be independent.\n",
      "Thus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\n",
      "a higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\n",
      "independent datasets, it is relevant to concatenate all mdatasets into one large dataset\n",
      "withN=nmobservations and learn a single predictor with these: the generaliza tion\n",
      "performance of the average of mpredictors will often be the same as the one of the single\n",
      "predictor on the large dataset, but with potential computational beneﬁts. We now give a\n",
      "few examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\n",
      "Thek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\n",
      "prediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\n",
      "bias was upper-bounded by 8 B2diam(X)2(2k\n",
      "n)2/d(ford⩾2). At the same time, the\n",
      "variance was bounded byσ2\n",
      "k, whereσ2is a bound on the noise variance on top of the\n",
      "target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\n",
      "replications, we get an excess risk upper-bounded by\n",
      "σ2\n",
      "km+ 8B2diam(X)2(2k\n",
      "n)2/d\n",
      ".'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\n",
      "replications, we get an excess risk upper-bounded by\n",
      "σ2\n",
      "km+ 8B2diam(X)2(2k\n",
      "n)2/d\n",
      ".\n",
      "When optimizing this bound with respect to k, we get that k1+2/d∝n2/d\n",
      "m, leading to\n",
      "k∝1\n",
      "md/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\n",
      "(which is consistent with favoring higher variance estimators). The overall excess risk ends\n",
      "up being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\n",
      "observations.\n",
      "Thus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\n",
      "estimating independently, and combining linearly does not lead to an ov erall improved\n",
      "statistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\n",
      "putational advantages when the mestimators can be computed in parallel (and totally\n",
      "independently). We thus obtain a distributed algorithm with the same worst-case pre-\n",
      "dictive performance as for a single machine.\n",
      "Note here that there is an upper bound on the number of replication s (and thus the\n",
      "ability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\n",
      "and thus,mcannot grow larger than n2/d.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'286 CHAPTER 10. ENSEMBLE LEARNING\n",
      "Exercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\n",
      "jority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\n",
      "section4.1.4to derive an upper bound on the expected risk, what is the corr esponding\n",
      "optimal choice of mwhen using independent datasets?\n",
      "Ridge regression. Following the analysis from section 7.6.6, the variance of the ridge\n",
      "regression estimator was proportional toσ2\n",
      "nλ−1/αand the bias proportional to λt/s(see\n",
      "precise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\n",
      "proportional toσ2\n",
      "nmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\n",
      "observations (and the same regularization parameter). Again, wit h the proper choice\n",
      "of regularization parameter (lower λthan for the full dataset), there is no statistical\n",
      "advantage. Still, there may be a computational one, not only for pa rallel processing but\n",
      "also with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\n",
      "time for ridge regression is superlinear in the number of observation s with running-time\n",
      "complexities between O(n2) andO(n3) if no low-rank approximations are used.\n",
      "Exercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\n",
      "time complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\n",
      "split of the data into mchunks? What is the optimal value of m?\n",
      "Exercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\n",
      "an estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\n",
      "problem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\n",
      "j=1ˆf(j)(x))\n",
      "the majority vote clas-\n",
      "siﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\n",
      "tion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\n",
      "E[1ˆfbag(x)̸=f∗(x)]⩽exp(\n",
      "−m\n",
      "2(ε(x))2\n",
      "+)\n",
      ".If∀x∈X,ε(x)⩾η >0, show that the expected\n",
      "excess risk is less than exp(\n",
      "−m\n",
      "2η2)\n",
      ".'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'E[1ˆfbag(x)̸=f∗(x)]⩽exp(\n",
      "−m\n",
      "2(ε(x))2\n",
      "+)\n",
      ".If∀x∈X,ε(x)⩾η >0, show that the expected\n",
      "excess risk is less than exp(\n",
      "−m\n",
      "2η2)\n",
      ".\n",
      "Beyond independent datasets. Having independent datasets may not be possible,\n",
      "and one typically needs to artiﬁcially create such replicated dataset s from a single one,\n",
      "which is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\n",
      "still, but this time with a potentially higher bias.\n",
      "10.1.2 Bagging\n",
      "We consider datasets D(b), obtained with random weights v(b)\n",
      "i∈R+,i= 1,...,n . For\n",
      "the bootstrap,1we consider nsamples from the original ndata points with replacement,\n",
      "which correspond to integer weights v(b)\n",
      "i∈N,i= 1,...,n , that sum to n. Such sets of\n",
      "weights are sampled independently mtimes. We study m=∞for simplicity; that is,\n",
      "inﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\n",
      "moderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\n",
      "1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\n",
      "(1994) for an introduction to bootstrapping methods in statistic s.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.1. AVERAGING/BAGGING 287\n",
      "which is important for highly variable predictors (which usually imply a lar ge estimation\n",
      "variance).\n",
      "For linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\n",
      "square loss, such as kernel ridge regression or local averaging, t his leads to another linear\n",
      "estimator. Therefore, this provides alternative ways of regulariz ing, which typically may\n",
      "not provide a strong statistical gain over existing methods but pro vide a computational\n",
      "gain, in particular when each estimator is very eﬃcient to compute (s ee related examples\n",
      "in section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\n",
      "variance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\n",
      "larizing methods. See also the end of section 10.2 for a short description of “random\n",
      "forests,” which is also partially based on bagging.\n",
      "For simplicity, we will consider averaging estimators obtained by rand omly selecting s\n",
      "observations from the navailable ones, doing this many times (inﬁnitely many for the\n",
      "analysis), and averaging the predictions.\n",
      "Exercise 10.4 Show that when sampling nelements with replacement from nitems, the\n",
      "expected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\n",
      "to inﬁnity.\n",
      "One-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\n",
      "where the strong eﬀect of bagging is striking. The analysis in this sub section follows\n",
      "from Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\n",
      "the pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\n",
      "(ignoring ties), then we can write the bagged estimate as\n",
      "ˆf(x) =n∑\n",
      "i=1Viy(i)(x),\n",
      "where the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\n",
      "probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\n",
      "subsample of size s. We consider sampling without replacement and leave sampling with'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\n",
      "subsample of size s. We consider sampling without replacement and leave sampling with\n",
      "replacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\n",
      "To select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\n",
      "that theith-nearest neighbor is selected but none of the closer neighbors, which leaves\n",
      "s−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\n",
      "Vi= 0, while otherwise Vi=(n\n",
      "s)−1(n−i\n",
      "s−1)\n",
      ", as the total number of subsets of size sis(n\n",
      "s)\n",
      ",\n",
      "and there are(n−i\n",
      "s−1)\n",
      "relevant ones.\n",
      "We can now use the reasoning from section 6.3.2. Since for any x, the weights given\n",
      "to each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\n",
      "variance term is equal to∑n\n",
      "i=1V2\n",
      "i. To obtain a bound, we note that for i⩽n−s+ 1,\n",
      "Vi=s\n",
      "n−s+ 1∏s−2\n",
      "j=0(n−i−j)\n",
      "∏s−2\n",
      "j=0(n−j)=s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n−j)\n",
      "⩽s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n)\n",
      ",'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'288 CHAPTER 10. ENSEMBLE LEARNING\n",
      "leading to, upper-bounding the sum by an integral,\n",
      "n∑\n",
      "i=1V2\n",
      "i⩽s2\n",
      "(n−s+ 1)2n∑\n",
      "i=1(\n",
      "1−i\n",
      "n)2(s−1)⩽ns2\n",
      "(n−s+ 1)2∫1\n",
      "0(1−t)2(s−1)dt\n",
      "⩽ns2\n",
      "(n−s+ 1)21\n",
      "2s−1⩽ns\n",
      "(n−s+ 1)2=s\n",
      "n1\n",
      "(1 + 1/n−s/n)2.\n",
      "For the bias term, we need to bound∑n\n",
      "i=1Vi·E[\n",
      "∥x−x(i)(x)∥2]\n",
      ", where the expectation\n",
      "is with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\n",
      "conditioning on the data and x, this isB2multiplied by the expectation of the distance\n",
      "to the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\n",
      "on a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\n",
      "s2/difd⩾2 (which we now\n",
      "assume).\n",
      "Thus, the overall excess risk is less than\n",
      "4B2diam(X)21\n",
      "s2/d+s\n",
      "n1\n",
      "(1 + 1/n−s/n)2,\n",
      "which we can balance by choosing s1+2/d∝n, leading to the same performance as the\n",
      "k-nearest neighbor for a well-chosen k, but now with a bagged esimate.\n",
      "In ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\n",
      "eﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\n",
      "neighbor estimate, and when sdecreases, the variance indeed decreases while the bias\n",
      "increases.\n",
      "10.2 Random Projections and Averaging\n",
      "In section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\n",
      "can also be done through random projections of all observations. Such random projections\n",
      "can be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\n",
      "(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\n",
      "and (3) for kernel methods, using random features such as pres ented in section 7.4. Such\n",
      "random projections can also reduce the number of samples while kee ping the dimension\n",
      "ﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\n",
      "reduced size).\n",
      "In this section, we consider random projections for ordinary least -squares (OLS), with\n",
      "the same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'reduced size).\n",
      "In this section, we consider random projections for ordinary least -squares (OLS), with\n",
      "the same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\n",
      "design matrix, in two settings:\n",
      "•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\n",
      "2by minθ∈Rd∥Sy−SΦθ∥2\n",
      "2, whereS∈Rs×n\n",
      "is an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\n",
      "ments). This is an idealization of subsampling done in the previous sect ion. Here,\n",
      "we typically have n > s > d (more observations than the feature dimension), and'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.2. RANDOM PROJECTIONS AND AVERAGING 289\n",
      "Figure 10.1. Subsampling estimates with m= 20 subsampled datasets, for varying sub-\n",
      "sampling ratios n/s, with an estimation of the testing error. When n/s= 1, we recover\n",
      "the 1-nearest neighbor classiﬁer (which overﬁts), and when n/sgrows, we get better ﬁts\n",
      "until underﬁtting kicks in. Optimal testing error is obtained for n/s= 8.\n",
      "one of the beneﬁts of sketching is to be able to store a reduced rep resentation of\n",
      "the data ( Rs×dinstead of Rn×d).\n",
      "•Random projection : Replacing min θ∈Rd∥y−Φθ∥2\n",
      "2by minη∈Rs∥y−ΦSη∥2\n",
      "2, where\n",
      "S∈Rd×sis a more general sketching matrix. Here, we typically have d > n >\n",
      "s(high-dimensional situation). The beneﬁts of random projection a re twofold:\n",
      "reduction in computation time and regularization. This corresponds to replacing\n",
      "the corresponding feature vectors ϕ(x)∈RdbyS⊤ϕ(x)∈Rs. We will consider\n",
      "Gaussian matrices, but also subsampling matrices, and draw connec tions with kernel\n",
      "methods.\n",
      "In the following sections, we study these precisely for the OLS fram ework (it could\n",
      "also be done for ridge regression). We ﬁrst brieﬂy mention a common ly used and related\n",
      "approach.\n",
      "Random forests. A popular algorithm called “random forests” ( Breiman ,2001) mixes\n",
      "both dimension reduction by projection and bagging: decision trees are learned on a\n",
      "bootstrapped sample of the data, while selecting a random subset o f features at every\n",
      "splitting decision. This algorithm has nice properties (invariance to re scaling of the\n",
      "variables and robustness in high dimension due to the random featur e selection) and can\n",
      "be extended in many ways. See Biau and Scornet (2016) for details.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'290 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.2.1 Gaussian Sketching\n",
      "Following section 3.3on OLS, we consider a design matrix Φ ∈Rn×dwith rankd(i.e.,\n",
      "Φ⊤Φ∈Rd×dinvertible), which implies n⩾d. We consider s > d Gaussian random\n",
      "projections, with typically s⩽n, but this is not necessary in the analysis that follows.\n",
      "The estimator ˆθ(j)is obtained by using S(j)∈Rs×n, withj= 1,...,m , wherem\n",
      "denotes the number of replications. We then consider ˆθ=1\n",
      "m∑m\n",
      "j=1ˆθ(j). Whenm= 1,\n",
      "this is a single sketch.\n",
      "We will consider the same ﬁxed design assumptions as in section 3.5; that is,y=\n",
      "Φθ∗+ε, whereε∈Rnhas independent zero-mean components with variance σ2, and\n",
      "θ∗∈Rd. Our goal is to compute the ﬁxed design error1\n",
      "nEε,S∥Φˆθ−Φθ∗∥2\n",
      "2, where we take\n",
      "expectations with respect to both the learning problem (in the ﬁxed design setting, the\n",
      "noise vector ε) and the added randomization (the sketching matrices S(j),j= 1,...,m ).\n",
      "To compute this error, we ﬁrst need to compute expectations and variances with\n",
      "respect to the random projections, assuming that εis ﬁxed.\n",
      "We ﬁrst introduce a representation tool that will allow simple expres sions of all predic-\n",
      "tion vectors S(j)Φ. Since the Gaussian matrices S(j)are invariant by left and right multi-\n",
      "plication by an orthogonal matrix, we can assume that the singular v alue decomposition\n",
      "(SVD) of Φ = UDV⊤, whereV∈Rd×dis orthogonal (i.e., V⊤V=VV⊤=I),D∈Rd×d\n",
      "is an invertible diagonal matrix, and U∈Rn×dhas orthonormal columns (i.e., U⊤U=I;\n",
      "remember than n⩾d), is such that U=(I\n",
      "0)\n",
      ", and we can write S(j)=(\n",
      "S(j)\n",
      "1S(j)\n",
      "2)\n",
      ", with\n",
      "S(j)\n",
      "1∈Rs×dandS(j)\n",
      "2∈Rs×(n−d). We can also split yasy=(y1\n",
      "y2)\n",
      "fory1∈Rdand\n",
      "y2∈Rn−d.\n",
      "We can write the normal equation that deﬁnes ˆθ(j)∈Rd, for eachj∈{1,...,m}(i.e.,\n",
      "(Φ⊤(S(j))⊤S(j)Φ)ˆθ(j)= Φ⊤(S(j))⊤S(j)y), leading to the following closed-form estimators\n",
      "ˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\n",
      "regarding the SVD of Φ, we have S(j)Φ =S(j)\n",
      "1DV⊤. We can then expand the prediction\n",
      "vector in Rnas'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'ˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\n",
      "regarding the SVD of Φ, we have S(j)Φ =S(j)\n",
      "1DV⊤. We can then expand the prediction\n",
      "vector in Rnas\n",
      "Φˆθ(j)= Φ(Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\n",
      "=(I\n",
      "0)\n",
      "DV⊤(VD(S(j)\n",
      "1)⊤S(j)\n",
      "1DV⊤)−1VD(S(j)\n",
      "1)⊤S(j)y\n",
      "=(I\n",
      "0)\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)y=(I\n",
      "0)\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤(S(j)\n",
      "1y1+S(j)\n",
      "2y2)\n",
      "=(y1+ ((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)\n",
      "2y2\n",
      "0)\n",
      ".\n",
      "Thus, since E[S(j)\n",
      "2] = 0 andS(j)\n",
      "2is independent of S(j)\n",
      "1, we getES(j)[\n",
      "Φˆθ(j)]\n",
      "=(y1\n",
      "0)\n",
      ", which\n",
      "happens to be exactly the OLS estimator Φ ˆθOLS= Φ(Φ⊤Φ)−1Φ⊤y=(I0\n",
      "0 0)\n",
      "y. Moreover,\n",
      "2Ifs⩾d, thenS(j)Φ has rank dalmost surely, and thus ˆθ(j)is uniquely deﬁned.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.2. RANDOM PROJECTIONS AND AVERAGING 291\n",
      "we have the model y= Φθ∗+εand, if we split εasε=(ε1\n",
      "ε2)\n",
      ", we havey=(I\n",
      "0)\n",
      "DV⊤θ∗+ε,\n",
      "and thusy2=ε2. We therefore get\n",
      "ES(j)[Φˆθ(j)−ES(j)Φˆθ(j)2\n",
      "2]\n",
      "=ES(j)[\n",
      "∥((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)\n",
      "2ε2∥2\n",
      "2]\n",
      ".\n",
      "Taking the expectation with respect to ε(through E[ε2ε⊤\n",
      "2] =σ2I), using the trace trick,\n",
      "and using expectations for the Wishart and inverse Wishart distribu tions,3this leads to\n",
      "Eε,S(j)[Φˆθ(j)−ES(j)Φˆθ(j)2\n",
      "2]\n",
      "=σ2ES(j)[\n",
      "tr(\n",
      "(S(j)\n",
      "2)⊤S(j)\n",
      "1((S(j)\n",
      "1)⊤S(j)\n",
      "1)−2(S(j)\n",
      "1)⊤S(j)\n",
      "2)]\n",
      "=σ2ES(j)[\n",
      "tr(\n",
      "S(j)\n",
      "2(S(j)\n",
      "2)⊤S(j)\n",
      "1((S(j)\n",
      "1)⊤S(j)\n",
      "1)−2(S(j)\n",
      "1)⊤)]\n",
      "= (n−d)σ2ES(j)\n",
      "1[\n",
      "tr(\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1)]\n",
      "=d\n",
      "s−d−1(n−d)σ2.\n",
      "We can now compute the overall expected generalization error:\n",
      "1\n",
      "nEε,S(j)[1\n",
      "mm∑\n",
      "j=1Φˆθ(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "Φˆθ(1)]\n",
      "−Φθ∗2\n",
      "2]\n",
      "+1\n",
      "nmEε,S(1)[Φˆθ(1)−ES(1)Φˆθ(1)2]\n",
      "=1\n",
      "nEε[ΦˆθOLS−Φθ∗2\n",
      "2]\n",
      "+σ2d\n",
      "nmn−d\n",
      "s−d−1\n",
      "=σ2d\n",
      "n+σ2d\n",
      "nmn−d\n",
      "s−d−1.\n",
      "Thus, when morstends to inﬁnity, we recover the traditional OLS behavior, while for m\n",
      "andsﬁnite, the performance degrades gracefully. Moreover, when s=n, even form= 1,\n",
      "we get essentially twice the performance of the OLS estimator. We n ote that to get the\n",
      "same performance as OLS (up to a factor of 2), we need m=n−d\n",
      "s−d−1∼n\n",
      "sreplications.\n",
      "As in section 10.1, there is no statistical gain (here, compared to OLS), but only\n",
      "potentially a computational one (because some computations may b e done in parallel\n",
      "and of reduced storage). See, for example, Dobriban and Liu (2019) for other criteria\n",
      "and sketching matrices.\n",
      "Beyond Gaussian sketching. In this section, we have chosen a Gaussian sketching\n",
      "matrixS. This made the analysis simple because of the properties of the Gaus sian\n",
      "distribution (invariance by rotation and availability of exact expecta tions for inverse\n",
      "Wishart distributions). The analysis can be extended with more comp lex tools to other\n",
      "random sketching matrices with more attractive computational pr operties, such as with'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'Wishart distributions). The analysis can be extended with more comp lex tools to other\n",
      "random sketching matrices with more attractive computational pr operties, such as with\n",
      "many zeros, leading to subsampling observations or dimensions. See Wang et al. (2018),\n",
      "Dobriban and Liu (2019), and the references therein. For the random projections that\n",
      "follow, our analysis will apply to more general sketches.\n",
      "3IfS∈Ra×bhas independent standard Gaussian components, then E[(S⊤S)−1] =1\n",
      "a−b−1Iifa > b+1,\n",
      "andE[SS⊤] =bI; seehttps://en.wikipedia.org/wiki/Inverse-Wishart_distr ibution.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'292 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.2.2 Random Projections\n",
      "We also consider the ﬁxed design setup, with a design matrix Φ ∈Rn×dand a response\n",
      "vector of the form y= Φθ∗+ε. We now assume that d>n (high-dimensional setup) and\n",
      "the rank of Φ is n. In this high-dimensional setup, we need some form of regularizatio n,\n",
      "which will come here from random projections.\n",
      "For eachj∈{1,...,n}, we consider a sketching matrix S(j)∈Rd×s, fors⩽nsampled\n",
      "independently from a distribution to be determined (we only assume t hat almost surely,\n",
      "its rank is equal to s). We then consider ˆ η(j)as a minimizer of min η∈Rs∥y−ΦS(j)η∥2\n",
      "2.\n",
      "For simplicity, we assume that matrix Φ S(j)has ranks, which is the case almost surely\n",
      "for Gaussian projections; this implies that ˆ η(j)is unique, but our result applies in all\n",
      "situations, as we are only interested in the denoised response vect or. We now consider\n",
      "the average ˆθ=1\n",
      "m∑m\n",
      "j=1S(j)ˆη(j).\n",
      "We thus consider the estimator ˆ η(j)=(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rs, ob-\n",
      "tained from the normal equation ( S(j))⊤Φ⊤ΦS(j)ˆη(j)= (S(j))⊤Φ⊤ywith the denoised\n",
      "response vector\n",
      "ˆy(j)= ΦS(j)ˆη(j)= ΦS(j)(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rn.\n",
      "Denoting Π(j)= ΦS(j)(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤, it takes the form ˆ y(j)= Π(j)y.\n",
      "Matrix Π(j)is almost surely an orthogonal projection matrix into an s-dimensional vector\n",
      "space, and its expectation is denoted as ∆ ∈Rn×n, which satisﬁes tr(∆) = s. We have,\n",
      "moreover, 0 ≼∆≼I; that is, all eigenvalues of ∆ are between 0 and 1.\n",
      "We can then compute expectations and variances as follows:\n",
      "ES(j)[\n",
      "ˆy(j)]\n",
      "=ES(j)[\n",
      "Π(j)y]\n",
      "= ∆y= ∆[\n",
      "Φθ∗+ε] = ∆ε+ ∆Φθ∗\n",
      "ES(j)[\n",
      "ˆy(j)]\n",
      "−Φθ∗= ∆ε+ [∆−I]Φθ∗ (10.1)\n",
      "ES(j)[ˆy(j)−ES(j)[\n",
      "ˆy(j)]2\n",
      "2]\n",
      "=ES(j)[(Π(j)−∆)y2\n",
      "2]\n",
      "=y⊤ES(j)[(\n",
      "Π(j)−∆)2]\n",
      "y\n",
      "=y⊤ES(j)[\n",
      "Π(j)−∆Π(j)−Π(j)∆+∆2]\n",
      "ysince Π(j)Π(j)=Π(j),\n",
      "=y⊤(\n",
      "∆−∆2)\n",
      "y,sinceE[Π(j)= ∆. (10.2)\n",
      "Thus, the overall (ﬁxed design) expected generalization error is e qual to, using that\n",
      "S(1),...,S(m)are i.i.d. matrices,\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "ˆy(1)]\n",
      "−Φθ∗2\n",
      "2+1'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'Thus, the overall (ﬁxed design) expected generalization error is e qual to, using that\n",
      "S(1),...,S(m)are i.i.d. matrices,\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "ˆy(1)]\n",
      "−Φθ∗2\n",
      "2+1\n",
      "mES(1)[ˆy(1)−ES(1)[\n",
      "ˆy(1)]2\n",
      "2]]\n",
      "by taking expectations with respect to all S(j),\n",
      "=1\n",
      "nEε[∆ε+ [∆−I]Φθ∗2\n",
      "2+1\n",
      "my⊤(\n",
      "∆−∆2)\n",
      "y]\n",
      "using equations ( 10.1) and ( 10.2).'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.2. RANDOM PROJECTIONS AND AVERAGING 293\n",
      "Using the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I, we get\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=σ2\n",
      "ntr(∆2) +1\n",
      "nθ⊤\n",
      "∗Φ⊤[I−∆]2Φθ∗+1\n",
      "nm[\n",
      "σ2(tr(∆)−tr(∆2)) +θ⊤\n",
      "∗Φ⊤(\n",
      "∆−∆2)\n",
      "Φθ∗]\n",
      "using the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I,\n",
      "=σ2\n",
      "n(\n",
      "1−1\n",
      "m)\n",
      "tr(∆2) +σ2s\n",
      "nm+1\n",
      "nθ⊤\n",
      "∗Φ⊤[∆−I]2Φθ∗+1\n",
      "nmθ⊤\n",
      "∗Φ⊤(\n",
      "∆−∆2)\n",
      "Φθ∗\n",
      "=σ2\n",
      "n(\n",
      "1−1\n",
      "m)\n",
      "tr(∆2) +σ2s\n",
      "nm+1\n",
      "nθ⊤\n",
      "∗Φ⊤[\n",
      "I−∆ +(1\n",
      "m−1)\n",
      "(∆−∆2)]\n",
      "Φθ∗\n",
      "⩽σ2s\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗,since ∆2≼∆, (10.3)\n",
      "which is the value for m= 1 (single replication). Note that the expectation (before taking\n",
      "the bound) decreases in m, with a limitσ2tr(∆2)\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤[I−∆]2Φθ∗whenm→+∞\n",
      "(with improved bias and variance terms). We now follow Kab´ an (2014) and Thanei et al.\n",
      "(2017) to bound the matrix I−∆.\n",
      "Since ∆ is the expectation of a projection matrix, we already know th at 0≼∆≼I.\n",
      "We omit the superscript 0(j)for clarity, and we consider Π = Φ S(\n",
      "S⊤Φ⊤ΦS)−1S⊤Φ. For\n",
      "any vector z∈Rn, we consider\n",
      "z⊤(I−∆)z=ES[\n",
      "z⊤(I−Π)z]\n",
      "=ES[\n",
      "z⊤z−z⊤ΦS(\n",
      "S⊤Φ⊤ΦS)−1S⊤Φ⊤z]\n",
      "=ES[\n",
      "min\n",
      "u∈Rs∥z−ΦSu∥2\n",
      "2]\n",
      "by deﬁnition of projections ,\n",
      "⩽ES[\n",
      "min\n",
      "v∈Rd∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "by minimizing over a smaller subspace,\n",
      "⩽min\n",
      "v∈RdES[\n",
      "∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "by properties of the expectation.\n",
      "We can expand this to get\n",
      "ES[\n",
      "∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "=∥z∥2\n",
      "2−2z⊤ΦES[\n",
      "SS⊤]\n",
      "v+v⊤ES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      "v,\n",
      "leading to, after selecting the optimal vasv=(\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤z,\n",
      "z⊤(I−∆)z⩽z⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "z.\n",
      "We then need to apply to z= Φθ∗and get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽θ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗.\n",
      "Thus, we get an overall upper bound of\n",
      "σ2s\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗,'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'294 CHAPTER 10. ENSEMBLE LEARNING\n",
      "composed of expectations that can be readily computed. As shown next for special\n",
      "cases, we obtain a bias-variance trade-oﬀ similar to equation ( 3.6) for ridge regression\n",
      "in section 3.6, but now with random projections. Note that in the ﬁxed design set ting,\n",
      "there is no explosion of the testing error when s=n, as opposed to the random design\n",
      "setting studied in section 12.2 in the context of “double descent” (where generalization\n",
      "to unseen inputs is required).\n",
      "Gaussian projections. If we assume Gaussian random projections, with S∈Rd×s\n",
      "with independent standard Gaussian components, we get, from pr operties of the Wishart\n",
      "distribution,4\n",
      "ES[SS⊤] =sIandES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      "=s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I.\n",
      "We then get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽θ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗\n",
      "=θ⊤\n",
      "∗Φ⊤(\n",
      "I−s2Φ(\n",
      "s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I)−1Φ⊤)\n",
      "Φθ∗\n",
      "=θ⊤\n",
      "∗Φ⊤Φθ∗−sθ∗(Φ⊤Φ)2(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "=θ⊤\n",
      "∗Φ⊤Φ(\n",
      "Φ⊤Φ + tr(Φ⊤Φ)I)(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "⩽2 tr(Φ⊤Φ)·θ⊤\n",
      "∗Φ⊤Φ(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "using that Φ⊤Φ + tr(Φ⊤Φ)Ihas eigenvalues less than 2 tr(Φ⊤Φ),\n",
      "⩽2 tr(Φ⊤Φ)∥θ∗∥2\n",
      "2\n",
      "s+ 1,\n",
      "since Φ⊤Φ(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1has eigenvalues less than 1 /(s+ 1). The overall\n",
      "excess risk is then less than\n",
      "σ2s\n",
      "n+2\n",
      "ntr(Φ⊤Φ)∥θ∗∥2\n",
      "2\n",
      "s+ 1, (10.4)\n",
      "which is exactly of the form obtained for ridge regression in equation (3.6) with the\n",
      "identiﬁcation s∼tr(Φ⊤Φ)\n",
      "λ. We can consider other sketching matrices with additional\n",
      "properties, such as sparsity (see exercise 10.5).\n",
      "Exercise 10.5 We consider a sketching matrix S∈Rd×s, where each column is equal\n",
      "to one of the dcanonical basis vectors of Rd, selected uniformly at random and indepen-\n",
      "dently. Compute E[SS⊤], as well as ES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      ", as well as a bound similar to\n",
      "equation ( 10.4).\n",
      "4IfW=S1S⊤\n",
      "1forS1∈Rn×swith independent standard Gaussian components, then E[W] =sIand\n",
      "for ann×ndiagonal matrix D, we have E[WD2W] =s(s+1)D2+str(D2)I.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.2. RANDOM PROJECTIONS AND AVERAGING 295\n",
      "Kernel methods. ( ♦)The random projection idea can be extended to kernel meth-\n",
      "ods discussed in chapter 7. We consider the kernel matrix K= ΦΦ⊤∈Rn×n, and the\n",
      "assumption y= Φθ∗+εwith∥θ∗∥2bounded is turned into y=y∗+εwithy⊤\n",
      "∗K−1y∗\n",
      "bounded. This corresponds to y∗=Kα, with a reproducing kernel Hilbert space (RKHS)\n",
      "normα⊤Kα. We then consider a random sketch ˆΦ∈Rn×sand an approximate ker-\n",
      "nel matrix ˆK. We then obtain an estimate ˆ y=ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y. Matrix Π is then\n",
      "Π = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤, and for the analysis, we need to compute its expectation, ∆ (this\n",
      "corresponds to replacing Φ Sin earlier developments starting at the beginning of sec-\n",
      "tion 10.2.2 with ˆΦ). We have, following the same reasoning as before, for an arbitra ry\n",
      "deterministic z∈Rn,\n",
      "z⊤(I−∆)z=EˆΦ[\n",
      "z⊤(I−Π)z]\n",
      "=EˆΦ[\n",
      "z⊤z−z⊤ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤z]\n",
      "=EˆΦ[\n",
      "min\n",
      "u∈Rs∥z−ˆΦu∥2\n",
      "2]\n",
      "by deﬁnition of projections ,\n",
      "⩽EˆΦ[\n",
      "min\n",
      "v∈Rn∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "by minimizing over a smaller subspace,\n",
      "⩽min\n",
      "v∈RnEˆΦ[\n",
      "∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "by properties of the expectation.\n",
      "We can expand this to get\n",
      "EˆΦ[\n",
      "∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "=∥z∥2\n",
      "2−2z⊤EˆΦ[ˆΦˆΦ⊤]\n",
      "v+v⊤EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "v,\n",
      "leading to, after selecting the optimal vasv=(\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤]\n",
      "z,\n",
      "z⊤(I−∆)z⩽z⊤(\n",
      "I−EˆΦ[ˆΦˆΦ⊤](\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\n",
      "z. (10.5)\n",
      "We then need to apply equation ( 10.5) toz=y∗to get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽y⊤\n",
      "∗(\n",
      "I−EˆΦ[ˆΦˆΦ⊤](\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\n",
      "y∗.\n",
      "We can, for example, consider each column of ˆΦ to be sampled from a Gaussian distri-\n",
      "bution with mean zero and covariance matrix K, for which we have\n",
      "EˆΦ[ˆΦˆΦ⊤]\n",
      "=sKandEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "=s(s+ 1)K2+str(K)·K.\n",
      "Using equation ( 10.3), with the same derivations that led to equation ( 10.4), this leads\n",
      "to a bound on the expected excess risk equal toσ2s\n",
      "n+2\n",
      "ntr(K)y⊤\n",
      "∗K−1y∗\n",
      "s+1, which is exactly\n",
      "the bound in equation ( 10.4) in the kernel context. However, it is not interesting in\n",
      "practice, as it requires the computation of the kernel matrix Kand typically a square'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'s+1, which is exactly\n",
      "the bound in equation ( 10.4) in the kernel context. However, it is not interesting in\n",
      "practice, as it requires the computation of the kernel matrix Kand typically a square\n",
      "root to sample from the multivariate Gaussian distribution, which has a running-time\n",
      "complexity of O(n3).\n",
      "In practice, many kernels come with a random feature expansion of the formk(x,x′) =\n",
      "Ev[\n",
      "ϕ(x,v)ϕ(x′,v)]\n",
      ", such that|ϕ(x,v)|⩽Ralmost surely (as presented in section 7.4).'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'296 CHAPTER 10. ENSEMBLE LEARNING\n",
      "We can take for each column of ˆΦ the vector ( ϕ(x1,v),...,ϕ (xn,v))⊤∈Rnfor a random\n",
      "independent v. Then we have E[ˆΦˆΦ⊤]\n",
      "=sKby construction, while a short calculation\n",
      "(left as an exercise) shows that the second-order moment can be bounded as\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "≼s(s−1)K2+sR2K.\n",
      "This leads to the boundσ2s\n",
      "n+1\n",
      "nR2y⊤\n",
      "∗K−1y∗\n",
      "s−1, which is almost the same as before, but with\n",
      "an eﬃcient practical algorithm (since we now have to solve a least-sq uares regression\n",
      "problem in dimension s, which is more eﬃcient than using the kernel trick if s<n ).\n",
      "Experiments. In ﬁgure 10.2, we consider a polynomial regression problem in dimen-\n",
      "siondX= 20, with polynomials of a maximum degree of 2, and thus a feature sp ace\n",
      "of dimension d= 1 +dX+dX(dX+ 1)/2 = 231. We also compare ridge regression with\n",
      "Gaussian random projections. We see better performance as mgrows, consistent with our\n",
      "bounds (underﬁtting for small s, overﬁtting with large s). Moreover, when the number\n",
      "mof times the dataset is randomly projected goes from 10 to 100, we obtain almost the\n",
      "same plot, with better performance than m= 1, highlighting the fact that m= 1 is not\n",
      "optimal but taking mtoo large is not useful.\n",
      "Johnson-Lindenstrauss lemma ( ♦).A related classical result in Gaussian random\n",
      "projections shows that nfeature vectors ϕ1,...,ϕn∈Rdcan be well represented in di-\n",
      "mensionsby Gaussian random projections, with sgrowing only logarithmically in n, and\n",
      "independent of the underlying dimension. Lemma 10.1 shows that all pairwise distances\n",
      "are preserved (a small modiﬁcation would lead to the preservation o f all dot products).\n",
      "Lemma 10.1 ( Johnson and Lindenstrauss ,1984)Givenϕ1,...,ϕn∈Rd, letS∈\n",
      "Rd×sbe a random matrix with independent standard Gaussian rando m variables. Then,\n",
      "for anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\n",
      "ε2logn2\n",
      "δ, with probability greater than 1−δ,\n",
      "we have\n",
      "∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\n",
      "2⩽s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\n",
      "2⩽(1+ε)∥ϕi−ϕj∥2\n",
      "2.\n",
      "(10.6)'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'for anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\n",
      "ε2logn2\n",
      "δ, with probability greater than 1−δ,\n",
      "we have\n",
      "∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\n",
      "2⩽s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\n",
      "2⩽(1+ε)∥ϕi−ϕj∥2\n",
      "2.\n",
      "(10.6)\n",
      "Proof (♦) Letψ∈Rdwith theℓ2-norm equal to 1. The random variable y=ψ⊤SS⊤ψ\n",
      "is the sum of srandom variables ψ⊤S·jS⊤\n",
      "·jψ, forS·jthejth column of S,j∈{1,...,s}.\n",
      "Each of these is the square of S⊤\n",
      "·jψ, which is Gaussian with mean zero and variance equal\n",
      "to∥ψ∥2\n",
      "2= 1. Thus, yis a chi-squared random variable.5We can thus apply concentration\n",
      "results from exercise 8.1, leading to\n",
      "P(\n",
      "|y−s|⩾sε)\n",
      "⩽(1−ε\n",
      "exp(−ε))s/2\n",
      "+(1 +ε\n",
      "exp(ε))s/2\n",
      ".\n",
      "We can then use the inequality log(1 + u)⩽u−u2\n",
      "3for any|u|⩽1\n",
      "2, applied to εand\n",
      "−ε, leading to the probability bound P(\n",
      "|y−s|⩾sε)\n",
      "⩽2 exp(\n",
      "−s\n",
      "2ε2\n",
      "3)\n",
      ".We then apply\n",
      "5Seehttps://en.wikipedia.org/wiki/Chi-squared_distribut ion.'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'10.2. RANDOM PROJECTIONS AND AVERAGING 297\n",
      "Figure 10.2. Polynomial regression in dimension 20, with polynomials of a maximum\n",
      "degree of 2, with n= 100. Top left: training and testing errors for ridge regression in t he\n",
      "ﬁxed design setting (the input data are ﬁxed, and only the noise var iables are resampled\n",
      "for computing the test error). All other plots: training and testin g errors for Gaussian\n",
      "random projections, with diﬀerent numbers of random projection s:m= 1 (top right),\n",
      "m= 10 (bottom left), and m= 100 (bottom right). All the curves are averaged over 100\n",
      "replications of the noise variables and the random projections.\n",
      "this bound for ψbeing then(n−1)/2 vectorsϕi−ϕj, fori̸=j, leading to, using a\n",
      "union bound, a probability that equation ( 10.6) is not satisﬁed with a probability less\n",
      "thann2exp(−sε2/6),leading to the desired result.\n",
      "In our context of least-squares regression, the Johnson-Linde nstrauss lemma shows that\n",
      "the kernel matrix is preserved by random projections so that pre dictions with the pro-\n",
      "jected data should be close to predictions with the original data. Th e results in this\n",
      "section provide a direct proof that aims to characterize directly th e predictive perfor-\n",
      "mance of such random projections (using the Johnson-Lindenstr auss lemma to obtain\n",
      "similar bounds is not straightforward as we consider unregularized r egression, where per-\n",
      "turbations of matrix inverses are harder to control).'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'298 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.3 Boosting\n",
      "In sections 10.1 and10.2, we focused on uniformly combining the outputs (e.g., plain av-\n",
      "eraging) of estimators obtained by randomly reweighted versions o f the original datasets.\n",
      "Reweighting was performed independent of the performance of th e resulting prediction\n",
      "functions, and the training procedures for all predictors could be done in parallel. In this\n",
      "section, we explore sequential reweightings of the training datasets that depend on the\n",
      "mistakes made by the current prediction functions. While the natur al parallelizability is\n",
      "lost, we will see that we get additional statistical beneﬁts.\n",
      "In the early boosting procedures adapted to binary classiﬁcation, the original learn-\n",
      "ing algorithms (going from datasets to prediction functions with bina ry values) were used\n",
      "directly on a reweighted version, such as Adaboost (see, e.g., Freund et al. ,1999). Our\n",
      "analysis will be carried out for boosting procedures, often referr ed to as “gradient boost-\n",
      "ing,” which are adapted to real-valued outputs, as done in the rest o f this book (noting\n",
      "that for classiﬁcation, we can use convex surrogates).\n",
      "The theory of boosting is rich, with many connections, and in this sec tion, we only\n",
      "provide a consistency proof in the simplest setting. See Schapire and Freund (2012) for\n",
      "more details.\n",
      "10.3.1 Problem Setup\n",
      "Given an input space Xandnobservations ( xi,yi)∈X×R,i= 1,...,n , we are given a\n",
      "set of predictors ϕ(·,w) :X→R, forw∈W, withWtypically being a compact subset of\n",
      "a ﬁnite-dimensional vector space.\n",
      "The main assumption is that given weights α∈Rn, one can reasonably easily ﬁnd\n",
      "the function ϕ(·,w) that minimizes with respect to w∈W:\n",
      "n∑\n",
      "i=1αiϕ(xi,w); (10.7)\n",
      "that is, the dot product between αand thenoutputs ofϕ(·,w) on thenobservations. In\n",
      "this section, for simplicity, we assume that this minimization can be don e exactly. This\n",
      "is often referred to as the “weak learner” assumption. Many exam ples are available, such\n",
      "as the following:'\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the below speech:\n",
      "Speech:'this section, for simplicity, we assume that this minimization can be don e exactly. This\n",
      "is often referred to as the “weak learner” assumption. Many exam ples are available, such\n",
      "as the following:\n",
      "•Linear stumps for X=Rd:ϕ(x,w) =±(w⊤\n",
      "0x+w1)+, wherew= (w0,w1)∈Rd×R,\n",
      "with sometimes the restriction that whas a single nonzero component (where the\n",
      "weak learning tractability assumption is indeed veriﬁed; see exercise 10.6). This\n",
      "will lead to a predictor, which is a one-hidden-layer neural network a s presented\n",
      "in chapter 9, but learned sequentially rather than by GD on the empirical risk. In\n",
      "the context of binary classiﬁcation, the weak learners are sometim es thresholded to\n",
      "values in{−1,1}by taking their signs.\n",
      "Exercise 10.6 For linear stumps with only one nonzero coordinate for the sl ope,\n",
      "show how to minimize equation ( 10.7) eﬃciently.'\n",
      "Summary:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8536 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire speech with these important points.\n",
      "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "points for the speech.\n",
      "Speech:The speech discusses **averaging/bagging** as a technique to improve the performance of machine learning models, particularly in the context of regression. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:**  You have multiple independent datasets (each with its own estimator) and want to combine them to get a better overall predictor.\n",
      "* **Solution:** Bagging involves averaging the predictions from each individual dataset's estimator. \n",
      "* **Benefits:**\n",
      "    * **Reduced Variance:** Since the datasets are independent, averaging them reduces the overall variance of the predictions. This leads to a more stable and generalized model.\n",
      "    * **Potential Computational Gains:**  Sometimes, learning a single model on a combined dataset can be computationally expensive. Bagging allows you to train multiple smaller models and then average their predictions, which might be more efficient.\n",
      "* **Trade-off:** Bagging generally leads to a higher variance (lower bias) estimator compared to using a single dataset.\n",
      "\n",
      "The speech then provides a specific example using k-nearest neighbor regression and analyzes how bagging impacts its bias and variance.\n",
      "\n",
      "**In essence, bagging is a way to leverage the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction.** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This passage discusses the performance of a distributed learning algorithm where a dataset is divided into chunks and each chunk is estimated independently. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **The Setup:** The algorithm divides a dataset of size N into m smaller chunks, each containing n = N/m observations. A separate model is trained on each chunk.\n",
      "\n",
      "* **Theoretical Analysis:** The authors analyze the excess risk (the difference between the algorithm's performance and the best possible performance) of this distributed approach. They compare it to a scenario where the entire dataset is used to train a single model.\n",
      "* **Key Findings:**\n",
      "    *  The excess risk of the distributed approach scales as 1/(mn)^(2/(d+2)), where d is the dimension of the data. This is the same rate as a single model trained on the entire dataset (N observations).\n",
      "    *  The number of chunks (m)  influences the number of neighbors (k) used in the estimation process. More chunks lead to fewer neighbors, favoring higher variance estimators.\n",
      "* **Practical Implications:**\n",
      "    *  While the distributed approach doesn't offer a statistical advantage over a single model, it can be significantly faster if the chunk estimations can be computed in parallel. \n",
      "    *  There's a limit to the number of chunks (m) that can be used for parallelization to maintain the optimal performance rate. The number of chunks must be smaller than n^(2/d).\n",
      "\n",
      "**In essence, the passage highlights that dividing a dataset for parallel processing doesn't improve the overall accuracy of a learning algorithm, but can significantly speed up the training process.**\n",
      "\n",
      "\n",
      "This excerpt discusses ensemble learning methods, particularly focusing on the use of multiple independent datasets to improve classification accuracy. \n",
      "\n",
      "Here are the key points:\n",
      "\n",
      "* **k-Nearest Neighbors (k-NN) with Majority Voting:** The speech derives an upper bound on the expected risk for k-NN multicategory classification with a majority vote rule. It highlights the relationship between the quadratic and 0-1 loss functions.\n",
      "\n",
      "* **Ridge Regression with Multiple Datasets:** The speech analyzes the performance of ridge regression when using multiple datasets. It notes that, similar to the full dataset analysis, the optimal regularization parameter (λ) is lower, and there's no statistical advantage in using multiple datasets. However,  computational benefits arise due to the potential for parallel processing and faster training times.\n",
      "\n",
      "* **Computational Complexity:** Exercises 10.2 and 10.3 delve into the computational complexity of ridge regression with data splitting and ensemble learning methods, respectively. They explore the optimal number of data splits (m) to minimize training time.\n",
      "\n",
      "* **Ensemble Classifier with Majority Voting:** Exercise 10.3 focuses on a specific ensemble classifier where each base learner (ˆf(j)) is trained on a separate dataset and the final prediction is made by majority voting. It derives an upper bound on the expected misclassification rate and explores the relationship between the expected excess risk and the individual base learner's performance.\n",
      "\n",
      "\n",
      "Overall, this excerpt emphasizes the potential benefits and computational considerations of utilizing multiple datasets in machine learning, particularly for classification tasks.\n",
      "\n",
      "\n",
      "This passage discusses the concept of **bagging** (Bootstrap Aggregating) as a technique to reduce the variance of machine learning models.  \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:**  Standard machine learning models can be sensitive to fluctuations in the training data (high variance).\n",
      "\n",
      "* **Solution:** Bagging aims to create multiple \"replica\" datasets from a single original dataset. These replicas are built by sampling data points *with replacement*, meaning some points might appear multiple times in a replica, while others might be left out.\n",
      "\n",
      "* **Bootstrap:** The specific method used for creating these replicas is called **bootstrapping**.\n",
      "\n",
      "* **Bagging Process:**\n",
      "    *  Randomly sample data points *with replacement* from the original dataset *m* times.\n",
      "    *  Train a separate model on each of these *m* replica datasets.\n",
      "    *  Combine the predictions from all *m* models (often by averaging) to produce the final prediction.\n",
      "\n",
      "* **Benefits:**\n",
      "    * **Variance Reduction:** By averaging predictions from diverse models trained on slightly different data, bagging effectively reduces the impact of outliers and random noise in the original dataset.\n",
      "    * **Stability:**  The passage mentions that  \"infinitely many bootstrap replications\" (although impractical) lead to a form of stabilization, implying greater robustness to data variations.\n",
      "\n",
      "* **Trade-off:** While bagging reduces variance, it can potentially increase bias (systematic errors) compared to a single, well-trained model.\n",
      "\n",
      "* **Practical Implementation:** In practice, a finite number of bootstrap replications (m) is used, with a moderate value often sufficient to achieve significant variance reduction.\n",
      "\n",
      "\n",
      "Let me know if you'd like more details on any specific aspect of bagging! \n",
      "\n",
      "\n",
      "This speech excerpt discusses **bagging**, a statistical technique for reducing the variance of an estimator (like a prediction model).  \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Bagging's Core Idea:**  Bagging involves creating multiple subsets of the training data by randomly sampling with replacement. A separate estimator is trained on each subset, and their predictions are then averaged. This averaging process smooths out the individual predictions, leading to a more stable and generalizable model.\n",
      "\n",
      "* **Benefits of Bagging:**\n",
      "    * **Reduces Variance:**  Bagging is particularly effective for high-variance predictors, leading to more reliable predictions.\n",
      "    * **Computational Gain:**  While bagging doesn't always offer a significant statistical advantage, it can be computationally efficient, especially when individual estimators are fast to compute.\n",
      "\n",
      "* **Trade-offs:**\n",
      "    * **Increased Bias:** Bagging can slightly increase the bias of the estimator. This is a common trade-off in regularization methods.\n",
      "\n",
      "* **Example: One-Nearest Neighbor Regression**\n",
      "    * The speech focuses on how bagging significantly improves the performance of the 1-nearest neighbor regression method.\n",
      "    * It explains how the bagged estimate is calculated by averaging the predictions from multiple 1-nearest neighbor models trained on different random subsets of the data.\n",
      "\n",
      "* **Key Observation:** The weight assigned to each neighbor's prediction reflects the probability of that neighbor being the closest in a random sample.\n",
      "\n",
      "* **Further Exploration:**\n",
      "    * The speech mentions \"random forests,\" another ensemble learning technique that builds on bagging principles. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "This passage discusses a technical aspect of nearest neighbor algorithms in machine learning, specifically focusing on the **variance** of predictions when using a subsampled set of data points.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:** The speech delves into the probability that the *ith* nearest neighbor to a given data point *x* will also be the *1st* nearest neighbor in a smaller, randomly selected subset of data (called a \"uniform subsample\").\n",
      "* **Sampling:**  The subsample is created without replacement, meaning once a data point is chosen, it's not put back in. This is important because it affects the probabilities involved.\n",
      "* **Calculations:** The speech derives a formula (*Vi*) to calculate the probability of the *ith* nearest neighbor being the *1st* in the subsample. This involves considering the number of ways to choose the subsample and the conditions required for the *ith* neighbor to be the closest.\n",
      "* **Variance Reduction:**  The main point is that using a smaller subsample can potentially reduce the variance of predictions. This is because the smaller set of neighbors may be more representative of the local neighborhood around *x*.\n",
      "* **Bounding the Variance:** The speech uses the derived formula (*Vi*) to bound the overall variance of predictions. This involves summing up the individual probabilities (*Vi*) squared.\n",
      "\n",
      "\n",
      "**In essence, the speech demonstrates a mathematical framework for understanding how subsampling can influence the accuracy and stability of nearest neighbor algorithms.** \n",
      "\n",
      "\n",
      "This excerpt discusses ensemble learning techniques, specifically bagging and random projections, for reducing variance and improving the performance of machine learning models. \n",
      "\n",
      "**Bagging:**\n",
      "\n",
      "* Bagging involves creating multiple subsets (with replacement) of the original training data. A model is trained on each subset, and their predictions are averaged to produce the final prediction.\n",
      "* The passage analyzes the theoretical bias-variance tradeoff of bagging.  It shows that bagging reduces variance by averaging predictions from different models trained on different subsets of the data. However, it also introduces some bias because each subset might not perfectly capture the underlying pattern in the data.\n",
      "* The optimal number of subsets (s) is shown to be proportional to the square root of the training data size (n), balancing the tradeoff between bias and variance.\n",
      "\n",
      "**Random Projections:**\n",
      "\n",
      "* Random projections offer an alternative to bagging by reducing the dimensionality of the data before training the model. This is achieved by randomly selecting a subset of features or projecting the data into a lower-dimensional subspace.\n",
      "* This technique can be applied to various algorithms, including ordinary least squares (OLS). \n",
      "* Random projections can help reduce the number of samples while keeping the dimension fixed, potentially speeding up training.\n",
      "\n",
      "**Overall:**\n",
      "\n",
      "This passage illustrates that both bagging and random projections are valuable ensemble learning techniques for improving the performance of machine learning models. Bagging focuses on averaging predictions from diverse models, while random projections focus on dimensionality reduction to reduce variance.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any further questions.\n",
      "\n",
      "\n",
      "This speech excerpt discusses using **random projections** to approximate the solution of an **ordinary least-squares (OLS)** problem. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **OLS Problem:** The goal is to find the best-fitting vector **θ** in a high-dimensional space (d dimensions) that explains the response vector **y** (n dimensions) using a design matrix **Φ** (n x d). \n",
      "* **The Challenge:**  Finding the best **θ** directly can be computationally expensive, especially when dealing with large datasets (n >> d).\n",
      "* **Random Projections Solution:**\n",
      "    *  Introduce a random Gaussian matrix **S** (s x n) with fewer rows (s) than observations (n) and independent zero-mean, unit-variance elements.\n",
      "    *  Replace the original OLS objective function (minimizing the squared error between **y** and **Φθ**) with a projected version (minimizing the squared error between **Sy** and **SΦθ**).\n",
      "\n",
      "Essentially, random projections act as a dimensionality reduction technique, allowing us to approximate the solution to the OLS problem more efficiently by working with a smaller, randomly sampled subset of the data. \n",
      "\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **s < n < d:** Typically, the number of rows in **S** (s) is smaller than the number of observations (n), and both are larger than the feature dimension (d).\n",
      "* **Idealization:** This approach is an idealized version of subsampling, which is a common technique used in practice to reduce the computational burden of OLS.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire speech with these important points.\n",
      "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "points for the speech.\n",
      "Speech:This speech segment discusses the benefits of **random projection** as a technique for dimensionality reduction in machine learning, particularly in the context of **sketches**  and **ordinary least squares (OLS) regression**.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Dimensionality Reduction:** Random projection reduces the dimensionality of data (from d dimensions to s dimensions) by projecting data points onto a lower-dimensional space using a random projection matrix (S). This results in a compressed representation of the data (Rs×d instead of Rn×d).\n",
      "* **Benefits:**\n",
      "    * **Reduced Computational Time:**  Working with lower-dimensional data speeds up computations.\n",
      "    * **Regularization:** Random projection acts as a form of regularization, preventing overfitting.\n",
      "* **Implementation:** The speech mentions using Gaussian matrices and subsampling matrices for random projection. \n",
      "* **Connection to Kernel Methods:** Random projection is related to kernel methods, which also implicitly map data to higher-dimensional spaces.\n",
      "* **Comparison to Random Forests:**  The speech briefly compares random projection to the popular  \"random forests\" algorithm, which combines dimensionality reduction through random feature selection with bagging (bootstrapping and averaging).\n",
      "\n",
      "\n",
      "**Overall, the speech highlights random projection as a valuable tool for simplifying high-dimensional data, improving computational efficiency, and mitigating overfitting in machine learning models.** \n",
      "\n",
      "\n",
      "This speech excerpt introduces **Gaussian Sketching**, a technique for simplifying large-scale linear regression problems.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Problem Setup:** The speech focuses on the fixed design setting where the design matrix Φ (representing features) has full rank (n >= d, where n is the number of samples and d is the number of features).\n",
      "* **Gaussian Projections:** The core idea is to use multiple (s) random Gaussian matrices S(j) to project the data into a lower-dimensional space. These projections are independent of each other.\n",
      "* **Estimator:** The estimator ˆθ is obtained by averaging the results of individual sketches: ˆθ = (1/m) * Σ(ˆθ(j)), where ˆθ(j) is the estimate from the j-th sketch.\n",
      "* **Goal:** The speech aims to analyze the **fixed design error**, which measures the difference between the estimated parameters ˆθ and the true parameters θ*  when considering both the noise in the data (ε) and the randomness introduced by the Gaussian projections.\n",
      "* **SVD Decomposition:** The speech uses the Singular Value Decomposition (SVD) of the design matrix Φ to simplify the mathematical expressions involved in the analysis.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "1. **SVD:** The design matrix is decomposed into U, D, and V matrices.\n",
      "2. **Projection:** The Gaussian projections are represented as S(j)1 and S(j)2 matrices, allowing for a clear expression of how the data is projected.\n",
      "3. **Normal Equation:** The normal equation is used to derive the closed-form estimator ˆθ(j) for each sketch.\n",
      "4. **Error Analysis:** Expectations and variances are computed with respect to the random projections (S(j)) while fixing the noise vector ε.\n",
      "\n",
      "\n",
      "The speech essentially sets the stage for analyzing the performance of Gaussian Sketching in reducing the complexity of large-scale linear regression.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "This passage demonstrates how to derive the ordinary least squares (OLS) estimator for a linear regression model using singular value decomposition (SVD).  \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. **Start with the prediction formula:** The author begins with the general formula for predicting the output (y) using a linear model with parameters θ and feature matrix Φ:\n",
      "\n",
      "   Φˆθ(j) = (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\n",
      "\n",
      "2. **Utilize SVD:**  The passage assumes a specific SVD representation of Φ: S(j)Φ = S(j)1DV⊤.  It then substitutes this into the prediction formula and simplifies the expression step-by-step.\n",
      "\n",
      "3. **Derive OLS estimator:** Through these algebraic manipulations, the author shows that the resulting prediction vector  Φˆθ(j)  is equivalent to the OLS estimator:\n",
      "\n",
      "   ΦˆθOLS = Φ(Φ⊤Φ)−1Φ⊤y\n",
      "\n",
      "4. **Uniqueness of the estimator:**  The passage further argues that if the number of features (fs) is greater than or equal to the number of parameters (d), then the estimator  ˆθ(j) is uniquely defined.\n",
      "\n",
      "**In essence, the passage provides a mathematical proof showcasing the connection between using SVD and obtaining the OLS estimator in a linear regression model.** \n",
      "\n",
      "\n",
      "Let me know if you'd like a deeper explanation of any specific step!\n",
      "\n",
      "\n",
      "This speech discusses randomized projection techniques for dimensionality reduction, particularly focusing on their application in estimating model parameters. \n",
      "\n",
      "The key points are:\n",
      "\n",
      "* **Problem:** The speech addresses the high-dimensional data problem where traditional methods like Ordinary Least Squares (OLS) can be computationally expensive.\n",
      "* **Solution:** Randomized projection using Gaussian sketching matrices is proposed as a solution. This involves projecting the data onto a lower-dimensional subspace using a random matrix, which allows for faster computation.\n",
      "* **Analysis:** The speech analyzes the expected generalization error of the estimated parameters using these projections. It shows that:\n",
      "    *  For a large number of projections (m), the performance of the randomized method approaches that of OLS.\n",
      "    *  For a finite number of projections, the performance degrades gracefully.\n",
      "    *  For a special case where the number of samples (n) equals the dimensionality of the data (d), even with a single projection, the performance is significantly better than OLS.\n",
      "* **Computational Benefits:** While there is no statistical gain over OLS, the randomized projection offers potential computational advantages due to parallel processing and reduced storage requirements.\n",
      "* **Generalization:** The speech acknowledges that the analysis presented here uses Gaussian sketching matrices for simplicity. It suggests that the techniques can be extended to other random sketching matrices with potentially better computational properties.\n",
      "\n",
      "\n",
      "Essentially, the speech presents randomized projection as a practical and efficient alternative to traditional methods for handling high-dimensional data, especially when computational resources are limited.\n",
      "\n",
      "\n",
      "This passage discusses the use of random sketching matrices, particularly focusing on Wishart distributions and their properties. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Wishart Distributions:**  The speaker mentions Wishart distributions (specifically the inverse Wishart distribution) and their connection to random sketching matrices. These matrices are used in data analysis to reduce dimensionality and complexity.\n",
      "* **Matrix Properties:** The passage highlights key properties of these matrices:\n",
      "    *  If matrix *S* is formed from independent standard Gaussian components, its transpose multiplied by itself (*S⊤S*) has a specific expected value if *a* (rows) is greater than *b* (columns) + 1.\n",
      "    * Similarly, the product of *S* by its transpose (*SS⊤*) has a known expected value equal to *b* times the identity matrix.\n",
      "* **Extensions:** The speaker suggests that these analysis methods can be applied to more complex random sketching matrices with beneficial computational characteristics, such as those with many zeros. This allows for subsampling of observations or dimensions.\n",
      "\n",
      "* **Further Reading:** The passage provides references to other research papers (Wang et al., 2018; Dobriban and Liu, 2019) for those interested in exploring the topic further.\n",
      "\n",
      "\n",
      "In essence, the passage sets the stage for discussing how random sketching matrices, particularly those based on Wishart distributions, can be effectively used in data analysis for dimensionality reduction and simplification. \n",
      "\n",
      "\n",
      "This speech excerpt discusses a technique called **random projections** used for regularization in high-dimensional machine learning problems. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:** The setup involves a high-dimensional design matrix (Φ) with more features (d) than samples (n).  This can lead to overfitting.\n",
      "\n",
      "* **Solution:** Random projections introduce randomness by using sketching matrices (S(j)) to project the data onto a lower-dimensional subspace (s < d). This effectively reduces the dimensionality of the problem.\n",
      "\n",
      "* **Estimator:** For each projection, the model parameter (η(j)) is estimated by minimizing the squared error between the projected data and the true response. \n",
      "\n",
      "* **Averaging:** The final estimate (θ) is obtained by averaging the parameter estimates across all projections.\n",
      "\n",
      "* **Properties:**\n",
      "    * The projection matrix (Π(j)) acts as an orthogonal projection, mapping data onto the s-dimensional subspace.\n",
      "    * The expected value of Π(j) is denoted as ∆, which is a projection matrix with trace equal to s and eigenvalues between 0 and 1.\n",
      "    * This framework allows for calculating the expected generalization error, which measures the model's performance on unseen data.\n",
      "\n",
      "**In essence, random projections act as a form of dimensionality reduction and regularization, helping to prevent overfitting in high-dimensional settings.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "The speech presents a mathematical formula for calculating the expected generalization error of a fixed-design machine learning model. \n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "* **Generalization Error:** This refers to how well a trained model will perform on unseen data. The goal is to minimize this error.\n",
      "* **Fixed-Design:**  This means the training data (represented by matrices S(1)..., S(m)) is fixed and doesn't change during training.\n",
      "* **Formula:** The formula calculates the expected generalization error by taking the average of squared errors between the model's predictions (ˆy(j)) and the true values (Φθ∗), considering the randomness introduced by the noise term (ε) and the data matrices.\n",
      "\n",
      "**Key Steps:**\n",
      "\n",
      "1. **Initialization:** The formula starts with the expected generalization error based on independent and identically distributed (i.i.d.) data matrices.\n",
      "\n",
      "2. **Expectation:** It then takes the expectation with respect to all data matrices S(j).\n",
      "\n",
      "3. **Simplification:** Using equations (10.1) and (10.2), the formula is simplified to express the expected generalization error in terms of:\n",
      "    * The noise term (ε)\n",
      "    * The difference between the model's predictions and the true values (∆ε)\n",
      "    * The model parameters (θ∗)\n",
      "    * The data matrices (S(j))\n",
      "\n",
      "**In essence, the speech provides a closed-form expression for calculating the expected generalization error in a fixed-design machine learning setting.** This expression can be used to analyze and compare different models, and to understand how the model's performance is affected by various factors like noise and data distribution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire speech with these important points.\n",
      "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "points for the speech.\n",
      "Speech:This speech excerpt delves into the analysis of a specific statistical model, denoted by *y = Φθ* + ε, where *y* is the output, *Φθ* represents a linear transformation involving parameters *θ*, and *ε* is an error term. \n",
      "\n",
      "The central focus is on minimizing a risk function related to the estimation of *θ* based on multiple replications of data. The speech presents mathematical derivations and inequalities to establish upper bounds on the expected value of the risk function.\n",
      "\n",
      "Key points:\n",
      "\n",
      "* **Model & Assumptions:**  The model assumes a linear relationship between input and output, with *ε* being a zero-mean random variable with a covariance matrix proportional to the identity.\n",
      "* **Risk Function:** The risk function quantifies the difference between the estimated *θ* and the true *θ*.\n",
      "* **Bound Derivation:** The speech employs various mathematical techniques, including matrix manipulations and projection properties, to derive an upper bound for the expected value of the risk function.\n",
      "* **Impact of Replications (m):** The analysis shows that the risk function decreases as the number of replications (m) increases, ultimately converging to a specific value as *m* approaches infinity.\n",
      "* **Importance of Projection Matrices:** The concept of projection matrices (Π) plays a crucial role in the derivation, specifically in relating the difference between the true and estimated *θ* to the projection of *z* onto a specific subspace.\n",
      "\n",
      "Overall, this speech excerpt provides a rigorous mathematical treatment of risk estimation in a linear model, highlighting the significance of multiple replications and the role of projection matrices in understanding the relationship between estimation error and data.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like more details on a specific part of the speech!\n",
      "\n",
      "\n",
      "This passage discusses a technique called **random projections** used in ensemble learning, particularly in the context of reducing the complexity of models and mitigating overfitting. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Ensemble Learning:**  This involves combining multiple models to improve overall performance.\n",
      "* **Random Projections:** A dimensionality reduction technique that randomly projects high-dimensional data onto a lower-dimensional space. This helps simplify the data and potentially improve generalization.\n",
      "* **Bias-Variance Tradeoff:** The passage highlights a trade-off similar to ridge regression, where increasing the number of projections (s) reduces variance but may increase bias. \n",
      "\n",
      "The specific focus is on using **Gaussian random projections** (i.e., projections based on independent standard Gaussian random variables).\n",
      "\n",
      "* **Mathematical Derivation:** The passage provides a mathematical derivation demonstrating how to bound the excess risk (the difference between the model's performance on the training data and its performance on unseen data) using Gaussian projections.\n",
      "\n",
      "* **Key Insights:**\n",
      "\n",
      "    * Random projections effectively reduce the dimensionality of the data while preserving important information.\n",
      "    * The number of projections (s) plays a crucial role in balancing bias and variance.\n",
      "    * The bound on excess risk resembles the one obtained in ridge regression, suggesting similarities in the underlying principles.\n",
      "\n",
      "* **Exercise:** The passage concludes with an exercise encouraging the reader to explore the properties of another type of random projection (using uniformly random basis vectors).\n",
      "\n",
      "\n",
      "Let me know if you have any further questions.\n",
      "\n",
      "\n",
      "This speech excerpt discusses applying the concept of **random projections** to **kernel methods**. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Kernel Methods:**  These methods use a kernel matrix (K) to implicitly represent data in a higher-dimensional space, making it easier to learn complex relationships.\n",
      "* **Random Projections:**  This technique involves projecting data onto a random subspace of lower dimension. This simplifies computations while preserving essential information.\n",
      "* **Adapting Random Projections to Kernels:** The speech explains how to generalize the random projection idea to kernel methods. Instead of working directly with the data points (Φ), we use the kernel matrix (K) to represent them.\n",
      "\n",
      "* **Key Steps:**\n",
      "    1. **Approximate Kernel Matrix:** Create a random sketch matrix (ˆΦ) and use it to approximate the kernel matrix (ˆK).\n",
      "    2. **Projection:** Project the data onto the random subspace using the approximate kernel matrix: ˆy = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y.\n",
      "    3. **Analysis:** Analyze the projection operator (Π = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤) and its expectation (∆) to understand how well the random projection preserves the original data structure.\n",
      "    4. **Bound on Excess Risk:** Derive a bound on the expected excess risk (difference between the true prediction and the projected prediction) using the properties of the random projection.\n",
      "\n",
      "* **Gaussian Sampling:** The speech mentions a specific way to sample the columns of the random sketch matrix (ˆΦ) from a Gaussian distribution with mean zero and covariance matrix K. This helps in calculating the expectation of the projection operator.\n",
      "\n",
      "* **Practical Implications:** While the derived bound on the excess risk is theoretically important, it often requires computing the full kernel matrix (K), which can be computationally expensive for large datasets.\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific aspect! \n",
      "\n",
      "\n",
      "The speech discusses the theoretical bound for kernel methods (equation 10.4) which is based on calculating the square root of the kernel matrix. While this bound is mathematically sound, it's impractical due to the high computational cost (O(n^3)) involved in computing the kernel matrix and the subsequent square root operation for sampling from a multivariate Gaussian distribution. \n",
      "\n",
      "To address this, the speech highlights a practical alternative: random feature expansion. This technique approximates the kernel function using a simplified form involving random vectors 'v'. This approximation significantly reduces computational complexity while maintaining a bound on the magnitude of the expanded features. \n",
      "\n",
      "\n",
      "This speech excerpt discusses **Gaussian random projections** as a technique for dimensionality reduction in machine learning, particularly in the context of **ensemble learning**. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:** High-dimensional data (many features) can be computationally expensive to process.\n",
      "\n",
      "* **Solution:**  Gaussian random projections randomly map the data to a lower-dimensional space (s dimensions) while preserving pairwise distances.\n",
      "\n",
      "* **How it works:**  \n",
      "    * A random matrix S with independent standard Gaussian random variables is used to project the data.\n",
      "    * The resulting projection is denoted as S<sup>-1/2</sup>S<sup>⊤</sup>ϕ, where ϕ is the original data point.\n",
      "* **Theoretical Guarantee:** The Johnson-Lindenstrauss lemma provides a probabilistic guarantee that distances between points are preserved up to a small factor (1-ε) and (1+ε) after projection, where ε is a small constant and s grows logarithmically with the original dimension n.\n",
      "* **Practical Implementation:** The speech explains how to apply this technique in the context of ensemble learning, using it to reduce the dimensionality of feature spaces before building models. It also discusses the impact of the number of projections (m) and provides experimental results demonstrating the effectiveness of this approach, particularly for polynomial regression problems.\n",
      "\n",
      "**Key Benefits:**\n",
      "\n",
      "* **Efficiency:**  Reduces computational cost by working in a lower-dimensional space.\n",
      "* **Performance:** Preserves important geometric relationships in the data, leading to better model performance.\n",
      "\n",
      "\n",
      "The speech emphasizes the practical advantages of using Gaussian random projections for dimensionality reduction, particularly in the context of ensemble learning, and backs up these claims with experimental evidence and theoretical guarantees.\n",
      "\n",
      "\n",
      "The speech provides a proof (marked with ♦) for a specific inequality related to random matrices. \n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "1. **Inequality:** The speech focuses on proving an inequality involving the norms of vectors  `ϕi` and `ϕj`  related to a matrix `S`. The inequality states that with a probability greater than 1-δ, the norm of the difference between two vectors `ϕi` and `ϕj`  after a specific transformation involving `S` will be within a factor of (1-ε) and (1+ε) of the original norm.\n",
      "\n",
      "2. **Proof Approach:**\n",
      "   -  The proof leverages properties of chi-squared random variables. \n",
      "   -  It defines a random variable `y` based on the matrix `S` and a unit vector `ψ`.\n",
      "   -  `y` is shown to follow a chi-squared distribution.\n",
      "\n",
      "3. **Concentration Results:**\n",
      "   - The speech utilizes concentration results (specifically from Exercise 8.1) for chi-squared distributions to bound the probability of `y` deviating significantly from its mean (which is `s`).\n",
      "\n",
      "4. **Probability Bound:**\n",
      "   -  Using the properties of logarithms, the probability bound is simplified to  2 * exp(-s * ε^2 / 3).\n",
      "\n",
      "**In essence, the speech demonstrates how concentration results for chi-squared distributions can be used to establish a probabilistic bound on the difference between the norm of a transformed vector and its original norm.**\n",
      "\n",
      "\n",
      "Let me know if you would like me to elaborate on any specific part of the proof or the concepts involved!\n",
      "\n",
      "\n",
      "This speech excerpt discusses the use of **random projections** for improving the performance of **least-squares regression**, particularly in high-dimensional settings.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Problem:**  Least-squares regression can struggle with high-dimensional data, leading to overfitting and poor generalization.\n",
      "* **Solution:** Random projections are used to reduce the dimensionality of the data while preserving its essential structure. This helps to avoid overfitting and improve predictive accuracy.\n",
      "* **Technical Details:**\n",
      "\n",
      "    * The Johnson-Lindenstrauss lemma is mentioned as a theoretical basis for understanding how random projections maintain data relationships.\n",
      "    *  The speech argues for a direct proof approach to analyze the predictive performance of random projections in the context of unregularized regression. This contrasts with using the Johnson-Lindenstrauss lemma, which can be complex in this setting due to the challenges of controlling matrix inverse perturbations.\n",
      "    * The speech also presents empirical evidence (Figure 10.2) comparing the performance of ridge regression (a regularized method) with random projections for a specific dataset and different numbers of random projections.\n",
      "\n",
      "* **Key Takeaways:** Random projections offer a promising technique for dealing with the complexities of high-dimensional regression, potentially leading to more accurate and robust models. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "This excerpt from a chapter on Ensemble Learning focuses on **Boosting**, a sequential method for improving prediction accuracy. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Boosting vs. Previous Methods:** Unlike previous ensemble methods that combined estimators in parallel using uniformly reweighted data, boosting involves **iteratively reweighting the data** based on the mistakes made by the previous prediction functions. This sequential approach leads to additional statistical benefits.\n",
      "\n",
      "* **Focus on Gradient Boosting:** The chapter specifically discusses **gradient boosting**, which is adapted to real-valued outputs (though it can be used for classification with convex surrogates).\n",
      "\n",
      "* **Problem Setup:** The core idea is to find a sequence of functions  ϕ(·,w) that minimize the weighted sum of errors.\n",
      "    * **Input:**  We have data points (xi, yi) where xi is an input and yi is the corresponding real-valued output.\n",
      "    * **Predictors:**  We have a set of \"weak learners\" ϕ(·,w), which are functions parameterized by w, that can be trained to minimize the weighted error on the data.\n",
      "    * **Weak Learner Assumption:**  The main assumption is that we can easily find the best w for each iteration, minimizing the weighted sum of errors.\n",
      "\n",
      "* **The Goal:** The ultimate goal is to combine these weak learners sequentially to create a strong learner that significantly outperforms any individual weak learner.\n",
      "\n",
      "\n",
      "The chapter promises a more detailed exploration of boosting theory and its connections to other concepts later on.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire speech with these important points.\n",
      "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "points for the speech.\n",
      "Speech:This section of the text discusses a simplifying assumption in machine learning called the \"weak learner\" assumption. \n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Weak Learner Assumption:**  It assumes that a learning algorithm can be found to create a weak learner, which is a model that performs slightly better than random guessing.\n",
      "* **Example: Linear Stumps:** A specific example of a weak learner is a linear stump for data in Rd (dimensional space). A linear stump makes a prediction based on a linear combination of features, with weights (w). \n",
      "    *  The weights can be restricted so that only one component is non-zero, making the prediction even simpler.\n",
      "* **Connection to Neural Networks:** This type of weak learner can be seen as a one-hidden-layer neural network, but learned sequentially rather than using gradient descent on the training data.\n",
      "* **Binary Classification:** In binary classification, the output of the weak learner might be thresholded to {-1, 1} based on its sign.\n",
      "* **Exercise:** The text provides an exercise to demonstrate how to efficiently minimize a specific equation (equation 10.7) related to linear stumps with only one non-zero weight component.\n",
      "\n",
      "\n",
      "Essentially, the section introduces the concept of weak learners and provides a concrete example (linear stumps) to illustrate this idea. It also highlights the connection between weak learners and simple neural networks.  \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire speech with these important points.\n",
      "Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "points for the speech.\n",
      "Speech:## Motivational Title:  **Unlocking Machine Learning's Potential:  The Power of Averaging (Bagging) and Dimensionality Reduction**\n",
      "\n",
      " **Introduction:**  This speech explores two powerful techniques used in machine learning to enhance model performance and efficiency: bagging and random projections. \n",
      "\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "1. **The Problem:** Machine learning models can be sensitive to fluctuations in the training data, leading to high variance (inaccurate predictions on new data).\n",
      "2. **Bagging's Solution:** Bagging (Bootstrap Aggregating) addresses this by creating multiple subsets of the training data and training a separate model on each subset. The final prediction is an average of the predictions from all models, effectively reducing variance.\n",
      "3. **Benefits of Bagging:**\n",
      "    * **Reduced Variance:** Bagging smooths out the predictions, leading to more stable and reliable models.\n",
      "    * **Computational Gain:** Training multiple smaller models can be more efficient than training a single large model on the entire dataset.\n",
      "4. **Trade-offs:** Bagging can slightly increase bias (systematic errors) compared to a single, well-trained model.\n",
      "5. **Example: One-Nearest Neighbor Regression:** The speech illustrates how bagging significantly improves the performance of the 1-nearest neighbor regression method, which is particularly prone to variance issues.\n",
      "6. **Random Projections for Dimensionality Reduction:** \n",
      "    *  Random projections offer a different approach by reducing the dimensionality of the data before training the model.\n",
      "    * This is achieved by projecting the data into a lower-dimensional subspace using a random Gaussian matrix.\n",
      "7. **Benefits of Random Projections:**\n",
      "    *  Reduces computational complexity by working with a smaller dataset.\n",
      "    *  Can be applied to various algorithms, including OLS.\n",
      "8. **Trade-offs:**  Random projections introduce some information loss due to dimensionality reduction.\n",
      "\n",
      "\n",
      "**Conclusion:**  Both bagging and random projections are valuable tools in a machine learning practitioner's arsenal, offering ways to improve model performance and efficiency.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is a great overview of the speech! The summaries are clear, concise, and accurately reflect the key points. Here's a possible title, combining motivation and the central theme:\n",
      "\n",
      "\n",
      "**Motivation Title:**  **Unlocking Efficiency in High-Dimensional Learning:  The Power of Random Projections** \n",
      "\n",
      "\n",
      "I've combined the ideas of efficiency and high-dimensional learning, which are strongly emphasized throughout the speech.  \n",
      "\n",
      "Let me know if you'd like to brainstorm other title options!\n",
      "\n",
      "This summary is excellent! It captures the key points of the speech excerpt concisely and accurately.  \n",
      "\n",
      "Here are a few minor suggestions:\n",
      "\n",
      "* **Motivation Title:** Consider a title that reflects the specific focus of the speech. For example, \"Random Projections for Improved Kernel Method Efficiency\" or \"Gaussian Random Projections in Ensemble Learning.\"\n",
      "* **Point 4 (Impact of Replications):** You could rephrase this to emphasize the practical implication. For example: \n",
      "\n",
      ">  \"The analysis demonstrates that increasing the number of data replications (m) leads to a decrease in the risk function, ultimately converging to a specific value as m approaches infinity.\"\n",
      "\n",
      "* **Point 6 (Gaussian Sampling):** This point could be made even clearer by explicitly mentioning that this Gaussian sampling method is used to ensure that the random projection operator's expectation is unbiased.\n",
      "\n",
      "* **Overall Flow:** The transitions between points are smooth, but you could consider adding a brief concluding sentence to summarize the overall significance of the speech.\n",
      "\n",
      "\n",
      "Overall, this is a well-structured and informative summary of the speech excerpt! \n",
      "\n",
      "\n",
      "## Unlocking Power Through Weakness: The Foundation of Boosting Algorithms\n",
      "\n",
      "This section delves into the intriguing concept of \"weak learners\" – a simplifying assumption in machine learning that underpins powerful boosting algorithms. It reveals how seemingly simple models, when strategically combined, can achieve remarkable predictive accuracy. \n",
      "\n",
      "Here are the key takeaways:\n",
      "\n",
      "1. **Defining Weak Learners:** A weak learner is a model that performs only slightly better than random guessing. Despite their limited individual performance, they form the building blocks of more powerful learning systems.\n",
      "\n",
      "2. **Linear Stumps: A Concrete Example:** The text uses linear stumps as a concrete illustration of weak learners. These models make predictions based on a linear combination of features, with a unique constraint: only one feature weight is allowed to be non-zero. This simplicity makes them easy to understand and train.\n",
      "\n",
      "3. **Connection to Neural Networks:**  Interestingly, a single-layer neural network can be viewed as a type of weak learner.  The learning process, however, differs from traditional gradient descent methods used in neural networks. Weak learners are learned sequentially.\n",
      "\n",
      "4. **Binary Classification with Thresholding:** In binary classification tasks, the output of a weak learner might be thresholded to {-1, 1} based on its sign, effectively assigning categories to the input data.\n",
      "\n",
      "5. **Practical Exercise:** The text includes an exercise to demonstrate how to efficiently minimize a specific equation related to linear stumps. This hands-on approach reinforces the understanding of weak learner principles.\n",
      "\n",
      "\n",
      "\n",
      "By introducing the concept of weak learners and providing a tangible example, this section lays the groundwork for understanding the powerful boosting algorithms that leverage the collective strength of these seemingly simple models. \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a great summary! You\\'ve accurately captured the key ideas and provided a clear explanation of how weak learners work. \\n\\nHere are a few minor suggestions:\\n\\n\\n* **Motivation Title:** Consider a title that emphasizes the power of weak learners in combination. Some options could be:\\n\\n    * **\"The Strength in Numbers:  Unleashing Power Through Weak Learners\"**\\n    *  **\"Building Giants from Little Bricks:  The Foundation of Boosting Algorithms\"**\\n\\n* **Point 2 (Linear Stumps):**  You could elaborate slightly on why linear stumps are considered weak learners. For example, you could mention their limited expressive power due to the single non-zero feature weight constraint.\\n\\n* **Point 3 (Neural Networks):** While it\\'s true that a single-layer neural network can be viewed as a weak learner, it\\'s important to note that the training process is significantly different from traditional gradient descent used in boosting algorithms. Consider adding a sentence to clarify this distinction.\\n\\n* **Conclusion:** You could end with a sentence emphasizing the broader significance of weak learners in the field of machine learning, such as their role in enabling the development of powerful ensemble methods like AdaBoost and Gradient Boosting. \\n\\n\\n\\nOverall, this is a well-structured and insightful summary that effectively conveys the core concepts of weak learners.  \\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = summary_chain.run(final_docs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine chain for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"10.1. AVERAGING/BAGGING 285\n",
      "ized situation, we have mindependent datasets of size n, composed of independent and\n",
      "identically distributed (i.i.d.) observations from the same distribution p(x,y) onX×Y.\n",
      "We obtain for each of them an estimator ˆf(j)\n",
      "λ, wherej∈{1,...,m}andλis an associ-\n",
      "ated hyperparameter speciﬁc to the learning procedure. Since we consider least-squares\n",
      "regression, the new predictor, ˆfbag\n",
      "λ, is simply the average of all ˆf(j)\n",
      "λ,j= 1,...,m .\n",
      "If we denote bias(j)(x) =E[ˆf(j)\n",
      "λ(x)]−f∗(x), and var(j)(x) = var[ˆf(j)\n",
      "λ(x)]\n",
      "(assuming\n",
      "thatxis ﬁxed and only taking expectations with respect to the data), the n they are the\n",
      "same for all j∈{1,...,m}and the bias of ˆfbag\n",
      "λis the same as the base bias for a single\n",
      "dataset (and thus so is the squared bias). At the same time, the va riance is divided by m\n",
      "because the datasets are assumed to be independent.\n",
      "Thus, in the bias/variance trade-oﬀ, the selected hyperparamet er will typically select\n",
      "a higher variance (or equivalently lower bias) estimator than for m= 1. In the context of\n",
      "independent datasets, it is relevant to concatenate all mdatasets into one large dataset\n",
      "withN=nmobservations and learn a single predictor with these: the generaliza tion\n",
      "performance of the average of mpredictors will often be the same as the one of the single\n",
      "predictor on the large dataset, but with potential computational beneﬁts. We now give a\n",
      "few examples for regression (we consider binary classiﬁcation in exe rcises 10.1 and10.3).\n",
      "Thek-nearest neighbor regression. We consider the analysis from section 6.3.2 on\n",
      "prediction problems over X⊂Rd, where we showed in proposition 6.2that the (squared)\n",
      "bias was upper-bounded by 8 B2diam(X)2(2k\n",
      "n)2/d(ford⩾2). At the same time, the\n",
      "variance was bounded byσ2\n",
      "k, whereσ2is a bound on the noise variance on top of the\n",
      "target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\n",
      "replications, we get an excess risk upper-bounded by\n",
      "σ2\n",
      "km+ 8B2diam(X)2(2k\n",
      "n)2/d\n",
      ".\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data (bootstrapped samples).\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset.\n",
      "* **Computational benefits:** Concatenating all datasets and training a single model on the combined data can offer similar performance to bagging with potential computational advantages.\n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk (difference between model performance and the best possible performance) by controlling both bias and variance. \n",
      "\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction.\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "target function f∗, whileBis the Lipschitz constant of the target function. Thus, with m\n",
      "replications, we get an excess risk upper-bounded by\n",
      "σ2\n",
      "km+ 8B2diam(X)2(2k\n",
      "n)2/d\n",
      ".\n",
      "When optimizing this bound with respect to k, we get that k1+2/d∝n2/d\n",
      "m, leading to\n",
      "k∝1\n",
      "md/(2+d)n2/(2+d). Compared to section 6.3.2, we obtain a smaller number of neighbors\n",
      "(which is consistent with favoring higher variance estimators). The overall excess risk ends\n",
      "up being proportional to 1 /(mn)2/(d+2), which is exactly the rate for a dataset of N=mn\n",
      "observations.\n",
      "Thus, dividing a dataset of Nobservations in mchunks ofn=N/m observations,\n",
      "estimating independently, and combining linearly does not lead to an ov erall improved\n",
      "statistical behavior compared to learning all at once. Still, it can hav e signiﬁcant com-\n",
      "putational advantages when the mestimators can be computed in parallel (and totally\n",
      "independently). We thus obtain a distributed algorithm with the same worst-case pre-\n",
      "dictive performance as for a single machine.\n",
      "Note here that there is an upper bound on the number of replication s (and thus the\n",
      "ability for parallelization) to get the same (optimal) rate, as we need kto be larger than 1,\n",
      "and thus,mcannot grow larger than n2/d.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's the refined summary, incorporating the new context:\n",
      "\n",
      "This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data (bootstrapped samples).\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset.\n",
      "* **Computational benefits:** While bagging can achieve similar performance to training a single model on the entire dataset,  dividing the data into chunks and training models independently doesn't necessarily lead to better statistical behavior. However, it can offer significant computational advantages, especially when models can be trained in parallel.\n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk (difference between model performance and the best possible performance) by controlling both bias and variance.\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It can be particularly beneficial in scenarios where computational resources allow for parallel processing. \n",
      "\n",
      "\n",
      "\n",
      "**Explanation of Changes:**\n",
      "\n",
      "The new context highlights that while bagging can be effective, training a single model on the entire dataset might offer comparable performance. The summary now reflects this, emphasizing the computational benefits of bagging over purely statistical gains. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "286 CHAPTER 10. ENSEMBLE LEARNING\n",
      "Exercise 10.1 ( ♦)Considerk-nearest neighbor multicategory classiﬁcation with a ma-\n",
      "jority vote rule. Using the relationship between the quadra tic loss and the 0–1 loss from\n",
      "section4.1.4to derive an upper bound on the expected risk, what is the corr esponding\n",
      "optimal choice of mwhen using independent datasets?\n",
      "Ridge regression. Following the analysis from section 7.6.6, the variance of the ridge\n",
      "regression estimator was proportional toσ2\n",
      "nλ−1/αand the bias proportional to λt/s(see\n",
      "precise deﬁnitions in section 7.6.6). Withmreplications, we thus get an excess risk\n",
      "proportional toσ2\n",
      "nmλ−1/α+λt/s, and the averaged estimator behaves like having N=nm\n",
      "observations (and the same regularization parameter). Again, wit h the proper choice\n",
      "of regularization parameter (lower λthan for the full dataset), there is no statistical\n",
      "advantage. Still, there may be a computational one, not only for pa rallel processing but\n",
      "also with a single machine (see exercise 10.2), since, as shown in section 7.4, the training\n",
      "time for ridge regression is superlinear in the number of observation s with running-time\n",
      "complexities between O(n2) andO(n3) if no low-rank approximations are used.\n",
      "Exercise 10.2 Assuming that obtaining an estimator for ridge regression h as running-\n",
      "time complexity O(nβ)forβ⩾1fornobservations, what is the complexity of using a\n",
      "split of the data into mchunks? What is the optimal value of m?\n",
      "Exercise 10.3 ( ♦)In the setup of this section with mindependent datasets, consider\n",
      "an estimator ˆf(j):X→{− 1,1}learned on the jth dataset for a binary classiﬁcation\n",
      "problem, for j∈{1,...,m}, with ˆfbag(x) = sign(∑m\n",
      "j=1ˆf(j)(x))\n",
      "the majority vote clas-\n",
      "siﬁer. Denoting f∗(x)∈ {− 1,1}the optimal prediction at x∈X, as deﬁned in sec-\n",
      "tion2.2.3, andε(x) =E[ˆf(j)(x)f∗(x)](which is the same for all j), show that we have\n",
      "E[1ˆfbag(x)̸=f∗(x)]⩽exp(\n",
      "−m\n",
      "2(ε(x))2\n",
      "+)\n",
      ".If∀x∈X,ε(x)⩾η >0, show that the expected\n",
      "excess risk is less than exp(\n",
      "−m\n",
      "2η2)\n",
      ".\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data (bootstrapped samples).\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset. While it can achieve similar performance, training a single model on the full dataset might offer comparable results.\n",
      "* **Computational benefits:** Bagging offers significant computational advantages, especially when models can be trained in parallel. It can be beneficial even if there's no clear statistical gain, as parallel processing can speed up training significantly. \n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk (difference between model performance and the best possible performance) by controlling both bias and variance.\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It can be particularly beneficial in scenarios where computational resources allow for parallel processing. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "E[1ˆfbag(x)̸=f∗(x)]⩽exp(\n",
      "−m\n",
      "2(ε(x))2\n",
      "+)\n",
      ".If∀x∈X,ε(x)⩾η >0, show that the expected\n",
      "excess risk is less than exp(\n",
      "−m\n",
      "2η2)\n",
      ".\n",
      "Beyond independent datasets. Having independent datasets may not be possible,\n",
      "and one typically needs to artiﬁcially create such replicated dataset s from a single one,\n",
      "which is precisely what bagging methods will do in section 10.1.2 , with a reduced variance\n",
      "still, but this time with a potentially higher bias.\n",
      "10.1.2 Bagging\n",
      "We consider datasets D(b), obtained with random weights v(b)\n",
      "i∈R+,i= 1,...,n . For\n",
      "the bootstrap,1we consider nsamples from the original ndata points with replacement,\n",
      "which correspond to integer weights v(b)\n",
      "i∈N,i= 1,...,n , that sum to n. Such sets of\n",
      "weights are sampled independently mtimes. We study m=∞for simplicity; that is,\n",
      "inﬁnitely many replications (in practice, the inﬁnite mbehavior can be achieved with\n",
      "moderatem’s). Inﬁnitely many bootstrap replications lead to a form of stabilizat ion,\n",
      "1Seehttps://en.wikipedia.org/wiki/Bootstrapping_(statis tics)andEfron and Tibshirani\n",
      "(1994) for an introduction to bootstrapping methods in statistic s.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary incorporating the new context:\n",
      "\n",
      "This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data created through bootstrapping. Bootstrapping involves sampling data points with replacement, essentially creating multiple \"replicas\" of the original dataset.\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset. While it can achieve similar performance, training a single model on the full dataset might offer comparable results.\n",
      "\n",
      "* **Theoretical Guarantee:** The passage provides a theoretical basis for bagging's effectiveness, showing that the expected excess risk (the difference between model performance and the best possible performance)  can be reduced using bagging.  This reduction in risk is particularly pronounced when the error of individual models is consistent across different data points.\n",
      "\n",
      "* **Computational benefits:** Bagging offers significant computational advantages, especially when models can be trained in parallel. It can be beneficial even if there's no clear statistical gain, as parallel processing can speed up training significantly. \n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk by controlling both bias and variance.\n",
      "\n",
      "\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It's particularly beneficial when computational resources allow for parallel processing, and it has theoretical backing that demonstrates its ability to reduce model risk. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific point!\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.1. AVERAGING/BAGGING 287\n",
      "which is important for highly variable predictors (which usually imply a lar ge estimation\n",
      "variance).\n",
      "For linear estimators (in the deﬁnition of section 6.2.1; see also section 7.6.1) with the\n",
      "square loss, such as kernel ridge regression or local averaging, t his leads to another linear\n",
      "estimator. Therefore, this provides alternative ways of regulariz ing, which typically may\n",
      "not provide a strong statistical gain over existing methods but pro vide a computational\n",
      "gain, in particular when each estimator is very eﬃcient to compute (s ee related examples\n",
      "in section 10.2). Overall, as will be shown for 1-nearest-neighbor, bagging will red uce\n",
      "variance while increasing the bias, thus leading to trade-oﬀs that ar e common in regu-\n",
      "larizing methods. See also the end of section 10.2 for a short description of “random\n",
      "forests,” which is also partially based on bagging.\n",
      "For simplicity, we will consider averaging estimators obtained by rand omly selecting s\n",
      "observations from the navailable ones, doing this many times (inﬁnitely many for the\n",
      "analysis), and averaging the predictions.\n",
      "Exercise 10.4 Show that when sampling nelements with replacement from nitems, the\n",
      "expected fraction of distinct items is 1−(1−1/n)nand it tends to 1−1/ewhenntends\n",
      "to inﬁnity.\n",
      "One-nearest neighbor regression. We focus on the 1-nearest neighbor estimator\n",
      "where the strong eﬀect of bagging is striking. The analysis in this sub section follows\n",
      "from Biau et al. (2010). The key observation is that if we denote as ( x(i)(x),y(i)(x))\n",
      "the pair of observations that is the ith-nearest neighbor of xfrom the dataset x1,...,xn\n",
      "(ignoring ties), then we can write the bagged estimate as\n",
      "ˆf(x) =n∑\n",
      "i=1Viy(i)(x),\n",
      "where the nonnegative weights Visum to 1 and do not depend on x. The weight Viis the\n",
      "probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\n",
      "subsample of size s. We consider sampling without replacement and leave sampling with\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary incorporating the new context:\n",
      "\n",
      "This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data created through bootstrapping. Bootstrapping involves sampling data points with replacement, essentially creating multiple \"replicas\" of the original dataset.\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset. While it can achieve similar performance, training a single model on the full dataset might offer comparable results.\n",
      "\n",
      "* **Theoretical Guarantee:** The passage provides a theoretical basis for bagging's effectiveness, showing that the expected excess risk (the difference between model performance and the best possible performance)  can be reduced using bagging.  This reduction in risk is particularly pronounced when the error of individual models is consistent across different data points. This is highlighted with the example of 1-nearest neighbor regression and emphasizes the effectiveness of bagging in controlling both bias and variance.\n",
      "\n",
      "* **Computational benefits:** Bagging offers significant computational advantages, especially when models can be trained in parallel. It can be beneficial even if there's no clear statistical gain, as parallel processing can speed up training significantly. \n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk by controlling both bias and variance. It further delves into the specific case of 1-nearest neighbor regression, showcasing the strong impact of bagging in reducing variance.\n",
      "\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It's particularly beneficial when computational resources allow for parallel processing, and it has theoretical backing that demonstrates its ability to reduce model risk. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific point!\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "probability that the ith-nearest neighbor of xis the 1-nearest-neighbor of xin a uniform\n",
      "subsample of size s. We consider sampling without replacement and leave sampling with\n",
      "replacement as an exercise (see Biau et al. ,2010, for more details). We assume that s⩾2.\n",
      "To select the ith-nearest neighbor as the 1-nearest-neighbor in a subsample, w e need\n",
      "that theith-nearest neighbor is selected but none of the closer neighbors, which leaves\n",
      "s−1 elements to choose among n−ipossibilities. This shows, that if i>n−s+ 1, then\n",
      "Vi= 0, while otherwise Vi=(n\n",
      "s)−1(n−i\n",
      "s−1)\n",
      ", as the total number of subsets of size sis(n\n",
      "s)\n",
      ",\n",
      "and there are(n−i\n",
      "s−1)\n",
      "relevant ones.\n",
      "We can now use the reasoning from section 6.3.2. Since for any x, the weights given\n",
      "to each observation (once they are ordered in terms of distance t ox) areV1,...,Vn, the\n",
      "variance term is equal to∑n\n",
      "i=1V2\n",
      "i. To obtain a bound, we note that for i⩽n−s+ 1,\n",
      "Vi=s\n",
      "n−s+ 1∏s−2\n",
      "j=0(n−i−j)\n",
      "∏s−2\n",
      "j=0(n−j)=s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n−j)\n",
      "⩽s\n",
      "n−s+ 1s−2∏\n",
      "j=0(\n",
      "1−i\n",
      "n)\n",
      ",\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's the original summary as it stands:\n",
      "\n",
      "\n",
      "This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data created through bootstrapping. Bootstrapping involves sampling data points with replacement, essentially creating multiple \"replicas\" of the original dataset.\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Trade-off:** Bagging generally leads to higher variance (lower bias) estimators compared to using a single model on the entire dataset. While it can achieve similar performance, training a single model on the full dataset might offer comparable results.\n",
      "\n",
      "* **Theoretical Guarantee:** The passage provides a theoretical basis for bagging's effectiveness, showing that the expected excess risk (the difference between model performance and the best possible performance)  can be reduced using bagging.  This reduction in risk is particularly pronounced when the error of individual models is consistent across different data points. This is highlighted with the example of 1-nearest neighbor regression and emphasizes the effectiveness of bagging in controlling both bias and variance.\n",
      "\n",
      "* **Computational benefits:** Bagging offers significant computational advantages, especially when models can be trained in parallel. It can be beneficial even if there's no clear statistical gain, as parallel processing can speed up training significantly. \n",
      "* **Example:** The passage uses k-nearest neighbors regression as an example to illustrate how bagging reduces the excess risk by controlling both bias and variance. It further delves into the specific case of 1-nearest neighbor regression, showcasing the strong impact of bagging in reducing variance.\n",
      "\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It's particularly beneficial when computational resources allow for parallel processing, and it has theoretical backing that demonstrates its ability to reduce model risk. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific point! \n",
      "\n",
      "\n",
      "\n",
      "The new context provided delves into the mathematical details of why bagging works, particularly in the case of 1-nearest neighbor regression.  However, it doesn't fundamentally change the overall summary points about what bagging is, why it's used, and its benefits. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "288 CHAPTER 10. ENSEMBLE LEARNING\n",
      "leading to, upper-bounding the sum by an integral,\n",
      "n∑\n",
      "i=1V2\n",
      "i⩽s2\n",
      "(n−s+ 1)2n∑\n",
      "i=1(\n",
      "1−i\n",
      "n)2(s−1)⩽ns2\n",
      "(n−s+ 1)2∫1\n",
      "0(1−t)2(s−1)dt\n",
      "⩽ns2\n",
      "(n−s+ 1)21\n",
      "2s−1⩽ns\n",
      "(n−s+ 1)2=s\n",
      "n1\n",
      "(1 + 1/n−s/n)2.\n",
      "For the bias term, we need to bound∑n\n",
      "i=1Vi·E[\n",
      "∥x−x(i)(x)∥2]\n",
      ", where the expectation\n",
      "is with respect to the data and the test point x. We note here that by deﬁnition of Vi, and\n",
      "conditioning on the data and x, this isB2multiplied by the expectation of the distance\n",
      "to the ﬁrst nearest neighbor from a random sample of size s, and thus, for the ℓ∞-norm\n",
      "on a subset XofRd, from lemma 6.1, less than 4 B2diam(X)21\n",
      "s2/difd⩾2 (which we now\n",
      "assume).\n",
      "Thus, the overall excess risk is less than\n",
      "4B2diam(X)21\n",
      "s2/d+s\n",
      "n1\n",
      "(1 + 1/n−s/n)2,\n",
      "which we can balance by choosing s1+2/d∝n, leading to the same performance as the\n",
      "k-nearest neighbor for a well-chosen k, but now with a bagged esimate.\n",
      "In ﬁgure 10.1, simulations in one dimension are plotted, showing the regularizing\n",
      "eﬀects of bagging; we see that when s=n(no subsampling), we recover the 1-nearest\n",
      "neighbor estimate, and when sdecreases, the variance indeed decreases while the bias\n",
      "increases.\n",
      "10.2 Random Projections and Averaging\n",
      "In section 10.1, we reweighted observations to be able to rerun the original algorit hm. This\n",
      "can also be done through random projections of all observations. Such random projections\n",
      "can be performed in several ways: (1) for data in Rdby selecting sof thedvariables;\n",
      "(2) still for data in Rd, by projecting the data in a more general s-dimensional subspace;\n",
      "and (3) for kernel methods, using random features such as pres ented in section 7.4. Such\n",
      "random projections can also reduce the number of samples while kee ping the dimension\n",
      "ﬁxed (this will depend if the design matrix is left- or right-multiplied by a matrix of\n",
      "reduced size).\n",
      "In this section, we consider random projections for ordinary least -squares (OLS), with\n",
      "the same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary incorporating the new mathematical context:\n",
      "\n",
      "This passage describes bagging (Bootstrap Aggregating) as a technique to improve the performance of machine learning models, particularly in regression.  \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bagging creates multiple models:** It trains several independent models (e.g., k-nearest neighbors) on different subsets of the training data created through bootstrapping. Bootstrapping involves sampling data points with replacement, essentially creating multiple \"replicas\" of the original dataset.\n",
      "* **Averaging improves generalization:** The final prediction is the average of all individual model predictions. This averaging reduces variance without significantly increasing bias.\n",
      "* **Theoretical Guarantee:** The passage provides a theoretical basis for bagging's effectiveness, demonstrating how it reduces excess risk (the difference between model performance and the best possible performance). This reduction is particularly pronounced when individual models have consistent error across data points. The example of 1-nearest neighbor regression highlights this, showing bagging's ability to control both bias and variance.\n",
      "* **Mathematical Insight:** The new context delves into the mathematical details of why bagging works for 1-nearest neighbor regression. It shows that by carefully choosing the subset size (s), bagging can achieve performance comparable to a well-chosen k-nearest neighbor, while simultaneously reducing variance. This is achieved through balancing bias and variance terms in the excess risk calculation.\n",
      "* **Computational benefits:** Bagging offers significant computational advantages, especially when models can be trained in parallel. It can be beneficial even if there's no clear statistical gain, as parallel processing can speed up training significantly. \n",
      "* **Example:** The passage uses k-nearest neighbors regression to illustrate how bagging reduces the excess risk by controlling both bias and variance, with a specific focus on 1-nearest neighbor regression and its improved performance through bagging.\n",
      "\n",
      "\n",
      "\n",
      "Essentially, bagging leverages the wisdom of the crowd by combining multiple models to produce a more robust and accurate prediction. It has theoretical backing that demonstrates its ability to reduce model risk, and practical benefits through computational efficiency, especially when utilizing parallel processing. \n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "reduced size).\n",
      "In this section, we consider random projections for ordinary least -squares (OLS), with\n",
      "the same notation as in chapter 3, withy∈Rnthe response vector and Φ ∈Rn×dthe\n",
      "design matrix, in two settings:\n",
      "•Sketching : Replacing min θ∈Rd∥y−Φθ∥2\n",
      "2by minθ∈Rd∥Sy−SΦθ∥2\n",
      "2, whereS∈Rs×n\n",
      "is an i.i.d. Gaussian matrix (with independent zero mean and unit varianc e ele-\n",
      "ments). This is an idealization of subsampling done in the previous sect ion. Here,\n",
      "we typically have n > s > d (more observations than the feature dimension), and\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The original summary is a good overview of bagging. The new context focuses on random projections and their application to ordinary least squares regression, which is a different topic.  \n",
      "\n",
      "Therefore, the new context doesn't directly improve the original summary of bagging.  \n",
      "\n",
      "**Here's why:**\n",
      "\n",
      "* **Different Techniques:** Bagging is an ensemble method that combines multiple models, while random projections are a dimensionality reduction technique. They address different aspects of machine learning.\n",
      "* **Focus on OLS:** The new context specifically discusses random projections within the context of ordinary least squares regression, which is a particular type of model.  \n",
      "\n",
      "**To summarize:** The new context provides valuable information about random projections but doesn't directly enhance the understanding of bagging.  They are distinct techniques with separate applications. \n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.2. RANDOM PROJECTIONS AND AVERAGING 289\n",
      "Figure 10.1. Subsampling estimates with m= 20 subsampled datasets, for varying sub-\n",
      "sampling ratios n/s, with an estimation of the testing error. When n/s= 1, we recover\n",
      "the 1-nearest neighbor classiﬁer (which overﬁts), and when n/sgrows, we get better ﬁts\n",
      "until underﬁtting kicks in. Optimal testing error is obtained for n/s= 8.\n",
      "one of the beneﬁts of sketching is to be able to store a reduced rep resentation of\n",
      "the data ( Rs×dinstead of Rn×d).\n",
      "•Random projection : Replacing min θ∈Rd∥y−Φθ∥2\n",
      "2by minη∈Rs∥y−ΦSη∥2\n",
      "2, where\n",
      "S∈Rd×sis a more general sketching matrix. Here, we typically have d > n >\n",
      "s(high-dimensional situation). The beneﬁts of random projection a re twofold:\n",
      "reduction in computation time and regularization. This corresponds to replacing\n",
      "the corresponding feature vectors ϕ(x)∈RdbyS⊤ϕ(x)∈Rs. We will consider\n",
      "Gaussian matrices, but also subsampling matrices, and draw connec tions with kernel\n",
      "methods.\n",
      "In the following sections, we study these precisely for the OLS fram ework (it could\n",
      "also be done for ridge regression). We ﬁrst brieﬂy mention a common ly used and related\n",
      "approach.\n",
      "Random forests. A popular algorithm called “random forests” ( Breiman ,2001) mixes\n",
      "both dimension reduction by projection and bagging: decision trees are learned on a\n",
      "bootstrapped sample of the data, while selecting a random subset o f features at every\n",
      "splitting decision. This algorithm has nice properties (invariance to re scaling of the\n",
      "variables and robustness in high dimension due to the random featur e selection) and can\n",
      "be extended in many ways. See Biau and Scornet (2016) for details.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can assess if it needs refining based on the new context. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "290 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.2.1 Gaussian Sketching\n",
      "Following section 3.3on OLS, we consider a design matrix Φ ∈Rn×dwith rankd(i.e.,\n",
      "Φ⊤Φ∈Rd×dinvertible), which implies n⩾d. We consider s > d Gaussian random\n",
      "projections, with typically s⩽n, but this is not necessary in the analysis that follows.\n",
      "The estimator ˆθ(j)is obtained by using S(j)∈Rs×n, withj= 1,...,m , wherem\n",
      "denotes the number of replications. We then consider ˆθ=1\n",
      "m∑m\n",
      "j=1ˆθ(j). Whenm= 1,\n",
      "this is a single sketch.\n",
      "We will consider the same ﬁxed design assumptions as in section 3.5; that is,y=\n",
      "Φθ∗+ε, whereε∈Rnhas independent zero-mean components with variance σ2, and\n",
      "θ∗∈Rd. Our goal is to compute the ﬁxed design error1\n",
      "nEε,S∥Φˆθ−Φθ∗∥2\n",
      "2, where we take\n",
      "expectations with respect to both the learning problem (in the ﬁxed design setting, the\n",
      "noise vector ε) and the added randomization (the sketching matrices S(j),j= 1,...,m ).\n",
      "To compute this error, we ﬁrst need to compute expectations and variances with\n",
      "respect to the random projections, assuming that εis ﬁxed.\n",
      "We ﬁrst introduce a representation tool that will allow simple expres sions of all predic-\n",
      "tion vectors S(j)Φ. Since the Gaussian matrices S(j)are invariant by left and right multi-\n",
      "plication by an orthogonal matrix, we can assume that the singular v alue decomposition\n",
      "(SVD) of Φ = UDV⊤, whereV∈Rd×dis orthogonal (i.e., V⊤V=VV⊤=I),D∈Rd×d\n",
      "is an invertible diagonal matrix, and U∈Rn×dhas orthonormal columns (i.e., U⊤U=I;\n",
      "remember than n⩾d), is such that U=(I\n",
      "0)\n",
      ", and we can write S(j)=(\n",
      "S(j)\n",
      "1S(j)\n",
      "2)\n",
      ", with\n",
      "S(j)\n",
      "1∈Rs×dandS(j)\n",
      "2∈Rs×(n−d). We can also split yasy=(y1\n",
      "y2)\n",
      "fory1∈Rdand\n",
      "y2∈Rn−d.\n",
      "We can write the normal equation that deﬁnes ˆθ(j)∈Rd, for eachj∈{1,...,m}(i.e.,\n",
      "(Φ⊤(S(j))⊤S(j)Φ)ˆθ(j)= Φ⊤(S(j))⊤S(j)y), leading to the following closed-form estimators\n",
      "ˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\n",
      "regarding the SVD of Φ, we have S(j)Φ =S(j)\n",
      "1DV⊤. We can then expand the prediction\n",
      "vector in Rnas\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can assess if it needs refining based on the new context. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "ˆθ(j)= (Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y.2Using the assumptions given previously\n",
      "regarding the SVD of Φ, we have S(j)Φ =S(j)\n",
      "1DV⊤. We can then expand the prediction\n",
      "vector in Rnas\n",
      "Φˆθ(j)= Φ(Φ⊤(S(j))⊤S(j)Φ)−1Φ⊤(S(j))⊤S(j)y\n",
      "=(I\n",
      "0)\n",
      "DV⊤(VD(S(j)\n",
      "1)⊤S(j)\n",
      "1DV⊤)−1VD(S(j)\n",
      "1)⊤S(j)y\n",
      "=(I\n",
      "0)\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)y=(I\n",
      "0)\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤(S(j)\n",
      "1y1+S(j)\n",
      "2y2)\n",
      "=(y1+ ((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)\n",
      "2y2\n",
      "0)\n",
      ".\n",
      "Thus, since E[S(j)\n",
      "2] = 0 andS(j)\n",
      "2is independent of S(j)\n",
      "1, we getES(j)[\n",
      "Φˆθ(j)]\n",
      "=(y1\n",
      "0)\n",
      ", which\n",
      "happens to be exactly the OLS estimator Φ ˆθOLS= Φ(Φ⊤Φ)−1Φ⊤y=(I0\n",
      "0 0)\n",
      "y. Moreover,\n",
      "2Ifs⩾d, thenS(j)Φ has rank dalmost surely, and thus ˆθ(j)is uniquely deﬁned.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide me with the new context so I can refine the summary. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.2. RANDOM PROJECTIONS AND AVERAGING 291\n",
      "we have the model y= Φθ∗+εand, if we split εasε=(ε1\n",
      "ε2)\n",
      ", we havey=(I\n",
      "0)\n",
      "DV⊤θ∗+ε,\n",
      "and thusy2=ε2. We therefore get\n",
      "ES(j)[Φˆθ(j)−ES(j)Φˆθ(j)2\n",
      "2]\n",
      "=ES(j)[\n",
      "∥((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1(S(j)\n",
      "1)⊤S(j)\n",
      "2ε2∥2\n",
      "2]\n",
      ".\n",
      "Taking the expectation with respect to ε(through E[ε2ε⊤\n",
      "2] =σ2I), using the trace trick,\n",
      "and using expectations for the Wishart and inverse Wishart distribu tions,3this leads to\n",
      "Eε,S(j)[Φˆθ(j)−ES(j)Φˆθ(j)2\n",
      "2]\n",
      "=σ2ES(j)[\n",
      "tr(\n",
      "(S(j)\n",
      "2)⊤S(j)\n",
      "1((S(j)\n",
      "1)⊤S(j)\n",
      "1)−2(S(j)\n",
      "1)⊤S(j)\n",
      "2)]\n",
      "=σ2ES(j)[\n",
      "tr(\n",
      "S(j)\n",
      "2(S(j)\n",
      "2)⊤S(j)\n",
      "1((S(j)\n",
      "1)⊤S(j)\n",
      "1)−2(S(j)\n",
      "1)⊤)]\n",
      "= (n−d)σ2ES(j)\n",
      "1[\n",
      "tr(\n",
      "((S(j)\n",
      "1)⊤S(j)\n",
      "1)−1)]\n",
      "=d\n",
      "s−d−1(n−d)σ2.\n",
      "We can now compute the overall expected generalization error:\n",
      "1\n",
      "nEε,S(j)[1\n",
      "mm∑\n",
      "j=1Φˆθ(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "Φˆθ(1)]\n",
      "−Φθ∗2\n",
      "2]\n",
      "+1\n",
      "nmEε,S(1)[Φˆθ(1)−ES(1)Φˆθ(1)2]\n",
      "=1\n",
      "nEε[ΦˆθOLS−Φθ∗2\n",
      "2]\n",
      "+σ2d\n",
      "nmn−d\n",
      "s−d−1\n",
      "=σ2d\n",
      "n+σ2d\n",
      "nmn−d\n",
      "s−d−1.\n",
      "Thus, when morstends to inﬁnity, we recover the traditional OLS behavior, while for m\n",
      "andsﬁnite, the performance degrades gracefully. Moreover, when s=n, even form= 1,\n",
      "we get essentially twice the performance of the OLS estimator. We n ote that to get the\n",
      "same performance as OLS (up to a factor of 2), we need m=n−d\n",
      "s−d−1∼n\n",
      "sreplications.\n",
      "As in section 10.1, there is no statistical gain (here, compared to OLS), but only\n",
      "potentially a computational one (because some computations may b e done in parallel\n",
      "and of reduced storage). See, for example, Dobriban and Liu (2019) for other criteria\n",
      "and sketching matrices.\n",
      "Beyond Gaussian sketching. In this section, we have chosen a Gaussian sketching\n",
      "matrixS. This made the analysis simple because of the properties of the Gaus sian\n",
      "distribution (invariance by rotation and availability of exact expecta tions for inverse\n",
      "Wishart distributions). The analysis can be extended with more comp lex tools to other\n",
      "random sketching matrices with more attractive computational pr operties, such as with\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide me with the original summary so I can refine it with the new context. \n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Wishart distributions). The analysis can be extended with more comp lex tools to other\n",
      "random sketching matrices with more attractive computational pr operties, such as with\n",
      "many zeros, leading to subsampling observations or dimensions. See Wang et al. (2018),\n",
      "Dobriban and Liu (2019), and the references therein. For the random projections that\n",
      "follow, our analysis will apply to more general sketches.\n",
      "3IfS∈Ra×bhas independent standard Gaussian components, then E[(S⊤S)−1] =1\n",
      "a−b−1Iifa > b+1,\n",
      "andE[SS⊤] =bI; seehttps://en.wikipedia.org/wiki/Inverse-Wishart_distr ibution.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can refine it with the new context.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "292 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.2.2 Random Projections\n",
      "We also consider the ﬁxed design setup, with a design matrix Φ ∈Rn×dand a response\n",
      "vector of the form y= Φθ∗+ε. We now assume that d>n (high-dimensional setup) and\n",
      "the rank of Φ is n. In this high-dimensional setup, we need some form of regularizatio n,\n",
      "which will come here from random projections.\n",
      "For eachj∈{1,...,n}, we consider a sketching matrix S(j)∈Rd×s, fors⩽nsampled\n",
      "independently from a distribution to be determined (we only assume t hat almost surely,\n",
      "its rank is equal to s). We then consider ˆ η(j)as a minimizer of min η∈Rs∥y−ΦS(j)η∥2\n",
      "2.\n",
      "For simplicity, we assume that matrix Φ S(j)has ranks, which is the case almost surely\n",
      "for Gaussian projections; this implies that ˆ η(j)is unique, but our result applies in all\n",
      "situations, as we are only interested in the denoised response vect or. We now consider\n",
      "the average ˆθ=1\n",
      "m∑m\n",
      "j=1S(j)ˆη(j).\n",
      "We thus consider the estimator ˆ η(j)=(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rs, ob-\n",
      "tained from the normal equation ( S(j))⊤Φ⊤ΦS(j)ˆη(j)= (S(j))⊤Φ⊤ywith the denoised\n",
      "response vector\n",
      "ˆy(j)= ΦS(j)ˆη(j)= ΦS(j)(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤y∈Rn.\n",
      "Denoting Π(j)= ΦS(j)(\n",
      "(S(j))⊤Φ⊤ΦS(j))−1(S(j))⊤Φ⊤, it takes the form ˆ y(j)= Π(j)y.\n",
      "Matrix Π(j)is almost surely an orthogonal projection matrix into an s-dimensional vector\n",
      "space, and its expectation is denoted as ∆ ∈Rn×n, which satisﬁes tr(∆) = s. We have,\n",
      "moreover, 0 ≼∆≼I; that is, all eigenvalues of ∆ are between 0 and 1.\n",
      "We can then compute expectations and variances as follows:\n",
      "ES(j)[\n",
      "ˆy(j)]\n",
      "=ES(j)[\n",
      "Π(j)y]\n",
      "= ∆y= ∆[\n",
      "Φθ∗+ε] = ∆ε+ ∆Φθ∗\n",
      "ES(j)[\n",
      "ˆy(j)]\n",
      "−Φθ∗= ∆ε+ [∆−I]Φθ∗ (10.1)\n",
      "ES(j)[ˆy(j)−ES(j)[\n",
      "ˆy(j)]2\n",
      "2]\n",
      "=ES(j)[(Π(j)−∆)y2\n",
      "2]\n",
      "=y⊤ES(j)[(\n",
      "Π(j)−∆)2]\n",
      "y\n",
      "=y⊤ES(j)[\n",
      "Π(j)−∆Π(j)−Π(j)∆+∆2]\n",
      "ysince Π(j)Π(j)=Π(j),\n",
      "=y⊤(\n",
      "∆−∆2)\n",
      "y,sinceE[Π(j)= ∆. (10.2)\n",
      "Thus, the overall (ﬁxed design) expected generalization error is e qual to, using that\n",
      "S(1),...,S(m)are i.i.d. matrices,\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "ˆy(1)]\n",
      "−Φθ∗2\n",
      "2+1\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the new context so I can refine the summary.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Thus, the overall (ﬁxed design) expected generalization error is e qual to, using that\n",
      "S(1),...,S(m)are i.i.d. matrices,\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=1\n",
      "nEε[ES(1)[\n",
      "ˆy(1)]\n",
      "−Φθ∗2\n",
      "2+1\n",
      "mES(1)[ˆy(1)−ES(1)[\n",
      "ˆy(1)]2\n",
      "2]]\n",
      "by taking expectations with respect to all S(j),\n",
      "=1\n",
      "nEε[∆ε+ [∆−I]Φθ∗2\n",
      "2+1\n",
      "my⊤(\n",
      "∆−∆2)\n",
      "y]\n",
      "using equations ( 10.1) and ( 10.2).\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the new context so I can refine the summary.  I need the additional information to understand how it relates to the existing summary and what changes might be necessary. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.2. RANDOM PROJECTIONS AND AVERAGING 293\n",
      "Using the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I, we get\n",
      "1\n",
      "nEε,S[1\n",
      "m∑m\n",
      "j=1ˆy(j)−Φθ∗2\n",
      "2]\n",
      "=σ2\n",
      "ntr(∆2) +1\n",
      "nθ⊤\n",
      "∗Φ⊤[I−∆]2Φθ∗+1\n",
      "nm[\n",
      "σ2(tr(∆)−tr(∆2)) +θ⊤\n",
      "∗Φ⊤(\n",
      "∆−∆2)\n",
      "Φθ∗]\n",
      "using the model y= Φθ∗+εand the fact that E[ε] = 0 and E[εε⊤] =σ2I,\n",
      "=σ2\n",
      "n(\n",
      "1−1\n",
      "m)\n",
      "tr(∆2) +σ2s\n",
      "nm+1\n",
      "nθ⊤\n",
      "∗Φ⊤[∆−I]2Φθ∗+1\n",
      "nmθ⊤\n",
      "∗Φ⊤(\n",
      "∆−∆2)\n",
      "Φθ∗\n",
      "=σ2\n",
      "n(\n",
      "1−1\n",
      "m)\n",
      "tr(∆2) +σ2s\n",
      "nm+1\n",
      "nθ⊤\n",
      "∗Φ⊤[\n",
      "I−∆ +(1\n",
      "m−1)\n",
      "(∆−∆2)]\n",
      "Φθ∗\n",
      "⩽σ2s\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗,since ∆2≼∆, (10.3)\n",
      "which is the value for m= 1 (single replication). Note that the expectation (before taking\n",
      "the bound) decreases in m, with a limitσ2tr(∆2)\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤[I−∆]2Φθ∗whenm→+∞\n",
      "(with improved bias and variance terms). We now follow Kab´ an (2014) and Thanei et al.\n",
      "(2017) to bound the matrix I−∆.\n",
      "Since ∆ is the expectation of a projection matrix, we already know th at 0≼∆≼I.\n",
      "We omit the superscript 0(j)for clarity, and we consider Π = Φ S(\n",
      "S⊤Φ⊤ΦS)−1S⊤Φ. For\n",
      "any vector z∈Rn, we consider\n",
      "z⊤(I−∆)z=ES[\n",
      "z⊤(I−Π)z]\n",
      "=ES[\n",
      "z⊤z−z⊤ΦS(\n",
      "S⊤Φ⊤ΦS)−1S⊤Φ⊤z]\n",
      "=ES[\n",
      "min\n",
      "u∈Rs∥z−ΦSu∥2\n",
      "2]\n",
      "by deﬁnition of projections ,\n",
      "⩽ES[\n",
      "min\n",
      "v∈Rd∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "by minimizing over a smaller subspace,\n",
      "⩽min\n",
      "v∈RdES[\n",
      "∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "by properties of the expectation.\n",
      "We can expand this to get\n",
      "ES[\n",
      "∥z−ΦSS⊤v∥2\n",
      "2]\n",
      "=∥z∥2\n",
      "2−2z⊤ΦES[\n",
      "SS⊤]\n",
      "v+v⊤ES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      "v,\n",
      "leading to, after selecting the optimal vasv=(\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤z,\n",
      "z⊤(I−∆)z⩽z⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "z.\n",
      "We then need to apply to z= Φθ∗and get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽θ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗.\n",
      "Thus, we get an overall upper bound of\n",
      "σ2s\n",
      "n+1\n",
      "nθ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗,\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can refine it with the new context.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "294 CHAPTER 10. ENSEMBLE LEARNING\n",
      "composed of expectations that can be readily computed. As shown next for special\n",
      "cases, we obtain a bias-variance trade-oﬀ similar to equation ( 3.6) for ridge regression\n",
      "in section 3.6, but now with random projections. Note that in the ﬁxed design set ting,\n",
      "there is no explosion of the testing error when s=n, as opposed to the random design\n",
      "setting studied in section 12.2 in the context of “double descent” (where generalization\n",
      "to unseen inputs is required).\n",
      "Gaussian projections. If we assume Gaussian random projections, with S∈Rd×s\n",
      "with independent standard Gaussian components, we get, from pr operties of the Wishart\n",
      "distribution,4\n",
      "ES[SS⊤] =sIandES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      "=s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I.\n",
      "We then get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽θ⊤\n",
      "∗Φ⊤(\n",
      "I−ΦES[\n",
      "SS⊤](\n",
      "ES[\n",
      "SS⊤Φ⊤ΦSS⊤])−1ES[\n",
      "SS⊤]\n",
      "Φ⊤)\n",
      "Φθ∗\n",
      "=θ⊤\n",
      "∗Φ⊤(\n",
      "I−s2Φ(\n",
      "s(s+ 1)Φ⊤Φ +str(Φ⊤Φ)I)−1Φ⊤)\n",
      "Φθ∗\n",
      "=θ⊤\n",
      "∗Φ⊤Φθ∗−sθ∗(Φ⊤Φ)2(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "=θ⊤\n",
      "∗Φ⊤Φ(\n",
      "Φ⊤Φ + tr(Φ⊤Φ)I)(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "⩽2 tr(Φ⊤Φ)·θ⊤\n",
      "∗Φ⊤Φ(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1θ∗\n",
      "using that Φ⊤Φ + tr(Φ⊤Φ)Ihas eigenvalues less than 2 tr(Φ⊤Φ),\n",
      "⩽2 tr(Φ⊤Φ)∥θ∗∥2\n",
      "2\n",
      "s+ 1,\n",
      "since Φ⊤Φ(\n",
      "(s+ 1)Φ⊤Φ + tr(Φ⊤Φ)I)−1has eigenvalues less than 1 /(s+ 1). The overall\n",
      "excess risk is then less than\n",
      "σ2s\n",
      "n+2\n",
      "ntr(Φ⊤Φ)∥θ∗∥2\n",
      "2\n",
      "s+ 1, (10.4)\n",
      "which is exactly of the form obtained for ridge regression in equation (3.6) with the\n",
      "identiﬁcation s∼tr(Φ⊤Φ)\n",
      "λ. We can consider other sketching matrices with additional\n",
      "properties, such as sparsity (see exercise 10.5).\n",
      "Exercise 10.5 We consider a sketching matrix S∈Rd×s, where each column is equal\n",
      "to one of the dcanonical basis vectors of Rd, selected uniformly at random and indepen-\n",
      "dently. Compute E[SS⊤], as well as ES[\n",
      "SS⊤Φ⊤ΦSS⊤]\n",
      ", as well as a bound similar to\n",
      "equation ( 10.4).\n",
      "4IfW=S1S⊤\n",
      "1forS1∈Rn×swith independent standard Gaussian components, then E[W] =sIand\n",
      "for ann×ndiagonal matrix D, we have E[WD2W] =s(s+1)D2+str(D2)I.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide me with the original summary so I can refine it with the new context. \n",
      "\n",
      "I'm ready to help you create a comprehensive and accurate final summary!  \n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.2. RANDOM PROJECTIONS AND AVERAGING 295\n",
      "Kernel methods. ( ♦)The random projection idea can be extended to kernel meth-\n",
      "ods discussed in chapter 7. We consider the kernel matrix K= ΦΦ⊤∈Rn×n, and the\n",
      "assumption y= Φθ∗+εwith∥θ∗∥2bounded is turned into y=y∗+εwithy⊤\n",
      "∗K−1y∗\n",
      "bounded. This corresponds to y∗=Kα, with a reproducing kernel Hilbert space (RKHS)\n",
      "normα⊤Kα. We then consider a random sketch ˆΦ∈Rn×sand an approximate ker-\n",
      "nel matrix ˆK. We then obtain an estimate ˆ y=ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤y. Matrix Π is then\n",
      "Π = ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤, and for the analysis, we need to compute its expectation, ∆ (this\n",
      "corresponds to replacing Φ Sin earlier developments starting at the beginning of sec-\n",
      "tion 10.2.2 with ˆΦ). We have, following the same reasoning as before, for an arbitra ry\n",
      "deterministic z∈Rn,\n",
      "z⊤(I−∆)z=EˆΦ[\n",
      "z⊤(I−Π)z]\n",
      "=EˆΦ[\n",
      "z⊤z−z⊤ˆΦ(ˆΦ⊤ˆΦ)−1ˆΦ⊤z]\n",
      "=EˆΦ[\n",
      "min\n",
      "u∈Rs∥z−ˆΦu∥2\n",
      "2]\n",
      "by deﬁnition of projections ,\n",
      "⩽EˆΦ[\n",
      "min\n",
      "v∈Rn∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "by minimizing over a smaller subspace,\n",
      "⩽min\n",
      "v∈RnEˆΦ[\n",
      "∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "by properties of the expectation.\n",
      "We can expand this to get\n",
      "EˆΦ[\n",
      "∥z−ˆΦˆΦ⊤v∥2\n",
      "2]\n",
      "=∥z∥2\n",
      "2−2z⊤EˆΦ[ˆΦˆΦ⊤]\n",
      "v+v⊤EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "v,\n",
      "leading to, after selecting the optimal vasv=(\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤]\n",
      "z,\n",
      "z⊤(I−∆)z⩽z⊤(\n",
      "I−EˆΦ[ˆΦˆΦ⊤](\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\n",
      "z. (10.5)\n",
      "We then need to apply equation ( 10.5) toz=y∗to get\n",
      "θ⊤\n",
      "∗Φ⊤[\n",
      "I−∆]\n",
      "Φθ∗⩽y⊤\n",
      "∗(\n",
      "I−EˆΦ[ˆΦˆΦ⊤](\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤])−1EˆΦ[ˆΦˆΦ⊤])\n",
      "y∗.\n",
      "We can, for example, consider each column of ˆΦ to be sampled from a Gaussian distri-\n",
      "bution with mean zero and covariance matrix K, for which we have\n",
      "EˆΦ[ˆΦˆΦ⊤]\n",
      "=sKandEˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "=s(s+ 1)K2+str(K)·K.\n",
      "Using equation ( 10.3), with the same derivations that led to equation ( 10.4), this leads\n",
      "to a bound on the expected excess risk equal toσ2s\n",
      "n+2\n",
      "ntr(K)y⊤\n",
      "∗K−1y∗\n",
      "s+1, which is exactly\n",
      "the bound in equation ( 10.4) in the kernel context. However, it is not interesting in\n",
      "practice, as it requires the computation of the kernel matrix Kand typically a square\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide me with the original summary so I can refine it with the new context. \n",
      "\n",
      "I'm ready to help you create a comprehensive and accurate final summary!  \n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "s+1, which is exactly\n",
      "the bound in equation ( 10.4) in the kernel context. However, it is not interesting in\n",
      "practice, as it requires the computation of the kernel matrix Kand typically a square\n",
      "root to sample from the multivariate Gaussian distribution, which has a running-time\n",
      "complexity of O(n3).\n",
      "In practice, many kernels come with a random feature expansion of the formk(x,x′) =\n",
      "Ev[\n",
      "ϕ(x,v)ϕ(x′,v)]\n",
      ", such that|ϕ(x,v)|⩽Ralmost surely (as presented in section 7.4).\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can refine it with the new context.  \n",
      "\n",
      "I need to see the existing summary to determine how the new information about random feature expansions and kernel computations should be integrated. \n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "296 CHAPTER 10. ENSEMBLE LEARNING\n",
      "We can take for each column of ˆΦ the vector ( ϕ(x1,v),...,ϕ (xn,v))⊤∈Rnfor a random\n",
      "independent v. Then we have E[ˆΦˆΦ⊤]\n",
      "=sKby construction, while a short calculation\n",
      "(left as an exercise) shows that the second-order moment can be bounded as\n",
      "EˆΦ[ˆΦˆΦ⊤ˆΦˆΦ⊤]\n",
      "≼s(s−1)K2+sR2K.\n",
      "This leads to the boundσ2s\n",
      "n+1\n",
      "nR2y⊤\n",
      "∗K−1y∗\n",
      "s−1, which is almost the same as before, but with\n",
      "an eﬃcient practical algorithm (since we now have to solve a least-sq uares regression\n",
      "problem in dimension s, which is more eﬃcient than using the kernel trick if s<n ).\n",
      "Experiments. In ﬁgure 10.2, we consider a polynomial regression problem in dimen-\n",
      "siondX= 20, with polynomials of a maximum degree of 2, and thus a feature sp ace\n",
      "of dimension d= 1 +dX+dX(dX+ 1)/2 = 231. We also compare ridge regression with\n",
      "Gaussian random projections. We see better performance as mgrows, consistent with our\n",
      "bounds (underﬁtting for small s, overﬁtting with large s). Moreover, when the number\n",
      "mof times the dataset is randomly projected goes from 10 to 100, we obtain almost the\n",
      "same plot, with better performance than m= 1, highlighting the fact that m= 1 is not\n",
      "optimal but taking mtoo large is not useful.\n",
      "Johnson-Lindenstrauss lemma ( ♦).A related classical result in Gaussian random\n",
      "projections shows that nfeature vectors ϕ1,...,ϕn∈Rdcan be well represented in di-\n",
      "mensionsby Gaussian random projections, with sgrowing only logarithmically in n, and\n",
      "independent of the underlying dimension. Lemma 10.1 shows that all pairwise distances\n",
      "are preserved (a small modiﬁcation would lead to the preservation o f all dot products).\n",
      "Lemma 10.1 ( Johnson and Lindenstrauss ,1984)Givenϕ1,...,ϕn∈Rd, letS∈\n",
      "Rd×sbe a random matrix with independent standard Gaussian rando m variables. Then,\n",
      "for anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\n",
      "ε2logn2\n",
      "δ, with probability greater than 1−δ,\n",
      "we have\n",
      "∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\n",
      "2⩽s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\n",
      "2⩽(1+ε)∥ϕi−ϕj∥2\n",
      "2.\n",
      "(10.6)\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide me with the original summary so I can refine it with the new context on random feature expansions and kernel computations. \n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "for anyε∈(0,1/2)andδ∈(0,1), ifs⩾6\n",
      "ε2logn2\n",
      "δ, with probability greater than 1−δ,\n",
      "we have\n",
      "∀i,j∈{1,...,n},(1−ε)∥ϕi−ϕj∥2\n",
      "2⩽s−1/2S⊤ϕi−s−1/2S⊤ϕj∥2\n",
      "2⩽(1+ε)∥ϕi−ϕj∥2\n",
      "2.\n",
      "(10.6)\n",
      "Proof (♦) Letψ∈Rdwith theℓ2-norm equal to 1. The random variable y=ψ⊤SS⊤ψ\n",
      "is the sum of srandom variables ψ⊤S·jS⊤\n",
      "·jψ, forS·jthejth column of S,j∈{1,...,s}.\n",
      "Each of these is the square of S⊤\n",
      "·jψ, which is Gaussian with mean zero and variance equal\n",
      "to∥ψ∥2\n",
      "2= 1. Thus, yis a chi-squared random variable.5We can thus apply concentration\n",
      "results from exercise 8.1, leading to\n",
      "P(\n",
      "|y−s|⩾sε)\n",
      "⩽(1−ε\n",
      "exp(−ε))s/2\n",
      "+(1 +ε\n",
      "exp(ε))s/2\n",
      ".\n",
      "We can then use the inequality log(1 + u)⩽u−u2\n",
      "3for any|u|⩽1\n",
      "2, applied to εand\n",
      "−ε, leading to the probability bound P(\n",
      "|y−s|⩾sε)\n",
      "⩽2 exp(\n",
      "−s\n",
      "2ε2\n",
      "3)\n",
      ".We then apply\n",
      "5Seehttps://en.wikipedia.org/wiki/Chi-squared_distribut ion.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can refine it with the new context on random feature expansions and kernel computations.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "10.2. RANDOM PROJECTIONS AND AVERAGING 297\n",
      "Figure 10.2. Polynomial regression in dimension 20, with polynomials of a maximum\n",
      "degree of 2, with n= 100. Top left: training and testing errors for ridge regression in t he\n",
      "ﬁxed design setting (the input data are ﬁxed, and only the noise var iables are resampled\n",
      "for computing the test error). All other plots: training and testin g errors for Gaussian\n",
      "random projections, with diﬀerent numbers of random projection s:m= 1 (top right),\n",
      "m= 10 (bottom left), and m= 100 (bottom right). All the curves are averaged over 100\n",
      "replications of the noise variables and the random projections.\n",
      "this bound for ψbeing then(n−1)/2 vectorsϕi−ϕj, fori̸=j, leading to, using a\n",
      "union bound, a probability that equation ( 10.6) is not satisﬁed with a probability less\n",
      "thann2exp(−sε2/6),leading to the desired result.\n",
      "In our context of least-squares regression, the Johnson-Linde nstrauss lemma shows that\n",
      "the kernel matrix is preserved by random projections so that pre dictions with the pro-\n",
      "jected data should be close to predictions with the original data. Th e results in this\n",
      "section provide a direct proof that aims to characterize directly th e predictive perfor-\n",
      "mance of such random projections (using the Johnson-Lindenstr auss lemma to obtain\n",
      "similar bounds is not straightforward as we consider unregularized r egression, where per-\n",
      "turbations of matrix inverses are harder to control).\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the original summary so I can refine it with the new context on random feature expansions and kernel computations.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "298 CHAPTER 10. ENSEMBLE LEARNING\n",
      "10.3 Boosting\n",
      "In sections 10.1 and10.2, we focused on uniformly combining the outputs (e.g., plain av-\n",
      "eraging) of estimators obtained by randomly reweighted versions o f the original datasets.\n",
      "Reweighting was performed independent of the performance of th e resulting prediction\n",
      "functions, and the training procedures for all predictors could be done in parallel. In this\n",
      "section, we explore sequential reweightings of the training datasets that depend on the\n",
      "mistakes made by the current prediction functions. While the natur al parallelizability is\n",
      "lost, we will see that we get additional statistical beneﬁts.\n",
      "In the early boosting procedures adapted to binary classiﬁcation, the original learn-\n",
      "ing algorithms (going from datasets to prediction functions with bina ry values) were used\n",
      "directly on a reweighted version, such as Adaboost (see, e.g., Freund et al. ,1999). Our\n",
      "analysis will be carried out for boosting procedures, often referr ed to as “gradient boost-\n",
      "ing,” which are adapted to real-valued outputs, as done in the rest o f this book (noting\n",
      "that for classiﬁcation, we can use convex surrogates).\n",
      "The theory of boosting is rich, with many connections, and in this sec tion, we only\n",
      "provide a consistency proof in the simplest setting. See Schapire and Freund (2012) for\n",
      "more details.\n",
      "10.3.1 Problem Setup\n",
      "Given an input space Xandnobservations ( xi,yi)∈X×R,i= 1,...,n , we are given a\n",
      "set of predictors ϕ(·,w) :X→R, forw∈W, withWtypically being a compact subset of\n",
      "a ﬁnite-dimensional vector space.\n",
      "The main assumption is that given weights α∈Rn, one can reasonably easily ﬁnd\n",
      "the function ϕ(·,w) that minimizes with respect to w∈W:\n",
      "n∑\n",
      "i=1αiϕ(xi,w); (10.7)\n",
      "that is, the dot product between αand thenoutputs ofϕ(·,w) on thenobservations. In\n",
      "this section, for simplicity, we assume that this minimization can be don e exactly. This\n",
      "is often referred to as the “weak learner” assumption. Many exam ples are available, such\n",
      "as the following:\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Please provide the new context on random feature expansions and kernel computations so I can refine the summary. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "this section, for simplicity, we assume that this minimization can be don e exactly. This\n",
      "is often referred to as the “weak learner” assumption. Many exam ples are available, such\n",
      "as the following:\n",
      "•Linear stumps for X=Rd:ϕ(x,w) =±(w⊤\n",
      "0x+w1)+, wherew= (w0,w1)∈Rd×R,\n",
      "with sometimes the restriction that whas a single nonzero component (where the\n",
      "weak learning tractability assumption is indeed veriﬁed; see exercise 10.6). This\n",
      "will lead to a predictor, which is a one-hidden-layer neural network a s presented\n",
      "in chapter 9, but learned sequentially rather than by GD on the empirical risk. In\n",
      "the context of binary classiﬁcation, the weak learners are sometim es thresholded to\n",
      "values in{−1,1}by taking their signs.\n",
      "Exercise 10.6 For linear stumps with only one nonzero coordinate for the sl ope,\n",
      "show how to minimize equation ( 10.7) eﬃciently.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please provide me with the new context on random feature expansions and kernel computations so I can refine the summary. \\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "output_summary = chain.run(final_docs)\n",
    "output_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
